package searchservice

// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See License.txt in the project root for license information.
//
// Code generated by Microsoft (R) AutoRest Code Generator.
// Changes may cause incorrect behavior and will be lost if the code is regenerated.

import (
	"encoding/json"

	"github.com/Azure/go-autorest/autorest"
	"github.com/Azure/go-autorest/autorest/date"
)

// The package's fully qualified name.
const fqdn = "github.com/Azure/azure-sdk-for-go/services/search/2019-05-01/searchservice"

// BasicAnalyzer abstract base class for analyzers.
type BasicAnalyzer interface {
	AsCustomAnalyzer() (*CustomAnalyzer, bool)
	AsPatternAnalyzer() (*PatternAnalyzer, bool)
	AsStandardAnalyzer() (*StandardAnalyzer, bool)
	AsStopAnalyzer() (*StopAnalyzer, bool)
	AsAnalyzer() (*Analyzer, bool)
}

// Analyzer abstract base class for analyzers.
type Analyzer struct {
	// Name - The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeAnalyzer', 'OdataTypeMicrosoftAzureSearchCustomAnalyzer', 'OdataTypeMicrosoftAzureSearchPatternAnalyzer', 'OdataTypeMicrosoftAzureSearchStandardAnalyzer', 'OdataTypeMicrosoftAzureSearchStopAnalyzer'
	OdataType OdataType `json:"@odata.type,omitempty"`
}

func unmarshalBasicAnalyzer(body []byte) (BasicAnalyzer, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeMicrosoftAzureSearchCustomAnalyzer):
		var ca CustomAnalyzer
		err := json.Unmarshal(body, &ca)
		return ca, err
	case string(OdataTypeMicrosoftAzureSearchPatternAnalyzer):
		var pa PatternAnalyzer
		err := json.Unmarshal(body, &pa)
		return pa, err
	case string(OdataTypeMicrosoftAzureSearchStandardAnalyzer):
		var sa StandardAnalyzer
		err := json.Unmarshal(body, &sa)
		return sa, err
	case string(OdataTypeMicrosoftAzureSearchStopAnalyzer):
		var sa StopAnalyzer
		err := json.Unmarshal(body, &sa)
		return sa, err
	default:
		var a Analyzer
		err := json.Unmarshal(body, &a)
		return a, err
	}
}
func unmarshalBasicAnalyzerArray(body []byte) ([]BasicAnalyzer, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	aArray := make([]BasicAnalyzer, len(rawMessages))

	for index, rawMessage := range rawMessages {
		a, err := unmarshalBasicAnalyzer(*rawMessage)
		if err != nil {
			return nil, err
		}
		aArray[index] = a
	}
	return aArray, nil
}

// MarshalJSON is the custom marshaler for Analyzer.
func (a Analyzer) MarshalJSON() ([]byte, error) {
	a.OdataType = OdataTypeAnalyzer
	objectMap := make(map[string]interface{})
	if a.Name != nil {
		objectMap["name"] = a.Name
	}
	if a.OdataType != "" {
		objectMap["@odata.type"] = a.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomAnalyzer is the BasicAnalyzer implementation for Analyzer.
func (a Analyzer) AsCustomAnalyzer() (*CustomAnalyzer, bool) {
	return nil, false
}

// AsPatternAnalyzer is the BasicAnalyzer implementation for Analyzer.
func (a Analyzer) AsPatternAnalyzer() (*PatternAnalyzer, bool) {
	return nil, false
}

// AsStandardAnalyzer is the BasicAnalyzer implementation for Analyzer.
func (a Analyzer) AsStandardAnalyzer() (*StandardAnalyzer, bool) {
	return nil, false
}

// AsStopAnalyzer is the BasicAnalyzer implementation for Analyzer.
func (a Analyzer) AsStopAnalyzer() (*StopAnalyzer, bool) {
	return nil, false
}

// AsAnalyzer is the BasicAnalyzer implementation for Analyzer.
func (a Analyzer) AsAnalyzer() (*Analyzer, bool) {
	return &a, true
}

// AsBasicAnalyzer is the BasicAnalyzer implementation for Analyzer.
func (a Analyzer) AsBasicAnalyzer() (BasicAnalyzer, bool) {
	return &a, true
}

// AnalyzeRequest specifies some text and analysis components used to break that text into tokens.
type AnalyzeRequest struct {
	// Text - The text to break into tokens.
	Text *string `json:"text,omitempty"`
	// Analyzer - The name of the analyzer to use to break the given text. If this parameter is not specified, you must specify a tokenizer instead. The tokenizer and analyzer parameters are mutually exclusive. Possible values include: 'Armicrosoft', 'Arlucene', 'Hylucene', 'Bnmicrosoft', 'Eulucene', 'Bgmicrosoft', 'Bglucene', 'Camicrosoft', 'Calucene', 'ZhHansmicrosoft', 'ZhHanslucene', 'ZhHantmicrosoft', 'ZhHantlucene', 'Hrmicrosoft', 'Csmicrosoft', 'Cslucene', 'Damicrosoft', 'Dalucene', 'Nlmicrosoft', 'Nllucene', 'Enmicrosoft', 'Enlucene', 'Etmicrosoft', 'Fimicrosoft', 'Filucene', 'Frmicrosoft', 'Frlucene', 'Gllucene', 'Demicrosoft', 'Delucene', 'Elmicrosoft', 'Ellucene', 'Gumicrosoft', 'Hemicrosoft', 'Himicrosoft', 'Hilucene', 'Humicrosoft', 'Hulucene', 'Ismicrosoft', 'Idmicrosoft', 'Idlucene', 'Galucene', 'Itmicrosoft', 'Itlucene', 'Jamicrosoft', 'Jalucene', 'Knmicrosoft', 'Komicrosoft', 'Kolucene', 'Lvmicrosoft', 'Lvlucene', 'Ltmicrosoft', 'Mlmicrosoft', 'Msmicrosoft', 'Mrmicrosoft', 'Nbmicrosoft', 'Nolucene', 'Falucene', 'Plmicrosoft', 'Pllucene', 'PtBRmicrosoft', 'PtBRlucene', 'PtPTmicrosoft', 'PtPTlucene', 'Pamicrosoft', 'Romicrosoft', 'Rolucene', 'Rumicrosoft', 'Rulucene', 'SrCyrillicmicrosoft', 'SrLatinmicrosoft', 'Skmicrosoft', 'Slmicrosoft', 'Esmicrosoft', 'Eslucene', 'Svmicrosoft', 'Svlucene', 'Tamicrosoft', 'Temicrosoft', 'Thmicrosoft', 'Thlucene', 'Trmicrosoft', 'Trlucene', 'Ukmicrosoft', 'Urmicrosoft', 'Vimicrosoft', 'Standardlucene', 'Standardasciifoldinglucene', 'Keyword', 'Pattern', 'Simple', 'Stop', 'Whitespace'
	Analyzer AnalyzerName `json:"analyzer,omitempty"`
	// Tokenizer - The name of the tokenizer to use to break the given text. If this parameter is not specified, you must specify an analyzer instead. The tokenizer and analyzer parameters are mutually exclusive. Possible values include: 'TokenizerNameClassic', 'TokenizerNameEdgeNGram', 'TokenizerNameKeyword', 'TokenizerNameLetter', 'TokenizerNameLowercase', 'TokenizerNameMicrosoftLanguageTokenizer', 'TokenizerNameMicrosoftLanguageStemmingTokenizer', 'TokenizerNameNGram', 'TokenizerNamePathHierarchy', 'TokenizerNamePattern', 'TokenizerNameStandard', 'TokenizerNameUaxURLEmail', 'TokenizerNameWhitespace'
	Tokenizer TokenizerName `json:"tokenizer,omitempty"`
	// TokenFilters - An optional list of token filters to use when breaking the given text. This parameter can only be set when using the tokenizer parameter.
	TokenFilters *[]TokenFilterName `json:"tokenFilters,omitempty"`
	// CharFilters - An optional list of character filters to use when breaking the given text. This parameter can only be set when using the tokenizer parameter.
	CharFilters *[]CharFilterName `json:"charFilters,omitempty"`
}

// AnalyzeResult the result of testing an analyzer on text.
type AnalyzeResult struct {
	autorest.Response `json:"-"`
	// Tokens - The list of tokens returned by the analyzer specified in the request.
	Tokens *[]TokenInfo `json:"tokens,omitempty"`
}

// ASCIIFoldingTokenFilter converts alphabetic, numeric, and symbolic Unicode characters which are not in
// the first 127 ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if such
// equivalents exist. This token filter is implemented using Apache Lucene.
type ASCIIFoldingTokenFilter struct {
	// PreserveOriginal - A value indicating whether the original token will be kept. Default is false.
	PreserveOriginal *bool `json:"preserveOriginal,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) MarshalJSON() ([]byte, error) {
	aftf.OdataType = OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter
	objectMap := make(map[string]interface{})
	if aftf.PreserveOriginal != nil {
		objectMap["preserveOriginal"] = aftf.PreserveOriginal
	}
	if aftf.Name != nil {
		objectMap["name"] = aftf.Name
	}
	if aftf.OdataType != "" {
		objectMap["@odata.type"] = aftf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return &aftf, true
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for ASCIIFoldingTokenFilter.
func (aftf ASCIIFoldingTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &aftf, true
}

// BasicCharFilter abstract base class for character filters.
type BasicCharFilter interface {
	AsMappingCharFilter() (*MappingCharFilter, bool)
	AsPatternReplaceCharFilter() (*PatternReplaceCharFilter, bool)
	AsCharFilter() (*CharFilter, bool)
}

// CharFilter abstract base class for character filters.
type CharFilter struct {
	// Name - The name of the char filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeCharFilter', 'OdataTypeMicrosoftAzureSearchMappingCharFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceCharFilter'
	OdataType OdataTypeBasicCharFilter `json:"@odata.type,omitempty"`
}

func unmarshalBasicCharFilter(body []byte) (BasicCharFilter, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeMicrosoftAzureSearchMappingCharFilter):
		var mcf MappingCharFilter
		err := json.Unmarshal(body, &mcf)
		return mcf, err
	case string(OdataTypeMicrosoftAzureSearchPatternReplaceCharFilter):
		var prcf PatternReplaceCharFilter
		err := json.Unmarshal(body, &prcf)
		return prcf, err
	default:
		var cf CharFilter
		err := json.Unmarshal(body, &cf)
		return cf, err
	}
}
func unmarshalBasicCharFilterArray(body []byte) ([]BasicCharFilter, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	cfArray := make([]BasicCharFilter, len(rawMessages))

	for index, rawMessage := range rawMessages {
		cf, err := unmarshalBasicCharFilter(*rawMessage)
		if err != nil {
			return nil, err
		}
		cfArray[index] = cf
	}
	return cfArray, nil
}

// MarshalJSON is the custom marshaler for CharFilter.
func (cf CharFilter) MarshalJSON() ([]byte, error) {
	cf.OdataType = OdataTypeCharFilter
	objectMap := make(map[string]interface{})
	if cf.Name != nil {
		objectMap["name"] = cf.Name
	}
	if cf.OdataType != "" {
		objectMap["@odata.type"] = cf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsMappingCharFilter is the BasicCharFilter implementation for CharFilter.
func (cf CharFilter) AsMappingCharFilter() (*MappingCharFilter, bool) {
	return nil, false
}

// AsPatternReplaceCharFilter is the BasicCharFilter implementation for CharFilter.
func (cf CharFilter) AsPatternReplaceCharFilter() (*PatternReplaceCharFilter, bool) {
	return nil, false
}

// AsCharFilter is the BasicCharFilter implementation for CharFilter.
func (cf CharFilter) AsCharFilter() (*CharFilter, bool) {
	return &cf, true
}

// AsBasicCharFilter is the BasicCharFilter implementation for CharFilter.
func (cf CharFilter) AsBasicCharFilter() (BasicCharFilter, bool) {
	return &cf, true
}

// CjkBigramTokenFilter forms bigrams of CJK terms that are generated from StandardTokenizer. This token
// filter is implemented using Apache Lucene.
type CjkBigramTokenFilter struct {
	// IgnoreScripts - The scripts to ignore.
	IgnoreScripts *[]CjkBigramTokenFilterScripts `json:"ignoreScripts,omitempty"`
	// OutputUnigrams - A value indicating whether to output both unigrams and bigrams (if true), or just bigrams (if false). Default is false.
	OutputUnigrams *bool `json:"outputUnigrams,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) MarshalJSON() ([]byte, error) {
	cbtf.OdataType = OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter
	objectMap := make(map[string]interface{})
	if cbtf.IgnoreScripts != nil {
		objectMap["ignoreScripts"] = cbtf.IgnoreScripts
	}
	if cbtf.OutputUnigrams != nil {
		objectMap["outputUnigrams"] = cbtf.OutputUnigrams
	}
	if cbtf.Name != nil {
		objectMap["name"] = cbtf.Name
	}
	if cbtf.OdataType != "" {
		objectMap["@odata.type"] = cbtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return &cbtf, true
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for CjkBigramTokenFilter.
func (cbtf CjkBigramTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &cbtf, true
}

// ClassicTokenizer grammar-based tokenizer that is suitable for processing most European-language
// documents. This tokenizer is implemented using Apache Lucene.
type ClassicTokenizer struct {
	// MaxTokenLength - The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ClassicTokenizer.
func (ct ClassicTokenizer) MarshalJSON() ([]byte, error) {
	ct.OdataType = OdataTypeMicrosoftAzureSearchClassicTokenizer
	objectMap := make(map[string]interface{})
	if ct.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = ct.MaxTokenLength
	}
	if ct.Name != nil {
		objectMap["name"] = ct.Name
	}
	if ct.OdataType != "" {
		objectMap["@odata.type"] = ct.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return &ct, true
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for ClassicTokenizer.
func (ct ClassicTokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &ct, true
}

// BasicCognitiveServicesAccount abstract base class for describing any cognitive service resource attached to the
// skillset.
type BasicCognitiveServicesAccount interface {
	AsDefaultCognitiveServicesAccount() (*DefaultCognitiveServicesAccount, bool)
	AsCognitiveServicesAccountKey() (*CognitiveServicesAccountKey, bool)
	AsCognitiveServicesAccount() (*CognitiveServicesAccount, bool)
}

// CognitiveServicesAccount abstract base class for describing any cognitive service resource attached to the
// skillset.
type CognitiveServicesAccount struct {
	Description *string `json:"description,omitempty"`
	// OdataType - Possible values include: 'OdataTypeCognitiveServicesAccount', 'OdataTypeMicrosoftAzureSearchDefaultCognitiveServices', 'OdataTypeMicrosoftAzureSearchCognitiveServicesByKey'
	OdataType OdataTypeBasicCognitiveServicesAccount `json:"@odata.type,omitempty"`
}

func unmarshalBasicCognitiveServicesAccount(body []byte) (BasicCognitiveServicesAccount, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeMicrosoftAzureSearchDefaultCognitiveServices):
		var dcsa DefaultCognitiveServicesAccount
		err := json.Unmarshal(body, &dcsa)
		return dcsa, err
	case string(OdataTypeMicrosoftAzureSearchCognitiveServicesByKey):
		var csak CognitiveServicesAccountKey
		err := json.Unmarshal(body, &csak)
		return csak, err
	default:
		var csa CognitiveServicesAccount
		err := json.Unmarshal(body, &csa)
		return csa, err
	}
}
func unmarshalBasicCognitiveServicesAccountArray(body []byte) ([]BasicCognitiveServicesAccount, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	csaArray := make([]BasicCognitiveServicesAccount, len(rawMessages))

	for index, rawMessage := range rawMessages {
		csa, err := unmarshalBasicCognitiveServicesAccount(*rawMessage)
		if err != nil {
			return nil, err
		}
		csaArray[index] = csa
	}
	return csaArray, nil
}

// MarshalJSON is the custom marshaler for CognitiveServicesAccount.
func (csa CognitiveServicesAccount) MarshalJSON() ([]byte, error) {
	csa.OdataType = OdataTypeCognitiveServicesAccount
	objectMap := make(map[string]interface{})
	if csa.Description != nil {
		objectMap["description"] = csa.Description
	}
	if csa.OdataType != "" {
		objectMap["@odata.type"] = csa.OdataType
	}
	return json.Marshal(objectMap)
}

// AsDefaultCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccount.
func (csa CognitiveServicesAccount) AsDefaultCognitiveServicesAccount() (*DefaultCognitiveServicesAccount, bool) {
	return nil, false
}

// AsCognitiveServicesAccountKey is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccount.
func (csa CognitiveServicesAccount) AsCognitiveServicesAccountKey() (*CognitiveServicesAccountKey, bool) {
	return nil, false
}

// AsCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccount.
func (csa CognitiveServicesAccount) AsCognitiveServicesAccount() (*CognitiveServicesAccount, bool) {
	return &csa, true
}

// AsBasicCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccount.
func (csa CognitiveServicesAccount) AsBasicCognitiveServicesAccount() (BasicCognitiveServicesAccount, bool) {
	return &csa, true
}

// CognitiveServicesAccountKey a cognitive service resource provisioned with a key that is attached to a
// skillset.
type CognitiveServicesAccountKey struct {
	Key         *string `json:"key,omitempty"`
	Description *string `json:"description,omitempty"`
	// OdataType - Possible values include: 'OdataTypeCognitiveServicesAccount', 'OdataTypeMicrosoftAzureSearchDefaultCognitiveServices', 'OdataTypeMicrosoftAzureSearchCognitiveServicesByKey'
	OdataType OdataTypeBasicCognitiveServicesAccount `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for CognitiveServicesAccountKey.
func (csak CognitiveServicesAccountKey) MarshalJSON() ([]byte, error) {
	csak.OdataType = OdataTypeMicrosoftAzureSearchCognitiveServicesByKey
	objectMap := make(map[string]interface{})
	if csak.Key != nil {
		objectMap["key"] = csak.Key
	}
	if csak.Description != nil {
		objectMap["description"] = csak.Description
	}
	if csak.OdataType != "" {
		objectMap["@odata.type"] = csak.OdataType
	}
	return json.Marshal(objectMap)
}

// AsDefaultCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccountKey.
func (csak CognitiveServicesAccountKey) AsDefaultCognitiveServicesAccount() (*DefaultCognitiveServicesAccount, bool) {
	return nil, false
}

// AsCognitiveServicesAccountKey is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccountKey.
func (csak CognitiveServicesAccountKey) AsCognitiveServicesAccountKey() (*CognitiveServicesAccountKey, bool) {
	return &csak, true
}

// AsCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccountKey.
func (csak CognitiveServicesAccountKey) AsCognitiveServicesAccount() (*CognitiveServicesAccount, bool) {
	return nil, false
}

// AsBasicCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for CognitiveServicesAccountKey.
func (csak CognitiveServicesAccountKey) AsBasicCognitiveServicesAccount() (BasicCognitiveServicesAccount, bool) {
	return &csak, true
}

// CommonGramTokenFilter construct bigrams for frequently occurring terms while indexing. Single terms are
// still indexed too, with bigrams overlaid. This token filter is implemented using Apache Lucene.
type CommonGramTokenFilter struct {
	// CommonWords - The set of common words.
	CommonWords *[]string `json:"commonWords,omitempty"`
	// IgnoreCase - A value indicating whether common words matching will be case insensitive. Default is false.
	IgnoreCase *bool `json:"ignoreCase,omitempty"`
	// UseQueryMode - A value that indicates whether the token filter is in query mode. When in query mode, the token filter generates bigrams and then removes common words and single terms followed by a common word. Default is false.
	UseQueryMode *bool `json:"queryMode,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) MarshalJSON() ([]byte, error) {
	cgtf.OdataType = OdataTypeMicrosoftAzureSearchCommonGramTokenFilter
	objectMap := make(map[string]interface{})
	if cgtf.CommonWords != nil {
		objectMap["commonWords"] = cgtf.CommonWords
	}
	if cgtf.IgnoreCase != nil {
		objectMap["ignoreCase"] = cgtf.IgnoreCase
	}
	if cgtf.UseQueryMode != nil {
		objectMap["queryMode"] = cgtf.UseQueryMode
	}
	if cgtf.Name != nil {
		objectMap["name"] = cgtf.Name
	}
	if cgtf.OdataType != "" {
		objectMap["@odata.type"] = cgtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return &cgtf, true
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for CommonGramTokenFilter.
func (cgtf CommonGramTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &cgtf, true
}

// ConditionalSkill a skill that enables scenarios that require a Boolean operation to determine the data
// to assign to an output.
type ConditionalSkill struct {
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ConditionalSkill.
func (cs ConditionalSkill) MarshalJSON() ([]byte, error) {
	cs.OdataType = OdataTypeMicrosoftSkillsUtilConditionalSkill
	objectMap := make(map[string]interface{})
	if cs.Name != nil {
		objectMap["name"] = cs.Name
	}
	if cs.Description != nil {
		objectMap["description"] = cs.Description
	}
	if cs.Context != nil {
		objectMap["context"] = cs.Context
	}
	if cs.Inputs != nil {
		objectMap["inputs"] = cs.Inputs
	}
	if cs.Outputs != nil {
		objectMap["outputs"] = cs.Outputs
	}
	if cs.OdataType != "" {
		objectMap["@odata.type"] = cs.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return &cs, true
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for ConditionalSkill.
func (cs ConditionalSkill) AsBasicSkill() (BasicSkill, bool) {
	return &cs, true
}

// CorsOptions defines options to control Cross-Origin Resource Sharing (CORS) for an index.
type CorsOptions struct {
	// AllowedOrigins - The list of origins from which JavaScript code will be granted access to your index. Can contain a list of hosts of the form {protocol}://{fully-qualified-domain-name}[:{port#}], or a single '*' to allow all origins (not recommended).
	AllowedOrigins *[]string `json:"allowedOrigins,omitempty"`
	// MaxAgeInSeconds - The duration for which browsers should cache CORS preflight responses. Defaults to 5 minutes.
	MaxAgeInSeconds *int64 `json:"maxAgeInSeconds,omitempty"`
}

// CustomAnalyzer allows you to take control over the process of converting text into indexable/searchable
// tokens. It's a user-defined configuration consisting of a single predefined tokenizer and one or more
// filters. The tokenizer is responsible for breaking text into tokens, and the filters for modifying
// tokens emitted by the tokenizer.
type CustomAnalyzer struct {
	// Tokenizer - The name of the tokenizer to use to divide continuous text into a sequence of tokens, such as breaking a sentence into words. Possible values include: 'TokenizerNameClassic', 'TokenizerNameEdgeNGram', 'TokenizerNameKeyword', 'TokenizerNameLetter', 'TokenizerNameLowercase', 'TokenizerNameMicrosoftLanguageTokenizer', 'TokenizerNameMicrosoftLanguageStemmingTokenizer', 'TokenizerNameNGram', 'TokenizerNamePathHierarchy', 'TokenizerNamePattern', 'TokenizerNameStandard', 'TokenizerNameUaxURLEmail', 'TokenizerNameWhitespace'
	Tokenizer TokenizerName `json:"tokenizer,omitempty"`
	// TokenFilters - A list of token filters used to filter out or modify the tokens generated by a tokenizer. For example, you can specify a lowercase filter that converts all characters to lowercase. The filters are run in the order in which they are listed.
	TokenFilters *[]TokenFilterName `json:"tokenFilters,omitempty"`
	// CharFilters - A list of character filters used to prepare input text before it is processed by the tokenizer. For instance, they can replace certain characters or symbols. The filters are run in the order in which they are listed.
	CharFilters *[]CharFilterName `json:"charFilters,omitempty"`
	// Name - The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeAnalyzer', 'OdataTypeMicrosoftAzureSearchCustomAnalyzer', 'OdataTypeMicrosoftAzureSearchPatternAnalyzer', 'OdataTypeMicrosoftAzureSearchStandardAnalyzer', 'OdataTypeMicrosoftAzureSearchStopAnalyzer'
	OdataType OdataType `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for CustomAnalyzer.
func (ca CustomAnalyzer) MarshalJSON() ([]byte, error) {
	ca.OdataType = OdataTypeMicrosoftAzureSearchCustomAnalyzer
	objectMap := make(map[string]interface{})
	if ca.Tokenizer != "" {
		objectMap["tokenizer"] = ca.Tokenizer
	}
	if ca.TokenFilters != nil {
		objectMap["tokenFilters"] = ca.TokenFilters
	}
	if ca.CharFilters != nil {
		objectMap["charFilters"] = ca.CharFilters
	}
	if ca.Name != nil {
		objectMap["name"] = ca.Name
	}
	if ca.OdataType != "" {
		objectMap["@odata.type"] = ca.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomAnalyzer is the BasicAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsCustomAnalyzer() (*CustomAnalyzer, bool) {
	return &ca, true
}

// AsPatternAnalyzer is the BasicAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsPatternAnalyzer() (*PatternAnalyzer, bool) {
	return nil, false
}

// AsStandardAnalyzer is the BasicAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsStandardAnalyzer() (*StandardAnalyzer, bool) {
	return nil, false
}

// AsStopAnalyzer is the BasicAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsStopAnalyzer() (*StopAnalyzer, bool) {
	return nil, false
}

// AsAnalyzer is the BasicAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsAnalyzer() (*Analyzer, bool) {
	return nil, false
}

// AsBasicAnalyzer is the BasicAnalyzer implementation for CustomAnalyzer.
func (ca CustomAnalyzer) AsBasicAnalyzer() (BasicAnalyzer, bool) {
	return &ca, true
}

// BasicDataChangeDetectionPolicy abstract base class for data change detection policies.
type BasicDataChangeDetectionPolicy interface {
	AsHighWaterMarkChangeDetectionPolicy() (*HighWaterMarkChangeDetectionPolicy, bool)
	AsSQLIntegratedChangeTrackingPolicy() (*SQLIntegratedChangeTrackingPolicy, bool)
	AsDataChangeDetectionPolicy() (*DataChangeDetectionPolicy, bool)
}

// DataChangeDetectionPolicy abstract base class for data change detection policies.
type DataChangeDetectionPolicy struct {
	// OdataType - Possible values include: 'OdataTypeDataChangeDetectionPolicy', 'OdataTypeMicrosoftAzureSearchHighWaterMarkChangeDetectionPolicy', 'OdataTypeMicrosoftAzureSearchSQLIntegratedChangeTrackingPolicy'
	OdataType OdataTypeBasicDataChangeDetectionPolicy `json:"@odata.type,omitempty"`
}

func unmarshalBasicDataChangeDetectionPolicy(body []byte) (BasicDataChangeDetectionPolicy, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeMicrosoftAzureSearchHighWaterMarkChangeDetectionPolicy):
		var hwmcdp HighWaterMarkChangeDetectionPolicy
		err := json.Unmarshal(body, &hwmcdp)
		return hwmcdp, err
	case string(OdataTypeMicrosoftAzureSearchSQLIntegratedChangeTrackingPolicy):
		var sictp SQLIntegratedChangeTrackingPolicy
		err := json.Unmarshal(body, &sictp)
		return sictp, err
	default:
		var dcdp DataChangeDetectionPolicy
		err := json.Unmarshal(body, &dcdp)
		return dcdp, err
	}
}
func unmarshalBasicDataChangeDetectionPolicyArray(body []byte) ([]BasicDataChangeDetectionPolicy, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	dcdpArray := make([]BasicDataChangeDetectionPolicy, len(rawMessages))

	for index, rawMessage := range rawMessages {
		dcdp, err := unmarshalBasicDataChangeDetectionPolicy(*rawMessage)
		if err != nil {
			return nil, err
		}
		dcdpArray[index] = dcdp
	}
	return dcdpArray, nil
}

// MarshalJSON is the custom marshaler for DataChangeDetectionPolicy.
func (dcdp DataChangeDetectionPolicy) MarshalJSON() ([]byte, error) {
	dcdp.OdataType = OdataTypeDataChangeDetectionPolicy
	objectMap := make(map[string]interface{})
	if dcdp.OdataType != "" {
		objectMap["@odata.type"] = dcdp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsHighWaterMarkChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for DataChangeDetectionPolicy.
func (dcdp DataChangeDetectionPolicy) AsHighWaterMarkChangeDetectionPolicy() (*HighWaterMarkChangeDetectionPolicy, bool) {
	return nil, false
}

// AsSQLIntegratedChangeTrackingPolicy is the BasicDataChangeDetectionPolicy implementation for DataChangeDetectionPolicy.
func (dcdp DataChangeDetectionPolicy) AsSQLIntegratedChangeTrackingPolicy() (*SQLIntegratedChangeTrackingPolicy, bool) {
	return nil, false
}

// AsDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for DataChangeDetectionPolicy.
func (dcdp DataChangeDetectionPolicy) AsDataChangeDetectionPolicy() (*DataChangeDetectionPolicy, bool) {
	return &dcdp, true
}

// AsBasicDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for DataChangeDetectionPolicy.
func (dcdp DataChangeDetectionPolicy) AsBasicDataChangeDetectionPolicy() (BasicDataChangeDetectionPolicy, bool) {
	return &dcdp, true
}

// DataContainer represents information about the entity (such as Azure SQL table or CosmosDB collection)
// that will be indexed.
type DataContainer struct {
	// Name - The name of the table or view (for Azure SQL data source) or collection (for CosmosDB data source) that will be indexed.
	Name *string `json:"name,omitempty"`
	// Query - A query that is applied to this data container. The syntax and meaning of this parameter is datasource-specific. Not supported by Azure SQL datasources.
	Query *string `json:"query,omitempty"`
}

// BasicDataDeletionDetectionPolicy abstract base class for data deletion detection policies.
type BasicDataDeletionDetectionPolicy interface {
	AsSoftDeleteColumnDeletionDetectionPolicy() (*SoftDeleteColumnDeletionDetectionPolicy, bool)
	AsDataDeletionDetectionPolicy() (*DataDeletionDetectionPolicy, bool)
}

// DataDeletionDetectionPolicy abstract base class for data deletion detection policies.
type DataDeletionDetectionPolicy struct {
	// OdataType - Possible values include: 'OdataTypeDataDeletionDetectionPolicy', 'OdataTypeMicrosoftAzureSearchSoftDeleteColumnDeletionDetectionPolicy'
	OdataType OdataTypeBasicDataDeletionDetectionPolicy `json:"@odata.type,omitempty"`
}

func unmarshalBasicDataDeletionDetectionPolicy(body []byte) (BasicDataDeletionDetectionPolicy, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeMicrosoftAzureSearchSoftDeleteColumnDeletionDetectionPolicy):
		var sdcddp SoftDeleteColumnDeletionDetectionPolicy
		err := json.Unmarshal(body, &sdcddp)
		return sdcddp, err
	default:
		var dddp DataDeletionDetectionPolicy
		err := json.Unmarshal(body, &dddp)
		return dddp, err
	}
}
func unmarshalBasicDataDeletionDetectionPolicyArray(body []byte) ([]BasicDataDeletionDetectionPolicy, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	dddpArray := make([]BasicDataDeletionDetectionPolicy, len(rawMessages))

	for index, rawMessage := range rawMessages {
		dddp, err := unmarshalBasicDataDeletionDetectionPolicy(*rawMessage)
		if err != nil {
			return nil, err
		}
		dddpArray[index] = dddp
	}
	return dddpArray, nil
}

// MarshalJSON is the custom marshaler for DataDeletionDetectionPolicy.
func (dddp DataDeletionDetectionPolicy) MarshalJSON() ([]byte, error) {
	dddp.OdataType = OdataTypeDataDeletionDetectionPolicy
	objectMap := make(map[string]interface{})
	if dddp.OdataType != "" {
		objectMap["@odata.type"] = dddp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsSoftDeleteColumnDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for DataDeletionDetectionPolicy.
func (dddp DataDeletionDetectionPolicy) AsSoftDeleteColumnDeletionDetectionPolicy() (*SoftDeleteColumnDeletionDetectionPolicy, bool) {
	return nil, false
}

// AsDataDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for DataDeletionDetectionPolicy.
func (dddp DataDeletionDetectionPolicy) AsDataDeletionDetectionPolicy() (*DataDeletionDetectionPolicy, bool) {
	return &dddp, true
}

// AsBasicDataDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for DataDeletionDetectionPolicy.
func (dddp DataDeletionDetectionPolicy) AsBasicDataDeletionDetectionPolicy() (BasicDataDeletionDetectionPolicy, bool) {
	return &dddp, true
}

// DataSource represents a datasource definition, which can be used to configure an indexer.
type DataSource struct {
	autorest.Response `json:"-"`
	// Name - The name of the datasource.
	Name *string `json:"name,omitempty"`
	// Description - The description of the datasource.
	Description *string `json:"description,omitempty"`
	// Type - The type of the datasource. Possible values include: 'AzureSQL', 'CosmosDb', 'AzureBlob', 'AzureTable'
	Type DataSourceType `json:"type,omitempty"`
	// Credentials - Credentials for the datasource.
	Credentials *DataSourceCredentials `json:"credentials,omitempty"`
	// Container - The data container for the datasource.
	Container *DataContainer `json:"container,omitempty"`
	// DataChangeDetectionPolicy - The data change detection policy for the datasource.
	DataChangeDetectionPolicy BasicDataChangeDetectionPolicy `json:"dataChangeDetectionPolicy,omitempty"`
	// DataDeletionDetectionPolicy - The data deletion detection policy for the datasource.
	DataDeletionDetectionPolicy BasicDataDeletionDetectionPolicy `json:"dataDeletionDetectionPolicy,omitempty"`
	// ETag - The ETag of the DataSource.
	ETag *string `json:"@odata.etag,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for DataSource struct.
func (ds *DataSource) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				ds.Name = &name
			}
		case "description":
			if v != nil {
				var description string
				err = json.Unmarshal(*v, &description)
				if err != nil {
					return err
				}
				ds.Description = &description
			}
		case "type":
			if v != nil {
				var typeVar DataSourceType
				err = json.Unmarshal(*v, &typeVar)
				if err != nil {
					return err
				}
				ds.Type = typeVar
			}
		case "credentials":
			if v != nil {
				var credentials DataSourceCredentials
				err = json.Unmarshal(*v, &credentials)
				if err != nil {
					return err
				}
				ds.Credentials = &credentials
			}
		case "container":
			if v != nil {
				var containerVar DataContainer
				err = json.Unmarshal(*v, &containerVar)
				if err != nil {
					return err
				}
				ds.Container = &containerVar
			}
		case "dataChangeDetectionPolicy":
			if v != nil {
				dataChangeDetectionPolicy, err := unmarshalBasicDataChangeDetectionPolicy(*v)
				if err != nil {
					return err
				}
				ds.DataChangeDetectionPolicy = dataChangeDetectionPolicy
			}
		case "dataDeletionDetectionPolicy":
			if v != nil {
				dataDeletionDetectionPolicy, err := unmarshalBasicDataDeletionDetectionPolicy(*v)
				if err != nil {
					return err
				}
				ds.DataDeletionDetectionPolicy = dataDeletionDetectionPolicy
			}
		case "@odata.etag":
			if v != nil {
				var eTag string
				err = json.Unmarshal(*v, &eTag)
				if err != nil {
					return err
				}
				ds.ETag = &eTag
			}
		}
	}

	return nil
}

// DataSourceCredentials represents credentials that can be used to connect to a datasource.
type DataSourceCredentials struct {
	// ConnectionString - The connection string for the datasource.
	ConnectionString *string `json:"connectionString,omitempty"`
}

// DefaultCognitiveServicesAccount an empty object that represents the default cognitive service resource
// for a skillset.
type DefaultCognitiveServicesAccount struct {
	Description *string `json:"description,omitempty"`
	// OdataType - Possible values include: 'OdataTypeCognitiveServicesAccount', 'OdataTypeMicrosoftAzureSearchDefaultCognitiveServices', 'OdataTypeMicrosoftAzureSearchCognitiveServicesByKey'
	OdataType OdataTypeBasicCognitiveServicesAccount `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for DefaultCognitiveServicesAccount.
func (dcsa DefaultCognitiveServicesAccount) MarshalJSON() ([]byte, error) {
	dcsa.OdataType = OdataTypeMicrosoftAzureSearchDefaultCognitiveServices
	objectMap := make(map[string]interface{})
	if dcsa.Description != nil {
		objectMap["description"] = dcsa.Description
	}
	if dcsa.OdataType != "" {
		objectMap["@odata.type"] = dcsa.OdataType
	}
	return json.Marshal(objectMap)
}

// AsDefaultCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for DefaultCognitiveServicesAccount.
func (dcsa DefaultCognitiveServicesAccount) AsDefaultCognitiveServicesAccount() (*DefaultCognitiveServicesAccount, bool) {
	return &dcsa, true
}

// AsCognitiveServicesAccountKey is the BasicCognitiveServicesAccount implementation for DefaultCognitiveServicesAccount.
func (dcsa DefaultCognitiveServicesAccount) AsCognitiveServicesAccountKey() (*CognitiveServicesAccountKey, bool) {
	return nil, false
}

// AsCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for DefaultCognitiveServicesAccount.
func (dcsa DefaultCognitiveServicesAccount) AsCognitiveServicesAccount() (*CognitiveServicesAccount, bool) {
	return nil, false
}

// AsBasicCognitiveServicesAccount is the BasicCognitiveServicesAccount implementation for DefaultCognitiveServicesAccount.
func (dcsa DefaultCognitiveServicesAccount) AsBasicCognitiveServicesAccount() (BasicCognitiveServicesAccount, bool) {
	return &dcsa, true
}

// DictionaryDecompounderTokenFilter decomposes compound words found in many Germanic languages. This token
// filter is implemented using Apache Lucene.
type DictionaryDecompounderTokenFilter struct {
	// WordList - The list of words to match against.
	WordList *[]string `json:"wordList,omitempty"`
	// MinWordSize - The minimum word size. Only words longer than this get processed. Default is 5. Maximum is 300.
	MinWordSize *int32 `json:"minWordSize,omitempty"`
	// MinSubwordSize - The minimum subword size. Only subwords longer than this are outputted. Default is 2. Maximum is 300.
	MinSubwordSize *int32 `json:"minSubwordSize,omitempty"`
	// MaxSubwordSize - The maximum subword size. Only subwords shorter than this are outputted. Default is 15. Maximum is 300.
	MaxSubwordSize *int32 `json:"maxSubwordSize,omitempty"`
	// OnlyLongestMatch - A value indicating whether to add only the longest matching subword to the output. Default is false.
	OnlyLongestMatch *bool `json:"onlyLongestMatch,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) MarshalJSON() ([]byte, error) {
	ddtf.OdataType = OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter
	objectMap := make(map[string]interface{})
	if ddtf.WordList != nil {
		objectMap["wordList"] = ddtf.WordList
	}
	if ddtf.MinWordSize != nil {
		objectMap["minWordSize"] = ddtf.MinWordSize
	}
	if ddtf.MinSubwordSize != nil {
		objectMap["minSubwordSize"] = ddtf.MinSubwordSize
	}
	if ddtf.MaxSubwordSize != nil {
		objectMap["maxSubwordSize"] = ddtf.MaxSubwordSize
	}
	if ddtf.OnlyLongestMatch != nil {
		objectMap["onlyLongestMatch"] = ddtf.OnlyLongestMatch
	}
	if ddtf.Name != nil {
		objectMap["name"] = ddtf.Name
	}
	if ddtf.OdataType != "" {
		objectMap["@odata.type"] = ddtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return &ddtf, true
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for DictionaryDecompounderTokenFilter.
func (ddtf DictionaryDecompounderTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ddtf, true
}

// DistanceScoringFunction defines a function that boosts scores based on distance from a geographic
// location.
type DistanceScoringFunction struct {
	// Parameters - Parameter values for the distance scoring function.
	Parameters *DistanceScoringParameters `json:"distance,omitempty"`
	// FieldName - The name of the field used as input to the scoring function.
	FieldName *string `json:"fieldName,omitempty"`
	// Boost - A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64 `json:"boost,omitempty"`
	// Interpolation - A value indicating how boosting will be interpolated across document scores; defaults to "Linear". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'
	Interpolation ScoringFunctionInterpolation `json:"interpolation,omitempty"`
	// Type - Possible values include: 'TypeScoringFunction', 'TypeDistance', 'TypeFreshness', 'TypeMagnitude', 'TypeTag'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for DistanceScoringFunction.
func (dsf DistanceScoringFunction) MarshalJSON() ([]byte, error) {
	dsf.Type = TypeDistance
	objectMap := make(map[string]interface{})
	if dsf.Parameters != nil {
		objectMap["distance"] = dsf.Parameters
	}
	if dsf.FieldName != nil {
		objectMap["fieldName"] = dsf.FieldName
	}
	if dsf.Boost != nil {
		objectMap["boost"] = dsf.Boost
	}
	if dsf.Interpolation != "" {
		objectMap["interpolation"] = dsf.Interpolation
	}
	if dsf.Type != "" {
		objectMap["type"] = dsf.Type
	}
	return json.Marshal(objectMap)
}

// AsDistanceScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsDistanceScoringFunction() (*DistanceScoringFunction, bool) {
	return &dsf, true
}

// AsFreshnessScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool) {
	return nil, false
}

// AsMagnitudeScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool) {
	return nil, false
}

// AsTagScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsTagScoringFunction() (*TagScoringFunction, bool) {
	return nil, false
}

// AsScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsScoringFunction() (*ScoringFunction, bool) {
	return nil, false
}

// AsBasicScoringFunction is the BasicScoringFunction implementation for DistanceScoringFunction.
func (dsf DistanceScoringFunction) AsBasicScoringFunction() (BasicScoringFunction, bool) {
	return &dsf, true
}

// DistanceScoringParameters provides parameter values to a distance scoring function.
type DistanceScoringParameters struct {
	// ReferencePointParameter - The name of the parameter passed in search queries to specify the reference location.
	ReferencePointParameter *string `json:"referencePointParameter,omitempty"`
	// BoostingDistance - The distance in kilometers from the reference location where the boosting range ends.
	BoostingDistance *float64 `json:"boostingDistance,omitempty"`
}

// EdgeNGramTokenFilter generates n-grams of the given size(s) starting from the front or the back of an
// input token. This token filter is implemented using Apache Lucene.
type EdgeNGramTokenFilter struct {
	// MinGram - The minimum n-gram length. Default is 1. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// Side - Specifies which side of the input the n-gram should be generated from. Default is "front". Possible values include: 'Front', 'Back'
	Side EdgeNGramTokenFilterSide `json:"side,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) MarshalJSON() ([]byte, error) {
	engtf.OdataType = OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter
	objectMap := make(map[string]interface{})
	if engtf.MinGram != nil {
		objectMap["minGram"] = engtf.MinGram
	}
	if engtf.MaxGram != nil {
		objectMap["maxGram"] = engtf.MaxGram
	}
	if engtf.Side != "" {
		objectMap["side"] = engtf.Side
	}
	if engtf.Name != nil {
		objectMap["name"] = engtf.Name
	}
	if engtf.OdataType != "" {
		objectMap["@odata.type"] = engtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return &engtf, true
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilter.
func (engtf EdgeNGramTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &engtf, true
}

// EdgeNGramTokenFilterV2 generates n-grams of the given size(s) starting from the front or the back of an
// input token. This token filter is implemented using Apache Lucene.
type EdgeNGramTokenFilterV2 struct {
	// MinGram - The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// Side - Specifies which side of the input the n-gram should be generated from. Default is "front". Possible values include: 'Front', 'Back'
	Side EdgeNGramTokenFilterSide `json:"side,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) MarshalJSON() ([]byte, error) {
	engtfv.OdataType = OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2
	objectMap := make(map[string]interface{})
	if engtfv.MinGram != nil {
		objectMap["minGram"] = engtfv.MinGram
	}
	if engtfv.MaxGram != nil {
		objectMap["maxGram"] = engtfv.MaxGram
	}
	if engtfv.Side != "" {
		objectMap["side"] = engtfv.Side
	}
	if engtfv.Name != nil {
		objectMap["name"] = engtfv.Name
	}
	if engtfv.OdataType != "" {
		objectMap["@odata.type"] = engtfv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return &engtfv, true
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for EdgeNGramTokenFilterV2.
func (engtfv EdgeNGramTokenFilterV2) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &engtfv, true
}

// EdgeNGramTokenizer tokenizes the input from an edge into n-grams of the given size(s). This tokenizer is
// implemented using Apache Lucene.
type EdgeNGramTokenizer struct {
	// MinGram - The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// TokenChars - Character classes to keep in the tokens.
	TokenChars *[]TokenCharacterKind `json:"tokenChars,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) MarshalJSON() ([]byte, error) {
	engt.OdataType = OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer
	objectMap := make(map[string]interface{})
	if engt.MinGram != nil {
		objectMap["minGram"] = engt.MinGram
	}
	if engt.MaxGram != nil {
		objectMap["maxGram"] = engt.MaxGram
	}
	if engt.TokenChars != nil {
		objectMap["tokenChars"] = engt.TokenChars
	}
	if engt.Name != nil {
		objectMap["name"] = engt.Name
	}
	if engt.OdataType != "" {
		objectMap["@odata.type"] = engt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return &engt, true
}

// AsKeywordTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for EdgeNGramTokenizer.
func (engt EdgeNGramTokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &engt, true
}

// ElisionTokenFilter removes elisions. For example, "l'avion" (the plane) will be converted to "avion"
// (plane). This token filter is implemented using Apache Lucene.
type ElisionTokenFilter struct {
	// Articles - The set of articles to remove.
	Articles *[]string `json:"articles,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ElisionTokenFilter.
func (etf ElisionTokenFilter) MarshalJSON() ([]byte, error) {
	etf.OdataType = OdataTypeMicrosoftAzureSearchElisionTokenFilter
	objectMap := make(map[string]interface{})
	if etf.Articles != nil {
		objectMap["articles"] = etf.Articles
	}
	if etf.Name != nil {
		objectMap["name"] = etf.Name
	}
	if etf.OdataType != "" {
		objectMap["@odata.type"] = etf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return &etf, true
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for ElisionTokenFilter.
func (etf ElisionTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &etf, true
}

// EntityRecognitionSkill text analytics entity recognition.
type EntityRecognitionSkill struct {
	// Categories - A list of entity categories that should be extracted.
	Categories *[]EntityCategory `json:"categories,omitempty"`
	// DefaultLanguageCode - A value indicating which language code to use. Default is en. Possible values include: 'Ar', 'Cs', 'ZhHans', 'ZhHant', 'Da', 'Nl', 'En', 'Fi', 'Fr', 'De', 'El', 'Hu', 'It', 'Ja', 'Ko', 'No', 'Pl', 'PtPT', 'PtBR', 'Ru', 'Es', 'Sv', 'Tr'
	DefaultLanguageCode EntityRecognitionSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// IncludeTypelessEntities - Determines whether or not to include entities which are well known but don't conform to a pre-defined type. If this configuration is not set (default), set to null or set to false, entities which don't conform to one of the pre-defined types will not be surfaced.
	IncludeTypelessEntities *bool `json:"includeTypelessEntities,omitempty"`
	// MinimumPrecision - A value between 0 and 1 that be used to only include entities whose confidence score is greater than the value specified. If not set (default), or if explicitly set to null, all entities will be included.
	MinimumPrecision *float64 `json:"minimumPrecision,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) MarshalJSON() ([]byte, error) {
	ers.OdataType = OdataTypeMicrosoftSkillsTextEntityRecognitionSkill
	objectMap := make(map[string]interface{})
	if ers.Categories != nil {
		objectMap["categories"] = ers.Categories
	}
	if ers.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = ers.DefaultLanguageCode
	}
	if ers.IncludeTypelessEntities != nil {
		objectMap["includeTypelessEntities"] = ers.IncludeTypelessEntities
	}
	if ers.MinimumPrecision != nil {
		objectMap["minimumPrecision"] = ers.MinimumPrecision
	}
	if ers.Name != nil {
		objectMap["name"] = ers.Name
	}
	if ers.Description != nil {
		objectMap["description"] = ers.Description
	}
	if ers.Context != nil {
		objectMap["context"] = ers.Context
	}
	if ers.Inputs != nil {
		objectMap["inputs"] = ers.Inputs
	}
	if ers.Outputs != nil {
		objectMap["outputs"] = ers.Outputs
	}
	if ers.OdataType != "" {
		objectMap["@odata.type"] = ers.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return &ers, true
}

// AsSentimentSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for EntityRecognitionSkill.
func (ers EntityRecognitionSkill) AsBasicSkill() (BasicSkill, bool) {
	return &ers, true
}

// Field represents a field in an index definition, which describes the name, data type, and search
// behavior of a field.
type Field struct {
	// Name - The name of the field, which must be unique within the fields collection of the index or parent field.
	Name *string `json:"name,omitempty"`
	// Type - The data type of the field. Possible values include: 'EdmString', 'EdmInt32', 'EdmInt64', 'EdmDouble', 'EdmBoolean', 'EdmDateTimeOffset', 'EdmGeographyPoint', 'EdmComplexType'
	Type DataType `json:"type,omitempty"`
	// Key - A value indicating whether the field uniquely identifies documents in the index. Exactly one top-level field in each index must be chosen as the key field and it must be of type Edm.String. Key fields can be used to look up documents directly and update or delete specific documents. Default is false for simple fields and null for complex fields.
	Key *bool `json:"key,omitempty"`
	// Retrievable - A value indicating whether the field can be returned in a search result. You can disable this option if you want to use a field (for example, margin) as a filter, sorting, or scoring mechanism but do not want the field to be visible to the end user. This property must be true for key fields, and it must be null for complex fields. This property can be changed on existing fields. Enabling this property does not cause any increase in index storage requirements. Default is true for simple fields and null for complex fields.
	Retrievable *bool `json:"retrievable,omitempty"`
	// Searchable - A value indicating whether the field is full-text searchable. This means it will undergo analysis such as word-breaking during indexing. If you set a searchable field to a value like "sunny day", internally it will be split into the individual tokens "sunny" and "day". This enables full-text searches for these terms. Fields of type Edm.String or Collection(Edm.String) are searchable by default. This property must be false for simple fields of other non-string data types, and it must be null for complex fields. Note: searchable fields consume extra space in your index since Azure Cognitive Search will store an additional tokenized version of the field value for full-text searches. If you want to save space in your index and you don't need a field to be included in searches, set searchable to false.
	Searchable *bool `json:"searchable,omitempty"`
	// Filterable - A value indicating whether to enable the field to be referenced in $filter queries. filterable differs from searchable in how strings are handled. Fields of type Edm.String or Collection(Edm.String) that are filterable do not undergo word-breaking, so comparisons are for exact matches only. For example, if you set such a field f to "sunny day", $filter=f eq 'sunny' will find no matches, but $filter=f eq 'sunny day' will. This property must be null for complex fields. Default is true for simple fields and null for complex fields.
	Filterable *bool `json:"filterable,omitempty"`
	// Sortable - A value indicating whether to enable the field to be referenced in $orderby expressions. By default Azure Cognitive Search sorts results by score, but in many experiences users will want to sort by fields in the documents. A simple field can be sortable only if it is single-valued (it has a single value in the scope of the parent document). Simple collection fields cannot be sortable, since they are multi-valued. Simple sub-fields of complex collections are also multi-valued, and therefore cannot be sortable. This is true whether it's an immediate parent field, or an ancestor field, that's the complex collection. Complex fields cannot be sortable and the sortable property must be null for such fields. The default for sortable is true for single-valued simple fields, false for multi-valued simple fields, and null for complex fields.
	Sortable *bool `json:"sortable,omitempty"`
	// Facetable - A value indicating whether to enable the field to be referenced in facet queries. Typically used in a presentation of search results that includes hit count by category (for example, search for digital cameras and see hits by brand, by megapixels, by price, and so on). This property must be null for complex fields. Fields of type Edm.GeographyPoint or Collection(Edm.GeographyPoint) cannot be facetable. Default is true for all other simple fields.
	Facetable *bool `json:"facetable,omitempty"`
	// Analyzer - The name of the language analyzer to use for the field. This option can be used only with searchable fields and it can't be set together with either searchAnalyzer or indexAnalyzer. Once the analyzer is chosen, it cannot be changed for the field. Must be null for complex fields. Possible values include: 'Armicrosoft', 'Arlucene', 'Hylucene', 'Bnmicrosoft', 'Eulucene', 'Bgmicrosoft', 'Bglucene', 'Camicrosoft', 'Calucene', 'ZhHansmicrosoft', 'ZhHanslucene', 'ZhHantmicrosoft', 'ZhHantlucene', 'Hrmicrosoft', 'Csmicrosoft', 'Cslucene', 'Damicrosoft', 'Dalucene', 'Nlmicrosoft', 'Nllucene', 'Enmicrosoft', 'Enlucene', 'Etmicrosoft', 'Fimicrosoft', 'Filucene', 'Frmicrosoft', 'Frlucene', 'Gllucene', 'Demicrosoft', 'Delucene', 'Elmicrosoft', 'Ellucene', 'Gumicrosoft', 'Hemicrosoft', 'Himicrosoft', 'Hilucene', 'Humicrosoft', 'Hulucene', 'Ismicrosoft', 'Idmicrosoft', 'Idlucene', 'Galucene', 'Itmicrosoft', 'Itlucene', 'Jamicrosoft', 'Jalucene', 'Knmicrosoft', 'Komicrosoft', 'Kolucene', 'Lvmicrosoft', 'Lvlucene', 'Ltmicrosoft', 'Mlmicrosoft', 'Msmicrosoft', 'Mrmicrosoft', 'Nbmicrosoft', 'Nolucene', 'Falucene', 'Plmicrosoft', 'Pllucene', 'PtBRmicrosoft', 'PtBRlucene', 'PtPTmicrosoft', 'PtPTlucene', 'Pamicrosoft', 'Romicrosoft', 'Rolucene', 'Rumicrosoft', 'Rulucene', 'SrCyrillicmicrosoft', 'SrLatinmicrosoft', 'Skmicrosoft', 'Slmicrosoft', 'Esmicrosoft', 'Eslucene', 'Svmicrosoft', 'Svlucene', 'Tamicrosoft', 'Temicrosoft', 'Thmicrosoft', 'Thlucene', 'Trmicrosoft', 'Trlucene', 'Ukmicrosoft', 'Urmicrosoft', 'Vimicrosoft', 'Standardlucene', 'Standardasciifoldinglucene', 'Keyword', 'Pattern', 'Simple', 'Stop', 'Whitespace'
	Analyzer AnalyzerName `json:"analyzer,omitempty"`
	// SearchAnalyzer - The name of the analyzer used at search time for the field. This option can be used only with searchable fields. It must be set together with indexAnalyzer and it cannot be set together with the analyzer option. This analyzer can be updated on an existing field. Must be null for complex fields. Possible values include: 'Armicrosoft', 'Arlucene', 'Hylucene', 'Bnmicrosoft', 'Eulucene', 'Bgmicrosoft', 'Bglucene', 'Camicrosoft', 'Calucene', 'ZhHansmicrosoft', 'ZhHanslucene', 'ZhHantmicrosoft', 'ZhHantlucene', 'Hrmicrosoft', 'Csmicrosoft', 'Cslucene', 'Damicrosoft', 'Dalucene', 'Nlmicrosoft', 'Nllucene', 'Enmicrosoft', 'Enlucene', 'Etmicrosoft', 'Fimicrosoft', 'Filucene', 'Frmicrosoft', 'Frlucene', 'Gllucene', 'Demicrosoft', 'Delucene', 'Elmicrosoft', 'Ellucene', 'Gumicrosoft', 'Hemicrosoft', 'Himicrosoft', 'Hilucene', 'Humicrosoft', 'Hulucene', 'Ismicrosoft', 'Idmicrosoft', 'Idlucene', 'Galucene', 'Itmicrosoft', 'Itlucene', 'Jamicrosoft', 'Jalucene', 'Knmicrosoft', 'Komicrosoft', 'Kolucene', 'Lvmicrosoft', 'Lvlucene', 'Ltmicrosoft', 'Mlmicrosoft', 'Msmicrosoft', 'Mrmicrosoft', 'Nbmicrosoft', 'Nolucene', 'Falucene', 'Plmicrosoft', 'Pllucene', 'PtBRmicrosoft', 'PtBRlucene', 'PtPTmicrosoft', 'PtPTlucene', 'Pamicrosoft', 'Romicrosoft', 'Rolucene', 'Rumicrosoft', 'Rulucene', 'SrCyrillicmicrosoft', 'SrLatinmicrosoft', 'Skmicrosoft', 'Slmicrosoft', 'Esmicrosoft', 'Eslucene', 'Svmicrosoft', 'Svlucene', 'Tamicrosoft', 'Temicrosoft', 'Thmicrosoft', 'Thlucene', 'Trmicrosoft', 'Trlucene', 'Ukmicrosoft', 'Urmicrosoft', 'Vimicrosoft', 'Standardlucene', 'Standardasciifoldinglucene', 'Keyword', 'Pattern', 'Simple', 'Stop', 'Whitespace'
	SearchAnalyzer AnalyzerName `json:"searchAnalyzer,omitempty"`
	// IndexAnalyzer - The name of the analyzer used at indexing time for the field. This option can be used only with searchable fields. It must be set together with searchAnalyzer and it cannot be set together with the analyzer option. Once the analyzer is chosen, it cannot be changed for the field. Must be null for complex fields. Possible values include: 'Armicrosoft', 'Arlucene', 'Hylucene', 'Bnmicrosoft', 'Eulucene', 'Bgmicrosoft', 'Bglucene', 'Camicrosoft', 'Calucene', 'ZhHansmicrosoft', 'ZhHanslucene', 'ZhHantmicrosoft', 'ZhHantlucene', 'Hrmicrosoft', 'Csmicrosoft', 'Cslucene', 'Damicrosoft', 'Dalucene', 'Nlmicrosoft', 'Nllucene', 'Enmicrosoft', 'Enlucene', 'Etmicrosoft', 'Fimicrosoft', 'Filucene', 'Frmicrosoft', 'Frlucene', 'Gllucene', 'Demicrosoft', 'Delucene', 'Elmicrosoft', 'Ellucene', 'Gumicrosoft', 'Hemicrosoft', 'Himicrosoft', 'Hilucene', 'Humicrosoft', 'Hulucene', 'Ismicrosoft', 'Idmicrosoft', 'Idlucene', 'Galucene', 'Itmicrosoft', 'Itlucene', 'Jamicrosoft', 'Jalucene', 'Knmicrosoft', 'Komicrosoft', 'Kolucene', 'Lvmicrosoft', 'Lvlucene', 'Ltmicrosoft', 'Mlmicrosoft', 'Msmicrosoft', 'Mrmicrosoft', 'Nbmicrosoft', 'Nolucene', 'Falucene', 'Plmicrosoft', 'Pllucene', 'PtBRmicrosoft', 'PtBRlucene', 'PtPTmicrosoft', 'PtPTlucene', 'Pamicrosoft', 'Romicrosoft', 'Rolucene', 'Rumicrosoft', 'Rulucene', 'SrCyrillicmicrosoft', 'SrLatinmicrosoft', 'Skmicrosoft', 'Slmicrosoft', 'Esmicrosoft', 'Eslucene', 'Svmicrosoft', 'Svlucene', 'Tamicrosoft', 'Temicrosoft', 'Thmicrosoft', 'Thlucene', 'Trmicrosoft', 'Trlucene', 'Ukmicrosoft', 'Urmicrosoft', 'Vimicrosoft', 'Standardlucene', 'Standardasciifoldinglucene', 'Keyword', 'Pattern', 'Simple', 'Stop', 'Whitespace'
	IndexAnalyzer AnalyzerName `json:"indexAnalyzer,omitempty"`
	// SynonymMaps - A list of the names of synonym maps to associate with this field. This option can be used only with searchable fields. Currently only one synonym map per field is supported. Assigning a synonym map to a field ensures that query terms targeting that field are expanded at query-time using the rules in the synonym map. This attribute can be changed on existing fields. Must be null or an empty collection for complex fields.
	SynonymMaps *[]string `json:"synonymMaps,omitempty"`
	// Fields - A list of sub-fields if this is a field of type Edm.ComplexType or Collection(Edm.ComplexType). Must be null or empty for simple fields.
	Fields *[]Field `json:"fields,omitempty"`
}

// FieldMapping defines a mapping between a field in a data source and a target field in an index.
type FieldMapping struct {
	// SourceFieldName - The name of the field in the data source.
	SourceFieldName *string `json:"sourceFieldName,omitempty"`
	// TargetFieldName - The name of the target field in the index. Same as the source field name by default.
	TargetFieldName *string `json:"targetFieldName,omitempty"`
	// MappingFunction - A function to apply to each source field value before indexing.
	MappingFunction *FieldMappingFunction `json:"mappingFunction,omitempty"`
}

// FieldMappingFunction represents a function that transforms a value from a data source before indexing.
type FieldMappingFunction struct {
	// Name - The name of the field mapping function.
	Name *string `json:"name,omitempty"`
	// Parameters - A dictionary of parameter name/value pairs to pass to the function. Each value must be of a primitive type.
	Parameters map[string]interface{} `json:"parameters"`
}

// MarshalJSON is the custom marshaler for FieldMappingFunction.
func (fmf FieldMappingFunction) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if fmf.Name != nil {
		objectMap["name"] = fmf.Name
	}
	if fmf.Parameters != nil {
		objectMap["parameters"] = fmf.Parameters
	}
	return json.Marshal(objectMap)
}

// FreshnessScoringFunction defines a function that boosts scores based on the value of a date-time field.
type FreshnessScoringFunction struct {
	// Parameters - Parameter values for the freshness scoring function.
	Parameters *FreshnessScoringParameters `json:"freshness,omitempty"`
	// FieldName - The name of the field used as input to the scoring function.
	FieldName *string `json:"fieldName,omitempty"`
	// Boost - A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64 `json:"boost,omitempty"`
	// Interpolation - A value indicating how boosting will be interpolated across document scores; defaults to "Linear". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'
	Interpolation ScoringFunctionInterpolation `json:"interpolation,omitempty"`
	// Type - Possible values include: 'TypeScoringFunction', 'TypeDistance', 'TypeFreshness', 'TypeMagnitude', 'TypeTag'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) MarshalJSON() ([]byte, error) {
	fsf.Type = TypeFreshness
	objectMap := make(map[string]interface{})
	if fsf.Parameters != nil {
		objectMap["freshness"] = fsf.Parameters
	}
	if fsf.FieldName != nil {
		objectMap["fieldName"] = fsf.FieldName
	}
	if fsf.Boost != nil {
		objectMap["boost"] = fsf.Boost
	}
	if fsf.Interpolation != "" {
		objectMap["interpolation"] = fsf.Interpolation
	}
	if fsf.Type != "" {
		objectMap["type"] = fsf.Type
	}
	return json.Marshal(objectMap)
}

// AsDistanceScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsDistanceScoringFunction() (*DistanceScoringFunction, bool) {
	return nil, false
}

// AsFreshnessScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool) {
	return &fsf, true
}

// AsMagnitudeScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool) {
	return nil, false
}

// AsTagScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsTagScoringFunction() (*TagScoringFunction, bool) {
	return nil, false
}

// AsScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsScoringFunction() (*ScoringFunction, bool) {
	return nil, false
}

// AsBasicScoringFunction is the BasicScoringFunction implementation for FreshnessScoringFunction.
func (fsf FreshnessScoringFunction) AsBasicScoringFunction() (BasicScoringFunction, bool) {
	return &fsf, true
}

// FreshnessScoringParameters provides parameter values to a freshness scoring function.
type FreshnessScoringParameters struct {
	// BoostingDuration - The expiration period after which boosting will stop for a particular document.
	BoostingDuration *string `json:"boostingDuration,omitempty"`
}

// GetIndexStatisticsResult statistics for a given index. Statistics are collected periodically and are not
// guaranteed to always be up-to-date.
type GetIndexStatisticsResult struct {
	autorest.Response `json:"-"`
	// DocumentCount - READ-ONLY; The number of documents in the index.
	DocumentCount *int64 `json:"documentCount,omitempty"`
	// StorageSize - READ-ONLY; The amount of storage in bytes consumed by the index.
	StorageSize *int64 `json:"storageSize,omitempty"`
}

// MarshalJSON is the custom marshaler for GetIndexStatisticsResult.
func (gisr GetIndexStatisticsResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// HighWaterMarkChangeDetectionPolicy defines a data change detection policy that captures changes based on
// the value of a high water mark column.
type HighWaterMarkChangeDetectionPolicy struct {
	// HighWaterMarkColumnName - The name of the high water mark column.
	HighWaterMarkColumnName *string `json:"highWaterMarkColumnName,omitempty"`
	// OdataType - Possible values include: 'OdataTypeDataChangeDetectionPolicy', 'OdataTypeMicrosoftAzureSearchHighWaterMarkChangeDetectionPolicy', 'OdataTypeMicrosoftAzureSearchSQLIntegratedChangeTrackingPolicy'
	OdataType OdataTypeBasicDataChangeDetectionPolicy `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for HighWaterMarkChangeDetectionPolicy.
func (hwmcdp HighWaterMarkChangeDetectionPolicy) MarshalJSON() ([]byte, error) {
	hwmcdp.OdataType = OdataTypeMicrosoftAzureSearchHighWaterMarkChangeDetectionPolicy
	objectMap := make(map[string]interface{})
	if hwmcdp.HighWaterMarkColumnName != nil {
		objectMap["highWaterMarkColumnName"] = hwmcdp.HighWaterMarkColumnName
	}
	if hwmcdp.OdataType != "" {
		objectMap["@odata.type"] = hwmcdp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsHighWaterMarkChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for HighWaterMarkChangeDetectionPolicy.
func (hwmcdp HighWaterMarkChangeDetectionPolicy) AsHighWaterMarkChangeDetectionPolicy() (*HighWaterMarkChangeDetectionPolicy, bool) {
	return &hwmcdp, true
}

// AsSQLIntegratedChangeTrackingPolicy is the BasicDataChangeDetectionPolicy implementation for HighWaterMarkChangeDetectionPolicy.
func (hwmcdp HighWaterMarkChangeDetectionPolicy) AsSQLIntegratedChangeTrackingPolicy() (*SQLIntegratedChangeTrackingPolicy, bool) {
	return nil, false
}

// AsDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for HighWaterMarkChangeDetectionPolicy.
func (hwmcdp HighWaterMarkChangeDetectionPolicy) AsDataChangeDetectionPolicy() (*DataChangeDetectionPolicy, bool) {
	return nil, false
}

// AsBasicDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for HighWaterMarkChangeDetectionPolicy.
func (hwmcdp HighWaterMarkChangeDetectionPolicy) AsBasicDataChangeDetectionPolicy() (BasicDataChangeDetectionPolicy, bool) {
	return &hwmcdp, true
}

// ImageAnalysisSkill a skill that analyzes image files. It extracts a rich set of visual features based on
// the image content.
type ImageAnalysisSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is en. Possible values include: 'ImageAnalysisSkillLanguageEn', 'ImageAnalysisSkillLanguageEs', 'ImageAnalysisSkillLanguageJa', 'ImageAnalysisSkillLanguagePt', 'ImageAnalysisSkillLanguageZh'
	DefaultLanguageCode ImageAnalysisSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// VisualFeatures - A list of visual features.
	VisualFeatures *[]VisualFeature `json:"visualFeatures,omitempty"`
	// Details - A string indicating which domain-specific details to return.
	Details *[]ImageDetail `json:"details,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) MarshalJSON() ([]byte, error) {
	ias.OdataType = OdataTypeMicrosoftSkillsVisionImageAnalysisSkill
	objectMap := make(map[string]interface{})
	if ias.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = ias.DefaultLanguageCode
	}
	if ias.VisualFeatures != nil {
		objectMap["visualFeatures"] = ias.VisualFeatures
	}
	if ias.Details != nil {
		objectMap["details"] = ias.Details
	}
	if ias.Name != nil {
		objectMap["name"] = ias.Name
	}
	if ias.Description != nil {
		objectMap["description"] = ias.Description
	}
	if ias.Context != nil {
		objectMap["context"] = ias.Context
	}
	if ias.Inputs != nil {
		objectMap["inputs"] = ias.Inputs
	}
	if ias.Outputs != nil {
		objectMap["outputs"] = ias.Outputs
	}
	if ias.OdataType != "" {
		objectMap["@odata.type"] = ias.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return &ias, true
}

// AsLanguageDetectionSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for ImageAnalysisSkill.
func (ias ImageAnalysisSkill) AsBasicSkill() (BasicSkill, bool) {
	return &ias, true
}

// Index represents a search index definition, which describes the fields and search behavior of an index.
type Index struct {
	autorest.Response `json:"-"`
	// Name - The name of the index.
	Name *string `json:"name,omitempty"`
	// Fields - The fields of the index.
	Fields *[]Field `json:"fields,omitempty"`
	// ScoringProfiles - The scoring profiles for the index.
	ScoringProfiles *[]ScoringProfile `json:"scoringProfiles,omitempty"`
	// DefaultScoringProfile - The name of the scoring profile to use if none is specified in the query. If this property is not set and no scoring profile is specified in the query, then default scoring (tf-idf) will be used.
	DefaultScoringProfile *string `json:"defaultScoringProfile,omitempty"`
	// CorsOptions - Options to control Cross-Origin Resource Sharing (CORS) for the index.
	CorsOptions *CorsOptions `json:"corsOptions,omitempty"`
	// Suggesters - The suggesters for the index.
	Suggesters *[]Suggester `json:"suggesters,omitempty"`
	// Analyzers - The analyzers for the index.
	Analyzers *[]BasicAnalyzer `json:"analyzers,omitempty"`
	// Tokenizers - The tokenizers for the index.
	Tokenizers *[]BasicTokenizer `json:"tokenizers,omitempty"`
	// TokenFilters - The token filters for the index.
	TokenFilters *[]BasicTokenFilter `json:"tokenFilters,omitempty"`
	// CharFilters - The character filters for the index.
	CharFilters *[]BasicCharFilter `json:"charFilters,omitempty"`
	// ETag - The ETag of the index.
	ETag *string `json:"@odata.etag,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for Index struct.
func (i *Index) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				i.Name = &name
			}
		case "fields":
			if v != nil {
				var fields []Field
				err = json.Unmarshal(*v, &fields)
				if err != nil {
					return err
				}
				i.Fields = &fields
			}
		case "scoringProfiles":
			if v != nil {
				var scoringProfiles []ScoringProfile
				err = json.Unmarshal(*v, &scoringProfiles)
				if err != nil {
					return err
				}
				i.ScoringProfiles = &scoringProfiles
			}
		case "defaultScoringProfile":
			if v != nil {
				var defaultScoringProfile string
				err = json.Unmarshal(*v, &defaultScoringProfile)
				if err != nil {
					return err
				}
				i.DefaultScoringProfile = &defaultScoringProfile
			}
		case "corsOptions":
			if v != nil {
				var corsOptions CorsOptions
				err = json.Unmarshal(*v, &corsOptions)
				if err != nil {
					return err
				}
				i.CorsOptions = &corsOptions
			}
		case "suggesters":
			if v != nil {
				var suggesters []Suggester
				err = json.Unmarshal(*v, &suggesters)
				if err != nil {
					return err
				}
				i.Suggesters = &suggesters
			}
		case "analyzers":
			if v != nil {
				analyzers, err := unmarshalBasicAnalyzerArray(*v)
				if err != nil {
					return err
				}
				i.Analyzers = &analyzers
			}
		case "tokenizers":
			if v != nil {
				tokenizers, err := unmarshalBasicTokenizerArray(*v)
				if err != nil {
					return err
				}
				i.Tokenizers = &tokenizers
			}
		case "tokenFilters":
			if v != nil {
				tokenFilters, err := unmarshalBasicTokenFilterArray(*v)
				if err != nil {
					return err
				}
				i.TokenFilters = &tokenFilters
			}
		case "charFilters":
			if v != nil {
				charFilters, err := unmarshalBasicCharFilterArray(*v)
				if err != nil {
					return err
				}
				i.CharFilters = &charFilters
			}
		case "@odata.etag":
			if v != nil {
				var eTag string
				err = json.Unmarshal(*v, &eTag)
				if err != nil {
					return err
				}
				i.ETag = &eTag
			}
		}
	}

	return nil
}

// Indexer represents an indexer.
type Indexer struct {
	autorest.Response `json:"-"`
	// Name - The name of the indexer.
	Name *string `json:"name,omitempty"`
	// Description - The description of the indexer.
	Description *string `json:"description,omitempty"`
	// DataSourceName - The name of the datasource from which this indexer reads data.
	DataSourceName *string `json:"dataSourceName,omitempty"`
	// SkillsetName - The name of the skillset executing with this indexer.
	SkillsetName *string `json:"skillsetName,omitempty"`
	// TargetIndexName - The name of the index to which this indexer writes data.
	TargetIndexName *string `json:"targetIndexName,omitempty"`
	// Schedule - The schedule for this indexer.
	Schedule *IndexingSchedule `json:"schedule,omitempty"`
	// Parameters - Parameters for indexer execution.
	Parameters *IndexingParameters `json:"parameters,omitempty"`
	// FieldMappings - Defines mappings between fields in the data source and corresponding target fields in the index.
	FieldMappings *[]FieldMapping `json:"fieldMappings,omitempty"`
	// OutputFieldMappings - Output field mappings are applied after enrichment and immediately before indexing.
	OutputFieldMappings *[]FieldMapping `json:"outputFieldMappings,omitempty"`
	// IsDisabled - A value indicating whether the indexer is disabled. Default is false.
	IsDisabled *bool `json:"disabled,omitempty"`
	// ETag - The ETag of the Indexer.
	ETag *string `json:"@odata.etag,omitempty"`
}

// IndexerExecutionInfo represents the current status and execution history of an indexer.
type IndexerExecutionInfo struct {
	autorest.Response `json:"-"`
	// Status - READ-ONLY; Overall indexer status. Possible values include: 'Unknown', 'Error', 'Running'
	Status IndexerStatus `json:"status,omitempty"`
	// LastResult - READ-ONLY; The result of the most recent or an in-progress indexer execution.
	LastResult *IndexerExecutionResult `json:"lastResult,omitempty"`
	// ExecutionHistory - READ-ONLY; History of the recent indexer executions, sorted in reverse chronological order.
	ExecutionHistory *[]IndexerExecutionResult `json:"executionHistory,omitempty"`
	// Limits - READ-ONLY; The execution limits for the indexer.
	Limits *IndexerLimits `json:"limits,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerExecutionInfo.
func (iei IndexerExecutionInfo) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// IndexerExecutionResult represents the result of an individual indexer execution.
type IndexerExecutionResult struct {
	// Status - READ-ONLY; The outcome of this indexer execution. Possible values include: 'TransientFailure', 'Success', 'InProgress', 'Reset'
	Status IndexerExecutionStatus `json:"status,omitempty"`
	// ErrorMessage - READ-ONLY; The error message indicating the top-level error, if any.
	ErrorMessage *string `json:"errorMessage,omitempty"`
	// StartTime - READ-ONLY; The start time of this indexer execution.
	StartTime *date.Time `json:"startTime,omitempty"`
	// EndTime - READ-ONLY; The end time of this indexer execution, if the execution has already completed.
	EndTime *date.Time `json:"endTime,omitempty"`
	// Errors - READ-ONLY; The item-level indexing errors.
	Errors *[]ItemError `json:"errors,omitempty"`
	// Warnings - READ-ONLY; The item-level indexing warnings.
	Warnings *[]ItemWarning `json:"warnings,omitempty"`
	// ItemCount - READ-ONLY; The number of items that were processed during this indexer execution. This includes both successfully processed items and items where indexing was attempted but failed.
	ItemCount *int32 `json:"itemsProcessed,omitempty"`
	// FailedItemCount - READ-ONLY; The number of items that failed to be indexed during this indexer execution.
	FailedItemCount *int32 `json:"itemsFailed,omitempty"`
	// InitialTrackingState - READ-ONLY; Change tracking state with which an indexer execution started.
	InitialTrackingState *string `json:"initialTrackingState,omitempty"`
	// FinalTrackingState - READ-ONLY; Change tracking state with which an indexer execution finished.
	FinalTrackingState *string `json:"finalTrackingState,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerExecutionResult.
func (ier IndexerExecutionResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// IndexerLimits ...
type IndexerLimits struct {
	// MaxRunTime - READ-ONLY; The maximum duration that the indexer is permitted to run for one execution.
	MaxRunTime *string `json:"maxRunTime,omitempty"`
	// MaxDocumentExtractionSize - READ-ONLY; The maximum size of a document, in bytes, which will be considered valid for indexing.
	MaxDocumentExtractionSize *float64 `json:"maxDocumentExtractionSize,omitempty"`
	// MaxDocumentContentCharactersToExtract - READ-ONLY; The maximum number of characters that will be extracted from a document picked up for indexing.
	MaxDocumentContentCharactersToExtract *float64 `json:"maxDocumentContentCharactersToExtract,omitempty"`
}

// MarshalJSON is the custom marshaler for IndexerLimits.
func (il IndexerLimits) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// IndexingParameters represents parameters for indexer execution.
type IndexingParameters struct {
	// BatchSize - The number of items that are read from the data source and indexed as a single batch in order to improve performance. The default depends on the data source type.
	BatchSize *int32 `json:"batchSize,omitempty"`
	// MaxFailedItems - The maximum number of items that can fail indexing for indexer execution to still be considered successful. -1 means no limit. Default is 0.
	MaxFailedItems *int32 `json:"maxFailedItems,omitempty"`
	// MaxFailedItemsPerBatch - The maximum number of items in a single batch that can fail indexing for the batch to still be considered successful. -1 means no limit. Default is 0.
	MaxFailedItemsPerBatch *int32 `json:"maxFailedItemsPerBatch,omitempty"`
	// Base64EncodeKeys - Whether indexer will base64-encode all values that are inserted into key field of the target index. This is needed if keys can contain characters that are invalid in keys (such as dot '.'). Default is false.
	Base64EncodeKeys *bool `json:"base64EncodeKeys,omitempty"`
	// Configuration - A dictionary of indexer-specific configuration properties. Each name is the name of a specific property. Each value must be of a primitive type.
	Configuration map[string]interface{} `json:"configuration"`
}

// MarshalJSON is the custom marshaler for IndexingParameters.
func (IP IndexingParameters) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if IP.BatchSize != nil {
		objectMap["batchSize"] = IP.BatchSize
	}
	if IP.MaxFailedItems != nil {
		objectMap["maxFailedItems"] = IP.MaxFailedItems
	}
	if IP.MaxFailedItemsPerBatch != nil {
		objectMap["maxFailedItemsPerBatch"] = IP.MaxFailedItemsPerBatch
	}
	if IP.Base64EncodeKeys != nil {
		objectMap["base64EncodeKeys"] = IP.Base64EncodeKeys
	}
	if IP.Configuration != nil {
		objectMap["configuration"] = IP.Configuration
	}
	return json.Marshal(objectMap)
}

// IndexingSchedule represents a schedule for indexer execution.
type IndexingSchedule struct {
	// Interval - The interval of time between indexer executions.
	Interval *string `json:"interval,omitempty"`
	// StartTime - The time when an indexer should start running.
	StartTime *date.Time `json:"startTime,omitempty"`
}

// InputFieldMappingEntry input field mapping for a skill.
type InputFieldMappingEntry struct {
	// Name - The name of the input.
	Name *string `json:"name,omitempty"`
	// Source - The source of the input.
	Source *string `json:"source,omitempty"`
	// SourceContext - The source context used for selecting recursive inputs.
	SourceContext *string `json:"sourceContext,omitempty"`
	// Inputs - The recursive inputs used when creating a complex type.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
}

// ItemError represents an item- or document-level indexing error.
type ItemError struct {
	// Key - READ-ONLY; The key of the item for which indexing failed.
	Key *string `json:"key,omitempty"`
	// ErrorMessage - READ-ONLY; The message describing the error that occurred while processing the item.
	ErrorMessage *string `json:"errorMessage,omitempty"`
	// StatusCode - READ-ONLY; The status code indicating why the indexing operation failed. Possible values include: 400 for a malformed input document, 404 for document not found, 409 for a version conflict, 422 when the index is temporarily unavailable, or 503 for when the service is too busy.
	StatusCode *int32 `json:"statusCode,omitempty"`
	// Name - READ-ONLY; The name of the source at which the error originated. For example, this could refer to a particular skill in the attached skillset. This may not be always available.
	Name *string `json:"name,omitempty"`
	// Details - READ-ONLY; Additional, verbose details about the error to assist in debugging the indexer. This may not be always available.
	Details *string `json:"details,omitempty"`
	// DocumentationLink - READ-ONLY; A link to a troubleshooting guide for these classes of errors. This may not be always available.
	DocumentationLink *string `json:"documentationLink,omitempty"`
}

// MarshalJSON is the custom marshaler for ItemError.
func (ie ItemError) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ItemWarning represents an item-level warning.
type ItemWarning struct {
	// Key - READ-ONLY; The key of the item which generated a warning.
	Key *string `json:"key,omitempty"`
	// Message - READ-ONLY; The message describing the warning that occurred while processing the item.
	Message *string `json:"message,omitempty"`
	// Name - READ-ONLY; The name of the source at which the warning originated. For example, this could refer to a particular skill in the attached skillset. This may not be always available.
	Name *string `json:"name,omitempty"`
	// Details - READ-ONLY; Additional, verbose details about the warning to assist in debugging the indexer. This may not be always available.
	Details *string `json:"details,omitempty"`
	// DocumentationLink - READ-ONLY; A link to a troubleshooting guide for these classes of warnings. This may not be always available.
	DocumentationLink *string `json:"documentationLink,omitempty"`
}

// MarshalJSON is the custom marshaler for ItemWarning.
func (iw ItemWarning) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// KeepTokenFilter a token filter that only keeps tokens with text contained in a specified list of words.
// This token filter is implemented using Apache Lucene.
type KeepTokenFilter struct {
	// KeepWords - The list of words to keep.
	KeepWords *[]string `json:"keepWords,omitempty"`
	// LowerCaseKeepWords - A value indicating whether to lower case all words first. Default is false.
	LowerCaseKeepWords *bool `json:"keepWordsCase,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for KeepTokenFilter.
func (ktf KeepTokenFilter) MarshalJSON() ([]byte, error) {
	ktf.OdataType = OdataTypeMicrosoftAzureSearchKeepTokenFilter
	objectMap := make(map[string]interface{})
	if ktf.KeepWords != nil {
		objectMap["keepWords"] = ktf.KeepWords
	}
	if ktf.LowerCaseKeepWords != nil {
		objectMap["keepWordsCase"] = ktf.LowerCaseKeepWords
	}
	if ktf.Name != nil {
		objectMap["name"] = ktf.Name
	}
	if ktf.OdataType != "" {
		objectMap["@odata.type"] = ktf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return &ktf, true
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for KeepTokenFilter.
func (ktf KeepTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ktf, true
}

// KeyPhraseExtractionSkill a skill that uses text analytics for key phrase extraction.
type KeyPhraseExtractionSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is en. Possible values include: 'KeyPhraseExtractionSkillLanguageDa', 'KeyPhraseExtractionSkillLanguageNl', 'KeyPhraseExtractionSkillLanguageEn', 'KeyPhraseExtractionSkillLanguageFi', 'KeyPhraseExtractionSkillLanguageFr', 'KeyPhraseExtractionSkillLanguageDe', 'KeyPhraseExtractionSkillLanguageIt', 'KeyPhraseExtractionSkillLanguageJa', 'KeyPhraseExtractionSkillLanguageKo', 'KeyPhraseExtractionSkillLanguageNo', 'KeyPhraseExtractionSkillLanguagePl', 'KeyPhraseExtractionSkillLanguagePtPT', 'KeyPhraseExtractionSkillLanguagePtBR', 'KeyPhraseExtractionSkillLanguageRu', 'KeyPhraseExtractionSkillLanguageEs', 'KeyPhraseExtractionSkillLanguageSv'
	DefaultLanguageCode KeyPhraseExtractionSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// MaxKeyPhraseCount - A number indicating how many key phrases to return. If absent, all identified key phrases will be returned.
	MaxKeyPhraseCount *int32 `json:"maxKeyPhraseCount,omitempty"`
	// ModelVersion - The version of the model to use when calling the Text Analytics service. It will default to the latest available when not specified. We recommend you do not specify this value unless absolutely necessary.
	ModelVersion *string `json:"modelVersion,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) MarshalJSON() ([]byte, error) {
	kpes.OdataType = OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill
	objectMap := make(map[string]interface{})
	if kpes.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = kpes.DefaultLanguageCode
	}
	if kpes.MaxKeyPhraseCount != nil {
		objectMap["maxKeyPhraseCount"] = kpes.MaxKeyPhraseCount
	}
	if kpes.ModelVersion != nil {
		objectMap["modelVersion"] = kpes.ModelVersion
	}
	if kpes.Name != nil {
		objectMap["name"] = kpes.Name
	}
	if kpes.Description != nil {
		objectMap["description"] = kpes.Description
	}
	if kpes.Context != nil {
		objectMap["context"] = kpes.Context
	}
	if kpes.Inputs != nil {
		objectMap["inputs"] = kpes.Inputs
	}
	if kpes.Outputs != nil {
		objectMap["outputs"] = kpes.Outputs
	}
	if kpes.OdataType != "" {
		objectMap["@odata.type"] = kpes.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return &kpes, true
}

// AsOcrSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for KeyPhraseExtractionSkill.
func (kpes KeyPhraseExtractionSkill) AsBasicSkill() (BasicSkill, bool) {
	return &kpes, true
}

// KeywordMarkerTokenFilter marks terms as keywords. This token filter is implemented using Apache Lucene.
type KeywordMarkerTokenFilter struct {
	// Keywords - A list of words to mark as keywords.
	Keywords *[]string `json:"keywords,omitempty"`
	// IgnoreCase - A value indicating whether to ignore case. If true, all words are converted to lower case first. Default is false.
	IgnoreCase *bool `json:"ignoreCase,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) MarshalJSON() ([]byte, error) {
	kmtf.OdataType = OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter
	objectMap := make(map[string]interface{})
	if kmtf.Keywords != nil {
		objectMap["keywords"] = kmtf.Keywords
	}
	if kmtf.IgnoreCase != nil {
		objectMap["ignoreCase"] = kmtf.IgnoreCase
	}
	if kmtf.Name != nil {
		objectMap["name"] = kmtf.Name
	}
	if kmtf.OdataType != "" {
		objectMap["@odata.type"] = kmtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return &kmtf, true
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for KeywordMarkerTokenFilter.
func (kmtf KeywordMarkerTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &kmtf, true
}

// KeywordTokenizer emits the entire input as a single token. This tokenizer is implemented using Apache
// Lucene.
type KeywordTokenizer struct {
	// BufferSize - The read buffer size in bytes. Default is 256.
	BufferSize *int32 `json:"bufferSize,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for KeywordTokenizer.
func (kt KeywordTokenizer) MarshalJSON() ([]byte, error) {
	kt.OdataType = OdataTypeMicrosoftAzureSearchKeywordTokenizer
	objectMap := make(map[string]interface{})
	if kt.BufferSize != nil {
		objectMap["bufferSize"] = kt.BufferSize
	}
	if kt.Name != nil {
		objectMap["name"] = kt.Name
	}
	if kt.OdataType != "" {
		objectMap["@odata.type"] = kt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return &kt, true
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for KeywordTokenizer.
func (kt KeywordTokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &kt, true
}

// KeywordTokenizerV2 emits the entire input as a single token. This tokenizer is implemented using Apache
// Lucene.
type KeywordTokenizerV2 struct {
	// MaxTokenLength - The maximum token length. Default is 256. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) MarshalJSON() ([]byte, error) {
	ktv.OdataType = OdataTypeMicrosoftAzureSearchKeywordTokenizerV2
	objectMap := make(map[string]interface{})
	if ktv.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = ktv.MaxTokenLength
	}
	if ktv.Name != nil {
		objectMap["name"] = ktv.Name
	}
	if ktv.OdataType != "" {
		objectMap["@odata.type"] = ktv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return &ktv, true
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for KeywordTokenizerV2.
func (ktv KeywordTokenizerV2) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &ktv, true
}

// LanguageDetectionSkill a skill that detects the language of input text and reports a single language
// code for every document submitted on the request. The language code is paired with a score indicating
// the confidence of the analysis.
type LanguageDetectionSkill struct {
	// DefaultCountryHint - A country code to use as a hint to the language detection model if it cannot disambiguate the language.
	DefaultCountryHint *string `json:"defaultCountryHint,omitempty"`
	// ModelVersion - The version of the model to use when calling the Text Analytics service. It will default to the latest available when not specified. We recommend you do not specify this value unless absolutely necessary.
	ModelVersion *string `json:"modelVersion,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) MarshalJSON() ([]byte, error) {
	lds.OdataType = OdataTypeMicrosoftSkillsTextLanguageDetectionSkill
	objectMap := make(map[string]interface{})
	if lds.DefaultCountryHint != nil {
		objectMap["defaultCountryHint"] = lds.DefaultCountryHint
	}
	if lds.ModelVersion != nil {
		objectMap["modelVersion"] = lds.ModelVersion
	}
	if lds.Name != nil {
		objectMap["name"] = lds.Name
	}
	if lds.Description != nil {
		objectMap["description"] = lds.Description
	}
	if lds.Context != nil {
		objectMap["context"] = lds.Context
	}
	if lds.Inputs != nil {
		objectMap["inputs"] = lds.Inputs
	}
	if lds.Outputs != nil {
		objectMap["outputs"] = lds.Outputs
	}
	if lds.OdataType != "" {
		objectMap["@odata.type"] = lds.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return &lds, true
}

// AsShaperSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for LanguageDetectionSkill.
func (lds LanguageDetectionSkill) AsBasicSkill() (BasicSkill, bool) {
	return &lds, true
}

// LengthTokenFilter removes words that are too long or too short. This token filter is implemented using
// Apache Lucene.
type LengthTokenFilter struct {
	// Min - The minimum length in characters. Default is 0. Maximum is 300. Must be less than the value of max.
	Min *int32 `json:"min,omitempty"`
	// Max - The maximum length in characters. Default and maximum is 300.
	Max *int32 `json:"max,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for LengthTokenFilter.
func (ltf LengthTokenFilter) MarshalJSON() ([]byte, error) {
	ltf.OdataType = OdataTypeMicrosoftAzureSearchLengthTokenFilter
	objectMap := make(map[string]interface{})
	if ltf.Min != nil {
		objectMap["min"] = ltf.Min
	}
	if ltf.Max != nil {
		objectMap["max"] = ltf.Max
	}
	if ltf.Name != nil {
		objectMap["name"] = ltf.Name
	}
	if ltf.OdataType != "" {
		objectMap["@odata.type"] = ltf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return &ltf, true
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for LengthTokenFilter.
func (ltf LengthTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ltf, true
}

// LimitTokenFilter limits the number of tokens while indexing. This token filter is implemented using
// Apache Lucene.
type LimitTokenFilter struct {
	// MaxTokenCount - The maximum number of tokens to produce. Default is 1.
	MaxTokenCount *int32 `json:"maxTokenCount,omitempty"`
	// ConsumeAllTokens - A value indicating whether all tokens from the input must be consumed even if maxTokenCount is reached. Default is false.
	ConsumeAllTokens *bool `json:"consumeAllTokens,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for LimitTokenFilter.
func (ltf LimitTokenFilter) MarshalJSON() ([]byte, error) {
	ltf.OdataType = OdataTypeMicrosoftAzureSearchLimitTokenFilter
	objectMap := make(map[string]interface{})
	if ltf.MaxTokenCount != nil {
		objectMap["maxTokenCount"] = ltf.MaxTokenCount
	}
	if ltf.ConsumeAllTokens != nil {
		objectMap["consumeAllTokens"] = ltf.ConsumeAllTokens
	}
	if ltf.Name != nil {
		objectMap["name"] = ltf.Name
	}
	if ltf.OdataType != "" {
		objectMap["@odata.type"] = ltf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return &ltf, true
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for LimitTokenFilter.
func (ltf LimitTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ltf, true
}

// ListDataSourcesResult response from a List Datasources request. If successful, it includes the full
// definitions of all datasources.
type ListDataSourcesResult struct {
	autorest.Response `json:"-"`
	// DataSources - READ-ONLY; The datasources in the Search service.
	DataSources *[]DataSource `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListDataSourcesResult.
func (ldsr ListDataSourcesResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ListIndexersResult response from a List Indexers request. If successful, it includes the full
// definitions of all indexers.
type ListIndexersResult struct {
	autorest.Response `json:"-"`
	// Indexers - READ-ONLY; The indexers in the Search service.
	Indexers *[]Indexer `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListIndexersResult.
func (lir ListIndexersResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ListIndexesResult response from a List Indexes request. If successful, it includes the full definitions
// of all indexes.
type ListIndexesResult struct {
	autorest.Response `json:"-"`
	// Indexes - READ-ONLY; The indexes in the Search service.
	Indexes *[]Index `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListIndexesResult.
func (lir ListIndexesResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ListSkillsetsResult response from a list Skillset request. If successful, it includes the full
// definitions of all skillsets.
type ListSkillsetsResult struct {
	autorest.Response `json:"-"`
	// Skillsets - READ-ONLY; The skillsets defined in the Search service.
	Skillsets *[]Skillset `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListSkillsetsResult.
func (lsr ListSkillsetsResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// ListSynonymMapsResult response from a List SynonymMaps request. If successful, it includes the full
// definitions of all synonym maps.
type ListSynonymMapsResult struct {
	autorest.Response `json:"-"`
	// SynonymMaps - READ-ONLY; The synonym maps in the Search service.
	SynonymMaps *[]SynonymMap `json:"value,omitempty"`
}

// MarshalJSON is the custom marshaler for ListSynonymMapsResult.
func (lsmr ListSynonymMapsResult) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// MagnitudeScoringFunction defines a function that boosts scores based on the magnitude of a numeric
// field.
type MagnitudeScoringFunction struct {
	// Parameters - Parameter values for the magnitude scoring function.
	Parameters *MagnitudeScoringParameters `json:"magnitude,omitempty"`
	// FieldName - The name of the field used as input to the scoring function.
	FieldName *string `json:"fieldName,omitempty"`
	// Boost - A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64 `json:"boost,omitempty"`
	// Interpolation - A value indicating how boosting will be interpolated across document scores; defaults to "Linear". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'
	Interpolation ScoringFunctionInterpolation `json:"interpolation,omitempty"`
	// Type - Possible values include: 'TypeScoringFunction', 'TypeDistance', 'TypeFreshness', 'TypeMagnitude', 'TypeTag'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) MarshalJSON() ([]byte, error) {
	msf.Type = TypeMagnitude
	objectMap := make(map[string]interface{})
	if msf.Parameters != nil {
		objectMap["magnitude"] = msf.Parameters
	}
	if msf.FieldName != nil {
		objectMap["fieldName"] = msf.FieldName
	}
	if msf.Boost != nil {
		objectMap["boost"] = msf.Boost
	}
	if msf.Interpolation != "" {
		objectMap["interpolation"] = msf.Interpolation
	}
	if msf.Type != "" {
		objectMap["type"] = msf.Type
	}
	return json.Marshal(objectMap)
}

// AsDistanceScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsDistanceScoringFunction() (*DistanceScoringFunction, bool) {
	return nil, false
}

// AsFreshnessScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool) {
	return nil, false
}

// AsMagnitudeScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool) {
	return &msf, true
}

// AsTagScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsTagScoringFunction() (*TagScoringFunction, bool) {
	return nil, false
}

// AsScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsScoringFunction() (*ScoringFunction, bool) {
	return nil, false
}

// AsBasicScoringFunction is the BasicScoringFunction implementation for MagnitudeScoringFunction.
func (msf MagnitudeScoringFunction) AsBasicScoringFunction() (BasicScoringFunction, bool) {
	return &msf, true
}

// MagnitudeScoringParameters provides parameter values to a magnitude scoring function.
type MagnitudeScoringParameters struct {
	// BoostingRangeStart - The field value at which boosting starts.
	BoostingRangeStart *float64 `json:"boostingRangeStart,omitempty"`
	// BoostingRangeEnd - The field value at which boosting ends.
	BoostingRangeEnd *float64 `json:"boostingRangeEnd,omitempty"`
	// ShouldBoostBeyondRangeByConstant - A value indicating whether to apply a constant boost for field values beyond the range end value; default is false.
	ShouldBoostBeyondRangeByConstant *bool `json:"constantBoostBeyondRange,omitempty"`
}

// MappingCharFilter a character filter that applies mappings defined with the mappings option. Matching is
// greedy (longest pattern matching at a given point wins). Replacement is allowed to be the empty string.
// This character filter is implemented using Apache Lucene.
type MappingCharFilter struct {
	// Mappings - A list of mappings of the following format: "a=>b" (all occurrences of the character "a" will be replaced with character "b").
	Mappings *[]string `json:"mappings,omitempty"`
	// Name - The name of the char filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeCharFilter', 'OdataTypeMicrosoftAzureSearchMappingCharFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceCharFilter'
	OdataType OdataTypeBasicCharFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for MappingCharFilter.
func (mcf MappingCharFilter) MarshalJSON() ([]byte, error) {
	mcf.OdataType = OdataTypeMicrosoftAzureSearchMappingCharFilter
	objectMap := make(map[string]interface{})
	if mcf.Mappings != nil {
		objectMap["mappings"] = mcf.Mappings
	}
	if mcf.Name != nil {
		objectMap["name"] = mcf.Name
	}
	if mcf.OdataType != "" {
		objectMap["@odata.type"] = mcf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsMappingCharFilter is the BasicCharFilter implementation for MappingCharFilter.
func (mcf MappingCharFilter) AsMappingCharFilter() (*MappingCharFilter, bool) {
	return &mcf, true
}

// AsPatternReplaceCharFilter is the BasicCharFilter implementation for MappingCharFilter.
func (mcf MappingCharFilter) AsPatternReplaceCharFilter() (*PatternReplaceCharFilter, bool) {
	return nil, false
}

// AsCharFilter is the BasicCharFilter implementation for MappingCharFilter.
func (mcf MappingCharFilter) AsCharFilter() (*CharFilter, bool) {
	return nil, false
}

// AsBasicCharFilter is the BasicCharFilter implementation for MappingCharFilter.
func (mcf MappingCharFilter) AsBasicCharFilter() (BasicCharFilter, bool) {
	return &mcf, true
}

// MergeSkill a skill for merging two or more strings into a single unified string, with an optional
// user-defined delimiter separating each component part.
type MergeSkill struct {
	// InsertPreTag - The tag indicates the start of the merged text. By default, the tag is an empty space.
	InsertPreTag *string `json:"insertPreTag,omitempty"`
	// InsertPostTag - The tag indicates the end of the merged text. By default, the tag is an empty space.
	InsertPostTag *string `json:"insertPostTag,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for MergeSkill.
func (ms MergeSkill) MarshalJSON() ([]byte, error) {
	ms.OdataType = OdataTypeMicrosoftSkillsTextMergeSkill
	objectMap := make(map[string]interface{})
	if ms.InsertPreTag != nil {
		objectMap["insertPreTag"] = ms.InsertPreTag
	}
	if ms.InsertPostTag != nil {
		objectMap["insertPostTag"] = ms.InsertPostTag
	}
	if ms.Name != nil {
		objectMap["name"] = ms.Name
	}
	if ms.Description != nil {
		objectMap["description"] = ms.Description
	}
	if ms.Context != nil {
		objectMap["context"] = ms.Context
	}
	if ms.Inputs != nil {
		objectMap["inputs"] = ms.Inputs
	}
	if ms.Outputs != nil {
		objectMap["outputs"] = ms.Outputs
	}
	if ms.OdataType != "" {
		objectMap["@odata.type"] = ms.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsMergeSkill() (*MergeSkill, bool) {
	return &ms, true
}

// AsEntityRecognitionSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for MergeSkill.
func (ms MergeSkill) AsBasicSkill() (BasicSkill, bool) {
	return &ms, true
}

// MicrosoftLanguageStemmingTokenizer divides text using language-specific rules and reduces words to their
// base forms.
type MicrosoftLanguageStemmingTokenizer struct {
	// MaxTokenLength - The maximum token length. Tokens longer than the maximum length are split. Maximum token length that can be used is 300 characters. Tokens longer than 300 characters are first split into tokens of length 300 and then each of those tokens is split based on the max token length set. Default is 255.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// IsSearchTokenizer - A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set to false if used as the indexing tokenizer. Default is false.
	IsSearchTokenizer *bool `json:"isSearchTokenizer,omitempty"`
	// Language - The language to use. The default is English. Possible values include: 'Arabic', 'Bangla', 'Bulgarian', 'Catalan', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Finnish', 'French', 'German', 'Greek', 'Gujarati', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Kannada', 'Latvian', 'Lithuanian', 'Malay', 'Malayalam', 'Marathi', 'NorwegianBokmaal', 'Polish', 'Portuguese', 'PortugueseBrazilian', 'Punjabi', 'Romanian', 'Russian', 'SerbianCyrillic', 'SerbianLatin', 'Slovak', 'Slovenian', 'Spanish', 'Swedish', 'Tamil', 'Telugu', 'Turkish', 'Ukrainian', 'Urdu'
	Language MicrosoftStemmingTokenizerLanguage `json:"language,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) MarshalJSON() ([]byte, error) {
	mlst.OdataType = OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer
	objectMap := make(map[string]interface{})
	if mlst.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = mlst.MaxTokenLength
	}
	if mlst.IsSearchTokenizer != nil {
		objectMap["isSearchTokenizer"] = mlst.IsSearchTokenizer
	}
	if mlst.Language != "" {
		objectMap["language"] = mlst.Language
	}
	if mlst.Name != nil {
		objectMap["name"] = mlst.Name
	}
	if mlst.OdataType != "" {
		objectMap["@odata.type"] = mlst.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return &mlst, true
}

// AsNGramTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for MicrosoftLanguageStemmingTokenizer.
func (mlst MicrosoftLanguageStemmingTokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &mlst, true
}

// MicrosoftLanguageTokenizer divides text using language-specific rules.
type MicrosoftLanguageTokenizer struct {
	// MaxTokenLength - The maximum token length. Tokens longer than the maximum length are split. Maximum token length that can be used is 300 characters. Tokens longer than 300 characters are first split into tokens of length 300 and then each of those tokens is split based on the max token length set. Default is 255.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// IsSearchTokenizer - A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set to false if used as the indexing tokenizer. Default is false.
	IsSearchTokenizer *bool `json:"isSearchTokenizer,omitempty"`
	// Language - The language to use. The default is English. Possible values include: 'MicrosoftTokenizerLanguageBangla', 'MicrosoftTokenizerLanguageBulgarian', 'MicrosoftTokenizerLanguageCatalan', 'MicrosoftTokenizerLanguageChineseSimplified', 'MicrosoftTokenizerLanguageChineseTraditional', 'MicrosoftTokenizerLanguageCroatian', 'MicrosoftTokenizerLanguageCzech', 'MicrosoftTokenizerLanguageDanish', 'MicrosoftTokenizerLanguageDutch', 'MicrosoftTokenizerLanguageEnglish', 'MicrosoftTokenizerLanguageFrench', 'MicrosoftTokenizerLanguageGerman', 'MicrosoftTokenizerLanguageGreek', 'MicrosoftTokenizerLanguageGujarati', 'MicrosoftTokenizerLanguageHindi', 'MicrosoftTokenizerLanguageIcelandic', 'MicrosoftTokenizerLanguageIndonesian', 'MicrosoftTokenizerLanguageItalian', 'MicrosoftTokenizerLanguageJapanese', 'MicrosoftTokenizerLanguageKannada', 'MicrosoftTokenizerLanguageKorean', 'MicrosoftTokenizerLanguageMalay', 'MicrosoftTokenizerLanguageMalayalam', 'MicrosoftTokenizerLanguageMarathi', 'MicrosoftTokenizerLanguageNorwegianBokmaal', 'MicrosoftTokenizerLanguagePolish', 'MicrosoftTokenizerLanguagePortuguese', 'MicrosoftTokenizerLanguagePortugueseBrazilian', 'MicrosoftTokenizerLanguagePunjabi', 'MicrosoftTokenizerLanguageRomanian', 'MicrosoftTokenizerLanguageRussian', 'MicrosoftTokenizerLanguageSerbianCyrillic', 'MicrosoftTokenizerLanguageSerbianLatin', 'MicrosoftTokenizerLanguageSlovenian', 'MicrosoftTokenizerLanguageSpanish', 'MicrosoftTokenizerLanguageSwedish', 'MicrosoftTokenizerLanguageTamil', 'MicrosoftTokenizerLanguageTelugu', 'MicrosoftTokenizerLanguageThai', 'MicrosoftTokenizerLanguageUkrainian', 'MicrosoftTokenizerLanguageUrdu', 'MicrosoftTokenizerLanguageVietnamese'
	Language MicrosoftTokenizerLanguage `json:"language,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) MarshalJSON() ([]byte, error) {
	mlt.OdataType = OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer
	objectMap := make(map[string]interface{})
	if mlt.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = mlt.MaxTokenLength
	}
	if mlt.IsSearchTokenizer != nil {
		objectMap["isSearchTokenizer"] = mlt.IsSearchTokenizer
	}
	if mlt.Language != "" {
		objectMap["language"] = mlt.Language
	}
	if mlt.Name != nil {
		objectMap["name"] = mlt.Name
	}
	if mlt.OdataType != "" {
		objectMap["@odata.type"] = mlt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return &mlt, true
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for MicrosoftLanguageTokenizer.
func (mlt MicrosoftLanguageTokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &mlt, true
}

// NGramTokenFilter generates n-grams of the given size(s). This token filter is implemented using Apache
// Lucene.
type NGramTokenFilter struct {
	// MinGram - The minimum n-gram length. Default is 1. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for NGramTokenFilter.
func (ngtf NGramTokenFilter) MarshalJSON() ([]byte, error) {
	ngtf.OdataType = OdataTypeMicrosoftAzureSearchNGramTokenFilter
	objectMap := make(map[string]interface{})
	if ngtf.MinGram != nil {
		objectMap["minGram"] = ngtf.MinGram
	}
	if ngtf.MaxGram != nil {
		objectMap["maxGram"] = ngtf.MaxGram
	}
	if ngtf.Name != nil {
		objectMap["name"] = ngtf.Name
	}
	if ngtf.OdataType != "" {
		objectMap["@odata.type"] = ngtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return &ngtf, true
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for NGramTokenFilter.
func (ngtf NGramTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ngtf, true
}

// NGramTokenFilterV2 generates n-grams of the given size(s). This token filter is implemented using Apache
// Lucene.
type NGramTokenFilterV2 struct {
	// MinGram - The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) MarshalJSON() ([]byte, error) {
	ngtfv.OdataType = OdataTypeMicrosoftAzureSearchNGramTokenFilterV2
	objectMap := make(map[string]interface{})
	if ngtfv.MinGram != nil {
		objectMap["minGram"] = ngtfv.MinGram
	}
	if ngtfv.MaxGram != nil {
		objectMap["maxGram"] = ngtfv.MaxGram
	}
	if ngtfv.Name != nil {
		objectMap["name"] = ngtfv.Name
	}
	if ngtfv.OdataType != "" {
		objectMap["@odata.type"] = ngtfv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return &ngtfv, true
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for NGramTokenFilterV2.
func (ngtfv NGramTokenFilterV2) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ngtfv, true
}

// NGramTokenizer tokenizes the input into n-grams of the given size(s). This tokenizer is implemented
// using Apache Lucene.
type NGramTokenizer struct {
	// MinGram - The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32 `json:"minGram,omitempty"`
	// MaxGram - The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32 `json:"maxGram,omitempty"`
	// TokenChars - Character classes to keep in the tokens.
	TokenChars *[]TokenCharacterKind `json:"tokenChars,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for NGramTokenizer.
func (ngt NGramTokenizer) MarshalJSON() ([]byte, error) {
	ngt.OdataType = OdataTypeMicrosoftAzureSearchNGramTokenizer
	objectMap := make(map[string]interface{})
	if ngt.MinGram != nil {
		objectMap["minGram"] = ngt.MinGram
	}
	if ngt.MaxGram != nil {
		objectMap["maxGram"] = ngt.MaxGram
	}
	if ngt.TokenChars != nil {
		objectMap["tokenChars"] = ngt.TokenChars
	}
	if ngt.Name != nil {
		objectMap["name"] = ngt.Name
	}
	if ngt.OdataType != "" {
		objectMap["@odata.type"] = ngt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return &ngt, true
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for NGramTokenizer.
func (ngt NGramTokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &ngt, true
}

// OcrSkill a skill that extracts text from image files.
type OcrSkill struct {
	// TextExtractionAlgorithm - A value indicating which algorithm to use for extracting text. Default is printed. Possible values include: 'Printed', 'Handwritten'
	TextExtractionAlgorithm TextExtractionAlgorithm `json:"textExtractionAlgorithm,omitempty"`
	// DefaultLanguageCode - A value indicating which language code to use. Default is en. Possible values include: 'OcrSkillLanguageZhHans', 'OcrSkillLanguageZhHant', 'OcrSkillLanguageCs', 'OcrSkillLanguageDa', 'OcrSkillLanguageNl', 'OcrSkillLanguageEn', 'OcrSkillLanguageFi', 'OcrSkillLanguageFr', 'OcrSkillLanguageDe', 'OcrSkillLanguageEl', 'OcrSkillLanguageHu', 'OcrSkillLanguageIt', 'OcrSkillLanguageJa', 'OcrSkillLanguageKo', 'OcrSkillLanguageNb', 'OcrSkillLanguagePl', 'OcrSkillLanguagePt', 'OcrSkillLanguageRu', 'OcrSkillLanguageEs', 'OcrSkillLanguageSv', 'OcrSkillLanguageTr', 'OcrSkillLanguageAr', 'OcrSkillLanguageRo', 'OcrSkillLanguageSrCyrl', 'OcrSkillLanguageSrLatn', 'OcrSkillLanguageSk'
	DefaultLanguageCode OcrSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// ShouldDetectOrientation - A value indicating to turn orientation detection on or not. Default is false.
	ShouldDetectOrientation *bool `json:"detectOrientation,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for OcrSkill.
func (osVar OcrSkill) MarshalJSON() ([]byte, error) {
	osVar.OdataType = OdataTypeMicrosoftSkillsVisionOcrSkill
	objectMap := make(map[string]interface{})
	if osVar.TextExtractionAlgorithm != "" {
		objectMap["textExtractionAlgorithm"] = osVar.TextExtractionAlgorithm
	}
	if osVar.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = osVar.DefaultLanguageCode
	}
	if osVar.ShouldDetectOrientation != nil {
		objectMap["detectOrientation"] = osVar.ShouldDetectOrientation
	}
	if osVar.Name != nil {
		objectMap["name"] = osVar.Name
	}
	if osVar.Description != nil {
		objectMap["description"] = osVar.Description
	}
	if osVar.Context != nil {
		objectMap["context"] = osVar.Context
	}
	if osVar.Inputs != nil {
		objectMap["inputs"] = osVar.Inputs
	}
	if osVar.Outputs != nil {
		objectMap["outputs"] = osVar.Outputs
	}
	if osVar.OdataType != "" {
		objectMap["@odata.type"] = osVar.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsOcrSkill() (*OcrSkill, bool) {
	return &osVar, true
}

// AsImageAnalysisSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for OcrSkill.
func (osVar OcrSkill) AsBasicSkill() (BasicSkill, bool) {
	return &osVar, true
}

// OutputFieldMappingEntry output field mapping for a skill.
type OutputFieldMappingEntry struct {
	// Name - The name of the output defined by the skill.
	Name *string `json:"name,omitempty"`
	// TargetName - The target name of the output. It is optional and default to name.
	TargetName *string `json:"targetName,omitempty"`
}

// PathHierarchyTokenizer tokenizer for path-like hierarchies. This tokenizer is implemented using Apache
// Lucene.
type PathHierarchyTokenizer struct {
	// Delimiter - The delimiter character to use. Default is "/".
	Delimiter *string `json:"delimiter,omitempty"`
	// Replacement - A value that, if set, replaces the delimiter character. Default is "/".
	Replacement *string `json:"replacement,omitempty"`
	// BufferSize - The buffer size. Default is 1024.
	BufferSize *int32 `json:"bufferSize,omitempty"`
	// ReverseTokenOrder - A value indicating whether to generate tokens in reverse order. Default is false.
	ReverseTokenOrder *bool `json:"reverse,omitempty"`
	// NumberOfTokensToSkip - The number of initial tokens to skip. Default is 0.
	NumberOfTokensToSkip *int32 `json:"skip,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) MarshalJSON() ([]byte, error) {
	pht.OdataType = OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer
	objectMap := make(map[string]interface{})
	if pht.Delimiter != nil {
		objectMap["delimiter"] = pht.Delimiter
	}
	if pht.Replacement != nil {
		objectMap["replacement"] = pht.Replacement
	}
	if pht.BufferSize != nil {
		objectMap["bufferSize"] = pht.BufferSize
	}
	if pht.ReverseTokenOrder != nil {
		objectMap["reverse"] = pht.ReverseTokenOrder
	}
	if pht.NumberOfTokensToSkip != nil {
		objectMap["skip"] = pht.NumberOfTokensToSkip
	}
	if pht.Name != nil {
		objectMap["name"] = pht.Name
	}
	if pht.OdataType != "" {
		objectMap["@odata.type"] = pht.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return &pht, true
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizer.
func (pht PathHierarchyTokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &pht, true
}

// PathHierarchyTokenizerV2 tokenizer for path-like hierarchies. This tokenizer is implemented using Apache
// Lucene.
type PathHierarchyTokenizerV2 struct {
	// Delimiter - The delimiter character to use. Default is "/".
	Delimiter *string `json:"delimiter,omitempty"`
	// Replacement - A value that, if set, replaces the delimiter character. Default is "/".
	Replacement *string `json:"replacement,omitempty"`
	// MaxTokenLength - The maximum token length. Default and maximum is 300.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// ReverseTokenOrder - A value indicating whether to generate tokens in reverse order. Default is false.
	ReverseTokenOrder *bool `json:"reverse,omitempty"`
	// NumberOfTokensToSkip - The number of initial tokens to skip. Default is 0.
	NumberOfTokensToSkip *int32 `json:"skip,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) MarshalJSON() ([]byte, error) {
	phtv.OdataType = OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2
	objectMap := make(map[string]interface{})
	if phtv.Delimiter != nil {
		objectMap["delimiter"] = phtv.Delimiter
	}
	if phtv.Replacement != nil {
		objectMap["replacement"] = phtv.Replacement
	}
	if phtv.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = phtv.MaxTokenLength
	}
	if phtv.ReverseTokenOrder != nil {
		objectMap["reverse"] = phtv.ReverseTokenOrder
	}
	if phtv.NumberOfTokensToSkip != nil {
		objectMap["skip"] = phtv.NumberOfTokensToSkip
	}
	if phtv.Name != nil {
		objectMap["name"] = phtv.Name
	}
	if phtv.OdataType != "" {
		objectMap["@odata.type"] = phtv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return &phtv, true
}

// AsPatternTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for PathHierarchyTokenizerV2.
func (phtv PathHierarchyTokenizerV2) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &phtv, true
}

// PatternAnalyzer flexibly separates text into terms via a regular expression pattern. This analyzer is
// implemented using Apache Lucene.
type PatternAnalyzer struct {
	// LowerCaseTerms - A value indicating whether terms should be lower-cased. Default is true.
	LowerCaseTerms *bool `json:"lowercase,omitempty"`
	// Pattern - A regular expression pattern to match token separators. Default is an expression that matches one or more whitespace characters.
	Pattern *string `json:"pattern,omitempty"`
	// Flags - Regular expression flags. Possible values include: 'CANONEQ', 'CASEINSENSITIVE', 'COMMENTS', 'DOTALL', 'LITERAL', 'MULTILINE', 'UNICODECASE', 'UNIXLINES'
	Flags RegexFlags `json:"flags,omitempty"`
	// Stopwords - A list of stopwords.
	Stopwords *[]string `json:"stopwords,omitempty"`
	// Name - The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeAnalyzer', 'OdataTypeMicrosoftAzureSearchCustomAnalyzer', 'OdataTypeMicrosoftAzureSearchPatternAnalyzer', 'OdataTypeMicrosoftAzureSearchStandardAnalyzer', 'OdataTypeMicrosoftAzureSearchStopAnalyzer'
	OdataType OdataType `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PatternAnalyzer.
func (pa PatternAnalyzer) MarshalJSON() ([]byte, error) {
	pa.OdataType = OdataTypeMicrosoftAzureSearchPatternAnalyzer
	objectMap := make(map[string]interface{})
	if pa.LowerCaseTerms != nil {
		objectMap["lowercase"] = pa.LowerCaseTerms
	}
	if pa.Pattern != nil {
		objectMap["pattern"] = pa.Pattern
	}
	if pa.Flags != "" {
		objectMap["flags"] = pa.Flags
	}
	if pa.Stopwords != nil {
		objectMap["stopwords"] = pa.Stopwords
	}
	if pa.Name != nil {
		objectMap["name"] = pa.Name
	}
	if pa.OdataType != "" {
		objectMap["@odata.type"] = pa.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomAnalyzer is the BasicAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsCustomAnalyzer() (*CustomAnalyzer, bool) {
	return nil, false
}

// AsPatternAnalyzer is the BasicAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsPatternAnalyzer() (*PatternAnalyzer, bool) {
	return &pa, true
}

// AsStandardAnalyzer is the BasicAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsStandardAnalyzer() (*StandardAnalyzer, bool) {
	return nil, false
}

// AsStopAnalyzer is the BasicAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsStopAnalyzer() (*StopAnalyzer, bool) {
	return nil, false
}

// AsAnalyzer is the BasicAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsAnalyzer() (*Analyzer, bool) {
	return nil, false
}

// AsBasicAnalyzer is the BasicAnalyzer implementation for PatternAnalyzer.
func (pa PatternAnalyzer) AsBasicAnalyzer() (BasicAnalyzer, bool) {
	return &pa, true
}

// PatternCaptureTokenFilter uses Java regexes to emit multiple tokens - one for each capture group in one
// or more patterns. This token filter is implemented using Apache Lucene.
type PatternCaptureTokenFilter struct {
	// Patterns - A list of patterns to match against each token.
	Patterns *[]string `json:"patterns,omitempty"`
	// PreserveOriginal - A value indicating whether to return the original token even if one of the patterns matches. Default is true.
	PreserveOriginal *bool `json:"preserveOriginal,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) MarshalJSON() ([]byte, error) {
	pctf.OdataType = OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter
	objectMap := make(map[string]interface{})
	if pctf.Patterns != nil {
		objectMap["patterns"] = pctf.Patterns
	}
	if pctf.PreserveOriginal != nil {
		objectMap["preserveOriginal"] = pctf.PreserveOriginal
	}
	if pctf.Name != nil {
		objectMap["name"] = pctf.Name
	}
	if pctf.OdataType != "" {
		objectMap["@odata.type"] = pctf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return &pctf, true
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for PatternCaptureTokenFilter.
func (pctf PatternCaptureTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &pctf, true
}

// PatternReplaceCharFilter a character filter that replaces characters in the input string. It uses a
// regular expression to identify character sequences to preserve and a replacement pattern to identify
// characters to replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and
// replacement "$1#$2", the result would be "aa#bb aa#bb". This character filter is implemented using
// Apache Lucene.
type PatternReplaceCharFilter struct {
	// Pattern - A regular expression pattern.
	Pattern *string `json:"pattern,omitempty"`
	// Replacement - The replacement text.
	Replacement *string `json:"replacement,omitempty"`
	// Name - The name of the char filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeCharFilter', 'OdataTypeMicrosoftAzureSearchMappingCharFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceCharFilter'
	OdataType OdataTypeBasicCharFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PatternReplaceCharFilter.
func (prcf PatternReplaceCharFilter) MarshalJSON() ([]byte, error) {
	prcf.OdataType = OdataTypeMicrosoftAzureSearchPatternReplaceCharFilter
	objectMap := make(map[string]interface{})
	if prcf.Pattern != nil {
		objectMap["pattern"] = prcf.Pattern
	}
	if prcf.Replacement != nil {
		objectMap["replacement"] = prcf.Replacement
	}
	if prcf.Name != nil {
		objectMap["name"] = prcf.Name
	}
	if prcf.OdataType != "" {
		objectMap["@odata.type"] = prcf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsMappingCharFilter is the BasicCharFilter implementation for PatternReplaceCharFilter.
func (prcf PatternReplaceCharFilter) AsMappingCharFilter() (*MappingCharFilter, bool) {
	return nil, false
}

// AsPatternReplaceCharFilter is the BasicCharFilter implementation for PatternReplaceCharFilter.
func (prcf PatternReplaceCharFilter) AsPatternReplaceCharFilter() (*PatternReplaceCharFilter, bool) {
	return &prcf, true
}

// AsCharFilter is the BasicCharFilter implementation for PatternReplaceCharFilter.
func (prcf PatternReplaceCharFilter) AsCharFilter() (*CharFilter, bool) {
	return nil, false
}

// AsBasicCharFilter is the BasicCharFilter implementation for PatternReplaceCharFilter.
func (prcf PatternReplaceCharFilter) AsBasicCharFilter() (BasicCharFilter, bool) {
	return &prcf, true
}

// PatternReplaceTokenFilter a character filter that replaces characters in the input string. It uses a
// regular expression to identify character sequences to preserve and a replacement pattern to identify
// characters to replace. For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and
// replacement "$1#$2", the result would be "aa#bb aa#bb". This token filter is implemented using Apache
// Lucene.
type PatternReplaceTokenFilter struct {
	// Pattern - A regular expression pattern.
	Pattern *string `json:"pattern,omitempty"`
	// Replacement - The replacement text.
	Replacement *string `json:"replacement,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) MarshalJSON() ([]byte, error) {
	prtf.OdataType = OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter
	objectMap := make(map[string]interface{})
	if prtf.Pattern != nil {
		objectMap["pattern"] = prtf.Pattern
	}
	if prtf.Replacement != nil {
		objectMap["replacement"] = prtf.Replacement
	}
	if prtf.Name != nil {
		objectMap["name"] = prtf.Name
	}
	if prtf.OdataType != "" {
		objectMap["@odata.type"] = prtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return &prtf, true
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for PatternReplaceTokenFilter.
func (prtf PatternReplaceTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &prtf, true
}

// PatternTokenizer tokenizer that uses regex pattern matching to construct distinct tokens. This tokenizer
// is implemented using Apache Lucene.
type PatternTokenizer struct {
	// Pattern - A regular expression pattern to match token separators. Default is an expression that matches one or more whitespace characters.
	Pattern *string `json:"pattern,omitempty"`
	// Flags - Regular expression flags. Possible values include: 'CANONEQ', 'CASEINSENSITIVE', 'COMMENTS', 'DOTALL', 'LITERAL', 'MULTILINE', 'UNICODECASE', 'UNIXLINES'
	Flags RegexFlags `json:"flags,omitempty"`
	// Group - The zero-based ordinal of the matching group in the regular expression pattern to extract into tokens. Use -1 if you want to use the entire pattern to split the input into tokens, irrespective of matching groups. Default is -1.
	Group *int32 `json:"group,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PatternTokenizer.
func (pt PatternTokenizer) MarshalJSON() ([]byte, error) {
	pt.OdataType = OdataTypeMicrosoftAzureSearchPatternTokenizer
	objectMap := make(map[string]interface{})
	if pt.Pattern != nil {
		objectMap["pattern"] = pt.Pattern
	}
	if pt.Flags != "" {
		objectMap["flags"] = pt.Flags
	}
	if pt.Group != nil {
		objectMap["group"] = pt.Group
	}
	if pt.Name != nil {
		objectMap["name"] = pt.Name
	}
	if pt.OdataType != "" {
		objectMap["@odata.type"] = pt.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return &pt, true
}

// AsStandardTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for PatternTokenizer.
func (pt PatternTokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &pt, true
}

// PhoneticTokenFilter create tokens for phonetic matches. This token filter is implemented using Apache
// Lucene.
type PhoneticTokenFilter struct {
	// Encoder - The phonetic encoder to use. Default is "metaphone". Possible values include: 'Metaphone', 'DoubleMetaphone', 'Soundex', 'RefinedSoundex', 'Caverphone1', 'Caverphone2', 'Cologne', 'Nysiis', 'KoelnerPhonetik', 'HaasePhonetik', 'BeiderMorse'
	Encoder PhoneticEncoder `json:"encoder,omitempty"`
	// ReplaceOriginalTokens - A value indicating whether encoded tokens should replace original tokens. If false, encoded tokens are added as synonyms. Default is true.
	ReplaceOriginalTokens *bool `json:"replace,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) MarshalJSON() ([]byte, error) {
	ptf.OdataType = OdataTypeMicrosoftAzureSearchPhoneticTokenFilter
	objectMap := make(map[string]interface{})
	if ptf.Encoder != "" {
		objectMap["encoder"] = ptf.Encoder
	}
	if ptf.ReplaceOriginalTokens != nil {
		objectMap["replace"] = ptf.ReplaceOriginalTokens
	}
	if ptf.Name != nil {
		objectMap["name"] = ptf.Name
	}
	if ptf.OdataType != "" {
		objectMap["@odata.type"] = ptf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return &ptf, true
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for PhoneticTokenFilter.
func (ptf PhoneticTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ptf, true
}

// ResourceCounter represents a resource's usage and quota.
type ResourceCounter struct {
	// Usage - The resource usage amount.
	Usage *int64 `json:"usage,omitempty"`
	// Quota - The resource amount quota.
	Quota *int64 `json:"quota,omitempty"`
}

// BasicScoringFunction abstract base class for functions that can modify document scores during ranking.
type BasicScoringFunction interface {
	AsDistanceScoringFunction() (*DistanceScoringFunction, bool)
	AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool)
	AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool)
	AsTagScoringFunction() (*TagScoringFunction, bool)
	AsScoringFunction() (*ScoringFunction, bool)
}

// ScoringFunction abstract base class for functions that can modify document scores during ranking.
type ScoringFunction struct {
	// FieldName - The name of the field used as input to the scoring function.
	FieldName *string `json:"fieldName,omitempty"`
	// Boost - A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64 `json:"boost,omitempty"`
	// Interpolation - A value indicating how boosting will be interpolated across document scores; defaults to "Linear". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'
	Interpolation ScoringFunctionInterpolation `json:"interpolation,omitempty"`
	// Type - Possible values include: 'TypeScoringFunction', 'TypeDistance', 'TypeFreshness', 'TypeMagnitude', 'TypeTag'
	Type Type `json:"type,omitempty"`
}

func unmarshalBasicScoringFunction(body []byte) (BasicScoringFunction, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["type"] {
	case string(TypeDistance):
		var dsf DistanceScoringFunction
		err := json.Unmarshal(body, &dsf)
		return dsf, err
	case string(TypeFreshness):
		var fsf FreshnessScoringFunction
		err := json.Unmarshal(body, &fsf)
		return fsf, err
	case string(TypeMagnitude):
		var msf MagnitudeScoringFunction
		err := json.Unmarshal(body, &msf)
		return msf, err
	case string(TypeTag):
		var tsf TagScoringFunction
		err := json.Unmarshal(body, &tsf)
		return tsf, err
	default:
		var sf ScoringFunction
		err := json.Unmarshal(body, &sf)
		return sf, err
	}
}
func unmarshalBasicScoringFunctionArray(body []byte) ([]BasicScoringFunction, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	sfArray := make([]BasicScoringFunction, len(rawMessages))

	for index, rawMessage := range rawMessages {
		sf, err := unmarshalBasicScoringFunction(*rawMessage)
		if err != nil {
			return nil, err
		}
		sfArray[index] = sf
	}
	return sfArray, nil
}

// MarshalJSON is the custom marshaler for ScoringFunction.
func (sf ScoringFunction) MarshalJSON() ([]byte, error) {
	sf.Type = TypeScoringFunction
	objectMap := make(map[string]interface{})
	if sf.FieldName != nil {
		objectMap["fieldName"] = sf.FieldName
	}
	if sf.Boost != nil {
		objectMap["boost"] = sf.Boost
	}
	if sf.Interpolation != "" {
		objectMap["interpolation"] = sf.Interpolation
	}
	if sf.Type != "" {
		objectMap["type"] = sf.Type
	}
	return json.Marshal(objectMap)
}

// AsDistanceScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsDistanceScoringFunction() (*DistanceScoringFunction, bool) {
	return nil, false
}

// AsFreshnessScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool) {
	return nil, false
}

// AsMagnitudeScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool) {
	return nil, false
}

// AsTagScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsTagScoringFunction() (*TagScoringFunction, bool) {
	return nil, false
}

// AsScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsScoringFunction() (*ScoringFunction, bool) {
	return &sf, true
}

// AsBasicScoringFunction is the BasicScoringFunction implementation for ScoringFunction.
func (sf ScoringFunction) AsBasicScoringFunction() (BasicScoringFunction, bool) {
	return &sf, true
}

// ScoringProfile defines parameters for a search index that influence scoring in search queries.
type ScoringProfile struct {
	// Name - The name of the scoring profile.
	Name *string `json:"name,omitempty"`
	// TextWeights - Parameters that boost scoring based on text matches in certain index fields.
	TextWeights *TextWeights `json:"text,omitempty"`
	// Functions - The collection of functions that influence the scoring of documents.
	Functions *[]BasicScoringFunction `json:"functions,omitempty"`
	// FunctionAggregation - A value indicating how the results of individual scoring functions should be combined. Defaults to "Sum". Ignored if there are no scoring functions. Possible values include: 'Sum', 'Average', 'Minimum', 'Maximum', 'FirstMatching'
	FunctionAggregation ScoringFunctionAggregation `json:"functionAggregation,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for ScoringProfile struct.
func (sp *ScoringProfile) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				sp.Name = &name
			}
		case "text":
			if v != nil {
				var textWeights TextWeights
				err = json.Unmarshal(*v, &textWeights)
				if err != nil {
					return err
				}
				sp.TextWeights = &textWeights
			}
		case "functions":
			if v != nil {
				functions, err := unmarshalBasicScoringFunctionArray(*v)
				if err != nil {
					return err
				}
				sp.Functions = &functions
			}
		case "functionAggregation":
			if v != nil {
				var functionAggregation ScoringFunctionAggregation
				err = json.Unmarshal(*v, &functionAggregation)
				if err != nil {
					return err
				}
				sp.FunctionAggregation = functionAggregation
			}
		}
	}

	return nil
}

// SentimentSkill text analytics positive-negative sentiment analysis, scored as a floating point value in
// a range of zero to 1.
type SentimentSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is en. Possible values include: 'SentimentSkillLanguageDa', 'SentimentSkillLanguageNl', 'SentimentSkillLanguageEn', 'SentimentSkillLanguageFi', 'SentimentSkillLanguageFr', 'SentimentSkillLanguageDe', 'SentimentSkillLanguageEl', 'SentimentSkillLanguageIt', 'SentimentSkillLanguageNo', 'SentimentSkillLanguagePl', 'SentimentSkillLanguagePtPT', 'SentimentSkillLanguageRu', 'SentimentSkillLanguageEs', 'SentimentSkillLanguageSv', 'SentimentSkillLanguageTr'
	DefaultLanguageCode SentimentSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SentimentSkill.
func (ss SentimentSkill) MarshalJSON() ([]byte, error) {
	ss.OdataType = OdataTypeMicrosoftSkillsTextSentimentSkill
	objectMap := make(map[string]interface{})
	if ss.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = ss.DefaultLanguageCode
	}
	if ss.Name != nil {
		objectMap["name"] = ss.Name
	}
	if ss.Description != nil {
		objectMap["description"] = ss.Description
	}
	if ss.Context != nil {
		objectMap["context"] = ss.Context
	}
	if ss.Inputs != nil {
		objectMap["inputs"] = ss.Inputs
	}
	if ss.Outputs != nil {
		objectMap["outputs"] = ss.Outputs
	}
	if ss.OdataType != "" {
		objectMap["@odata.type"] = ss.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return &ss, true
}

// AsSplitSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for SentimentSkill.
func (ss SentimentSkill) AsBasicSkill() (BasicSkill, bool) {
	return &ss, true
}

// ServiceCounters represents service-level resource counters and quotas.
type ServiceCounters struct {
	// DocumentCounter - Total number of documents across all indexes in the service.
	DocumentCounter *ResourceCounter `json:"documentCount,omitempty"`
	// IndexCounter - Total number of indexes.
	IndexCounter *ResourceCounter `json:"indexesCount,omitempty"`
	// IndexerCounter - Total number of indexers.
	IndexerCounter *ResourceCounter `json:"indexersCount,omitempty"`
	// DataSourceCounter - Total number of data sources.
	DataSourceCounter *ResourceCounter `json:"dataSourcesCount,omitempty"`
	// StorageSizeCounter - Total size of used storage in bytes.
	StorageSizeCounter *ResourceCounter `json:"storageSize,omitempty"`
	// SynonymMapCounter - Total number of synonym maps.
	SynonymMapCounter *ResourceCounter `json:"synonymMaps,omitempty"`
}

// ServiceLimits represents various service level limits.
type ServiceLimits struct {
	// MaxFieldsPerIndex - The maximum allowed fields per index.
	MaxFieldsPerIndex *int32 `json:"maxFieldsPerIndex,omitempty"`
	// MaxFieldNestingDepthPerIndex - The maximum depth which you can nest sub-fields in an index, including the top-level complex field. For example, a/b/c has a nesting depth of 3.
	MaxFieldNestingDepthPerIndex *int32 `json:"maxFieldNestingDepthPerIndex,omitempty"`
	// MaxComplexCollectionFieldsPerIndex - The maximum number of fields of type Collection(Edm.ComplexType) allowed in an index.
	MaxComplexCollectionFieldsPerIndex *int32 `json:"maxComplexCollectionFieldsPerIndex,omitempty"`
	// MaxComplexObjectsInCollectionsPerDocument - The maximum number of objects in complex collections allowed per document.
	MaxComplexObjectsInCollectionsPerDocument *int32 `json:"maxComplexObjectsInCollectionsPerDocument,omitempty"`
}

// ServiceStatistics response from a get service statistics request. If successful, it includes service
// level counters and limits.
type ServiceStatistics struct {
	autorest.Response `json:"-"`
	// Counters - Service level resource counters.
	Counters *ServiceCounters `json:"counters,omitempty"`
	// Limits - Service level general limits.
	Limits *ServiceLimits `json:"limits,omitempty"`
}

// ShaperSkill a skill for reshaping the outputs. It creates a complex type to support composite fields
// (also known as multipart fields).
type ShaperSkill struct {
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ShaperSkill.
func (ss ShaperSkill) MarshalJSON() ([]byte, error) {
	ss.OdataType = OdataTypeMicrosoftSkillsUtilShaperSkill
	objectMap := make(map[string]interface{})
	if ss.Name != nil {
		objectMap["name"] = ss.Name
	}
	if ss.Description != nil {
		objectMap["description"] = ss.Description
	}
	if ss.Context != nil {
		objectMap["context"] = ss.Context
	}
	if ss.Inputs != nil {
		objectMap["inputs"] = ss.Inputs
	}
	if ss.Outputs != nil {
		objectMap["outputs"] = ss.Outputs
	}
	if ss.OdataType != "" {
		objectMap["@odata.type"] = ss.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return &ss, true
}

// AsMergeSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for ShaperSkill.
func (ss ShaperSkill) AsBasicSkill() (BasicSkill, bool) {
	return &ss, true
}

// ShingleTokenFilter creates combinations of tokens as a single token. This token filter is implemented
// using Apache Lucene.
type ShingleTokenFilter struct {
	// MaxShingleSize - The maximum shingle size. Default and minimum value is 2.
	MaxShingleSize *int32 `json:"maxShingleSize,omitempty"`
	// MinShingleSize - The minimum shingle size. Default and minimum value is 2. Must be less than the value of maxShingleSize.
	MinShingleSize *int32 `json:"minShingleSize,omitempty"`
	// OutputUnigrams - A value indicating whether the output stream will contain the input tokens (unigrams) as well as shingles. Default is true.
	OutputUnigrams *bool `json:"outputUnigrams,omitempty"`
	// OutputUnigramsIfNoShingles - A value indicating whether to output unigrams for those times when no shingles are available. This property takes precedence when outputUnigrams is set to false. Default is false.
	OutputUnigramsIfNoShingles *bool `json:"outputUnigramsIfNoShingles,omitempty"`
	// TokenSeparator - The string to use when joining adjacent tokens to form a shingle. Default is a single space (" ").
	TokenSeparator *string `json:"tokenSeparator,omitempty"`
	// FilterToken - The string to insert for each position at which there is no token. Default is an underscore ("_").
	FilterToken *string `json:"filterToken,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for ShingleTokenFilter.
func (stf ShingleTokenFilter) MarshalJSON() ([]byte, error) {
	stf.OdataType = OdataTypeMicrosoftAzureSearchShingleTokenFilter
	objectMap := make(map[string]interface{})
	if stf.MaxShingleSize != nil {
		objectMap["maxShingleSize"] = stf.MaxShingleSize
	}
	if stf.MinShingleSize != nil {
		objectMap["minShingleSize"] = stf.MinShingleSize
	}
	if stf.OutputUnigrams != nil {
		objectMap["outputUnigrams"] = stf.OutputUnigrams
	}
	if stf.OutputUnigramsIfNoShingles != nil {
		objectMap["outputUnigramsIfNoShingles"] = stf.OutputUnigramsIfNoShingles
	}
	if stf.TokenSeparator != nil {
		objectMap["tokenSeparator"] = stf.TokenSeparator
	}
	if stf.FilterToken != nil {
		objectMap["filterToken"] = stf.FilterToken
	}
	if stf.Name != nil {
		objectMap["name"] = stf.Name
	}
	if stf.OdataType != "" {
		objectMap["@odata.type"] = stf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return &stf, true
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for ShingleTokenFilter.
func (stf ShingleTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &stf, true
}

// BasicSkill abstract base class for skills.
type BasicSkill interface {
	AsConditionalSkill() (*ConditionalSkill, bool)
	AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool)
	AsOcrSkill() (*OcrSkill, bool)
	AsImageAnalysisSkill() (*ImageAnalysisSkill, bool)
	AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool)
	AsShaperSkill() (*ShaperSkill, bool)
	AsMergeSkill() (*MergeSkill, bool)
	AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool)
	AsSentimentSkill() (*SentimentSkill, bool)
	AsSplitSkill() (*SplitSkill, bool)
	AsTextTranslationSkill() (*TextTranslationSkill, bool)
	AsWebAPISkill() (*WebAPISkill, bool)
	AsSkill() (*Skill, bool)
}

// Skill abstract base class for skills.
type Skill struct {
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

func unmarshalBasicSkill(body []byte) (BasicSkill, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeMicrosoftSkillsUtilConditionalSkill):
		var cs ConditionalSkill
		err := json.Unmarshal(body, &cs)
		return cs, err
	case string(OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill):
		var kpes KeyPhraseExtractionSkill
		err := json.Unmarshal(body, &kpes)
		return kpes, err
	case string(OdataTypeMicrosoftSkillsVisionOcrSkill):
		var osVar OcrSkill
		err := json.Unmarshal(body, &osVar)
		return osVar, err
	case string(OdataTypeMicrosoftSkillsVisionImageAnalysisSkill):
		var ias ImageAnalysisSkill
		err := json.Unmarshal(body, &ias)
		return ias, err
	case string(OdataTypeMicrosoftSkillsTextLanguageDetectionSkill):
		var lds LanguageDetectionSkill
		err := json.Unmarshal(body, &lds)
		return lds, err
	case string(OdataTypeMicrosoftSkillsUtilShaperSkill):
		var ss ShaperSkill
		err := json.Unmarshal(body, &ss)
		return ss, err
	case string(OdataTypeMicrosoftSkillsTextMergeSkill):
		var ms MergeSkill
		err := json.Unmarshal(body, &ms)
		return ms, err
	case string(OdataTypeMicrosoftSkillsTextEntityRecognitionSkill):
		var ers EntityRecognitionSkill
		err := json.Unmarshal(body, &ers)
		return ers, err
	case string(OdataTypeMicrosoftSkillsTextSentimentSkill):
		var ss SentimentSkill
		err := json.Unmarshal(body, &ss)
		return ss, err
	case string(OdataTypeMicrosoftSkillsTextSplitSkill):
		var ss SplitSkill
		err := json.Unmarshal(body, &ss)
		return ss, err
	case string(OdataTypeMicrosoftSkillsTextTranslationSkill):
		var tts TextTranslationSkill
		err := json.Unmarshal(body, &tts)
		return tts, err
	case string(OdataTypeMicrosoftSkillsCustomWebAPISkill):
		var was WebAPISkill
		err := json.Unmarshal(body, &was)
		return was, err
	default:
		var s Skill
		err := json.Unmarshal(body, &s)
		return s, err
	}
}
func unmarshalBasicSkillArray(body []byte) ([]BasicSkill, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	sArray := make([]BasicSkill, len(rawMessages))

	for index, rawMessage := range rawMessages {
		s, err := unmarshalBasicSkill(*rawMessage)
		if err != nil {
			return nil, err
		}
		sArray[index] = s
	}
	return sArray, nil
}

// MarshalJSON is the custom marshaler for Skill.
func (s Skill) MarshalJSON() ([]byte, error) {
	s.OdataType = OdataTypeSkill
	objectMap := make(map[string]interface{})
	if s.Name != nil {
		objectMap["name"] = s.Name
	}
	if s.Description != nil {
		objectMap["description"] = s.Description
	}
	if s.Context != nil {
		objectMap["context"] = s.Context
	}
	if s.Inputs != nil {
		objectMap["inputs"] = s.Inputs
	}
	if s.Outputs != nil {
		objectMap["outputs"] = s.Outputs
	}
	if s.OdataType != "" {
		objectMap["@odata.type"] = s.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for Skill.
func (s Skill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for Skill.
func (s Skill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for Skill.
func (s Skill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for Skill.
func (s Skill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for Skill.
func (s Skill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for Skill.
func (s Skill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for Skill.
func (s Skill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for Skill.
func (s Skill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for Skill.
func (s Skill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for Skill.
func (s Skill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for Skill.
func (s Skill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for Skill.
func (s Skill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for Skill.
func (s Skill) AsSkill() (*Skill, bool) {
	return &s, true
}

// AsBasicSkill is the BasicSkill implementation for Skill.
func (s Skill) AsBasicSkill() (BasicSkill, bool) {
	return &s, true
}

// Skillset a list of skills.
type Skillset struct {
	autorest.Response `json:"-"`
	// Name - The name of the skillset.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skillset.
	Description *string `json:"description,omitempty"`
	// Skills - A list of skills in the skillset.
	Skills *[]BasicSkill `json:"skills,omitempty"`
	// CognitiveServicesAccount - Details about cognitive services to be used when running skills.
	CognitiveServicesAccount BasicCognitiveServicesAccount `json:"cognitiveServices,omitempty"`
	// ETag - The ETag of the skillset.
	ETag *string `json:"@odata.etag,omitempty"`
}

// UnmarshalJSON is the custom unmarshaler for Skillset struct.
func (s *Skillset) UnmarshalJSON(body []byte) error {
	var m map[string]*json.RawMessage
	err := json.Unmarshal(body, &m)
	if err != nil {
		return err
	}
	for k, v := range m {
		switch k {
		case "name":
			if v != nil {
				var name string
				err = json.Unmarshal(*v, &name)
				if err != nil {
					return err
				}
				s.Name = &name
			}
		case "description":
			if v != nil {
				var description string
				err = json.Unmarshal(*v, &description)
				if err != nil {
					return err
				}
				s.Description = &description
			}
		case "skills":
			if v != nil {
				skills, err := unmarshalBasicSkillArray(*v)
				if err != nil {
					return err
				}
				s.Skills = &skills
			}
		case "cognitiveServices":
			if v != nil {
				cognitiveServicesAccount, err := unmarshalBasicCognitiveServicesAccount(*v)
				if err != nil {
					return err
				}
				s.CognitiveServicesAccount = cognitiveServicesAccount
			}
		case "@odata.etag":
			if v != nil {
				var eTag string
				err = json.Unmarshal(*v, &eTag)
				if err != nil {
					return err
				}
				s.ETag = &eTag
			}
		}
	}

	return nil
}

// SnowballTokenFilter a filter that stems words using a Snowball-generated stemmer. This token filter is
// implemented using Apache Lucene.
type SnowballTokenFilter struct {
	// Language - The language to use. Possible values include: 'SnowballTokenFilterLanguageArmenian', 'SnowballTokenFilterLanguageBasque', 'SnowballTokenFilterLanguageCatalan', 'SnowballTokenFilterLanguageDanish', 'SnowballTokenFilterLanguageDutch', 'SnowballTokenFilterLanguageEnglish', 'SnowballTokenFilterLanguageFinnish', 'SnowballTokenFilterLanguageFrench', 'SnowballTokenFilterLanguageGerman', 'SnowballTokenFilterLanguageGerman2', 'SnowballTokenFilterLanguageHungarian', 'SnowballTokenFilterLanguageItalian', 'SnowballTokenFilterLanguageKp', 'SnowballTokenFilterLanguageLovins', 'SnowballTokenFilterLanguageNorwegian', 'SnowballTokenFilterLanguagePorter', 'SnowballTokenFilterLanguagePortuguese', 'SnowballTokenFilterLanguageRomanian', 'SnowballTokenFilterLanguageRussian', 'SnowballTokenFilterLanguageSpanish', 'SnowballTokenFilterLanguageSwedish', 'SnowballTokenFilterLanguageTurkish'
	Language SnowballTokenFilterLanguage `json:"language,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SnowballTokenFilter.
func (stf SnowballTokenFilter) MarshalJSON() ([]byte, error) {
	stf.OdataType = OdataTypeMicrosoftAzureSearchSnowballTokenFilter
	objectMap := make(map[string]interface{})
	if stf.Language != "" {
		objectMap["language"] = stf.Language
	}
	if stf.Name != nil {
		objectMap["name"] = stf.Name
	}
	if stf.OdataType != "" {
		objectMap["@odata.type"] = stf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return &stf, true
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for SnowballTokenFilter.
func (stf SnowballTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &stf, true
}

// SoftDeleteColumnDeletionDetectionPolicy defines a data deletion detection policy that implements a
// soft-deletion strategy. It determines whether an item should be deleted based on the value of a
// designated 'soft delete' column.
type SoftDeleteColumnDeletionDetectionPolicy struct {
	// SoftDeleteColumnName - The name of the column to use for soft-deletion detection.
	SoftDeleteColumnName *string `json:"softDeleteColumnName,omitempty"`
	// SoftDeleteMarkerValue - The marker value that identifies an item as deleted.
	SoftDeleteMarkerValue *string `json:"softDeleteMarkerValue,omitempty"`
	// OdataType - Possible values include: 'OdataTypeDataDeletionDetectionPolicy', 'OdataTypeMicrosoftAzureSearchSoftDeleteColumnDeletionDetectionPolicy'
	OdataType OdataTypeBasicDataDeletionDetectionPolicy `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SoftDeleteColumnDeletionDetectionPolicy.
func (sdcddp SoftDeleteColumnDeletionDetectionPolicy) MarshalJSON() ([]byte, error) {
	sdcddp.OdataType = OdataTypeMicrosoftAzureSearchSoftDeleteColumnDeletionDetectionPolicy
	objectMap := make(map[string]interface{})
	if sdcddp.SoftDeleteColumnName != nil {
		objectMap["softDeleteColumnName"] = sdcddp.SoftDeleteColumnName
	}
	if sdcddp.SoftDeleteMarkerValue != nil {
		objectMap["softDeleteMarkerValue"] = sdcddp.SoftDeleteMarkerValue
	}
	if sdcddp.OdataType != "" {
		objectMap["@odata.type"] = sdcddp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsSoftDeleteColumnDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for SoftDeleteColumnDeletionDetectionPolicy.
func (sdcddp SoftDeleteColumnDeletionDetectionPolicy) AsSoftDeleteColumnDeletionDetectionPolicy() (*SoftDeleteColumnDeletionDetectionPolicy, bool) {
	return &sdcddp, true
}

// AsDataDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for SoftDeleteColumnDeletionDetectionPolicy.
func (sdcddp SoftDeleteColumnDeletionDetectionPolicy) AsDataDeletionDetectionPolicy() (*DataDeletionDetectionPolicy, bool) {
	return nil, false
}

// AsBasicDataDeletionDetectionPolicy is the BasicDataDeletionDetectionPolicy implementation for SoftDeleteColumnDeletionDetectionPolicy.
func (sdcddp SoftDeleteColumnDeletionDetectionPolicy) AsBasicDataDeletionDetectionPolicy() (BasicDataDeletionDetectionPolicy, bool) {
	return &sdcddp, true
}

// SplitSkill a skill to split a string into chunks of text.
type SplitSkill struct {
	// DefaultLanguageCode - A value indicating which language code to use. Default is en. Possible values include: 'SplitSkillLanguageDa', 'SplitSkillLanguageDe', 'SplitSkillLanguageEn', 'SplitSkillLanguageEs', 'SplitSkillLanguageFi', 'SplitSkillLanguageFr', 'SplitSkillLanguageIt', 'SplitSkillLanguageKo', 'SplitSkillLanguagePt'
	DefaultLanguageCode SplitSkillLanguage `json:"defaultLanguageCode,omitempty"`
	// TextSplitMode - A value indicating which split mode to perform. Possible values include: 'Pages', 'Sentences'
	TextSplitMode TextSplitMode `json:"textSplitMode,omitempty"`
	// MaximumPageLength - The desired maximum page length. Default is 10000.
	MaximumPageLength *int32 `json:"maximumPageLength,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SplitSkill.
func (ss SplitSkill) MarshalJSON() ([]byte, error) {
	ss.OdataType = OdataTypeMicrosoftSkillsTextSplitSkill
	objectMap := make(map[string]interface{})
	if ss.DefaultLanguageCode != "" {
		objectMap["defaultLanguageCode"] = ss.DefaultLanguageCode
	}
	if ss.TextSplitMode != "" {
		objectMap["textSplitMode"] = ss.TextSplitMode
	}
	if ss.MaximumPageLength != nil {
		objectMap["maximumPageLength"] = ss.MaximumPageLength
	}
	if ss.Name != nil {
		objectMap["name"] = ss.Name
	}
	if ss.Description != nil {
		objectMap["description"] = ss.Description
	}
	if ss.Context != nil {
		objectMap["context"] = ss.Context
	}
	if ss.Inputs != nil {
		objectMap["inputs"] = ss.Inputs
	}
	if ss.Outputs != nil {
		objectMap["outputs"] = ss.Outputs
	}
	if ss.OdataType != "" {
		objectMap["@odata.type"] = ss.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsSplitSkill() (*SplitSkill, bool) {
	return &ss, true
}

// AsTextTranslationSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for SplitSkill.
func (ss SplitSkill) AsBasicSkill() (BasicSkill, bool) {
	return &ss, true
}

// SQLIntegratedChangeTrackingPolicy defines a data change detection policy that captures changes using the
// Integrated Change Tracking feature of Azure SQL Database.
type SQLIntegratedChangeTrackingPolicy struct {
	// OdataType - Possible values include: 'OdataTypeDataChangeDetectionPolicy', 'OdataTypeMicrosoftAzureSearchHighWaterMarkChangeDetectionPolicy', 'OdataTypeMicrosoftAzureSearchSQLIntegratedChangeTrackingPolicy'
	OdataType OdataTypeBasicDataChangeDetectionPolicy `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SQLIntegratedChangeTrackingPolicy.
func (sictp SQLIntegratedChangeTrackingPolicy) MarshalJSON() ([]byte, error) {
	sictp.OdataType = OdataTypeMicrosoftAzureSearchSQLIntegratedChangeTrackingPolicy
	objectMap := make(map[string]interface{})
	if sictp.OdataType != "" {
		objectMap["@odata.type"] = sictp.OdataType
	}
	return json.Marshal(objectMap)
}

// AsHighWaterMarkChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for SQLIntegratedChangeTrackingPolicy.
func (sictp SQLIntegratedChangeTrackingPolicy) AsHighWaterMarkChangeDetectionPolicy() (*HighWaterMarkChangeDetectionPolicy, bool) {
	return nil, false
}

// AsSQLIntegratedChangeTrackingPolicy is the BasicDataChangeDetectionPolicy implementation for SQLIntegratedChangeTrackingPolicy.
func (sictp SQLIntegratedChangeTrackingPolicy) AsSQLIntegratedChangeTrackingPolicy() (*SQLIntegratedChangeTrackingPolicy, bool) {
	return &sictp, true
}

// AsDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for SQLIntegratedChangeTrackingPolicy.
func (sictp SQLIntegratedChangeTrackingPolicy) AsDataChangeDetectionPolicy() (*DataChangeDetectionPolicy, bool) {
	return nil, false
}

// AsBasicDataChangeDetectionPolicy is the BasicDataChangeDetectionPolicy implementation for SQLIntegratedChangeTrackingPolicy.
func (sictp SQLIntegratedChangeTrackingPolicy) AsBasicDataChangeDetectionPolicy() (BasicDataChangeDetectionPolicy, bool) {
	return &sictp, true
}

// StandardAnalyzer standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase filter
// and stop filter.
type StandardAnalyzer struct {
	// MaxTokenLength - The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Stopwords - A list of stopwords.
	Stopwords *[]string `json:"stopwords,omitempty"`
	// Name - The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeAnalyzer', 'OdataTypeMicrosoftAzureSearchCustomAnalyzer', 'OdataTypeMicrosoftAzureSearchPatternAnalyzer', 'OdataTypeMicrosoftAzureSearchStandardAnalyzer', 'OdataTypeMicrosoftAzureSearchStopAnalyzer'
	OdataType OdataType `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StandardAnalyzer.
func (sa StandardAnalyzer) MarshalJSON() ([]byte, error) {
	sa.OdataType = OdataTypeMicrosoftAzureSearchStandardAnalyzer
	objectMap := make(map[string]interface{})
	if sa.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = sa.MaxTokenLength
	}
	if sa.Stopwords != nil {
		objectMap["stopwords"] = sa.Stopwords
	}
	if sa.Name != nil {
		objectMap["name"] = sa.Name
	}
	if sa.OdataType != "" {
		objectMap["@odata.type"] = sa.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomAnalyzer is the BasicAnalyzer implementation for StandardAnalyzer.
func (sa StandardAnalyzer) AsCustomAnalyzer() (*CustomAnalyzer, bool) {
	return nil, false
}

// AsPatternAnalyzer is the BasicAnalyzer implementation for StandardAnalyzer.
func (sa StandardAnalyzer) AsPatternAnalyzer() (*PatternAnalyzer, bool) {
	return nil, false
}

// AsStandardAnalyzer is the BasicAnalyzer implementation for StandardAnalyzer.
func (sa StandardAnalyzer) AsStandardAnalyzer() (*StandardAnalyzer, bool) {
	return &sa, true
}

// AsStopAnalyzer is the BasicAnalyzer implementation for StandardAnalyzer.
func (sa StandardAnalyzer) AsStopAnalyzer() (*StopAnalyzer, bool) {
	return nil, false
}

// AsAnalyzer is the BasicAnalyzer implementation for StandardAnalyzer.
func (sa StandardAnalyzer) AsAnalyzer() (*Analyzer, bool) {
	return nil, false
}

// AsBasicAnalyzer is the BasicAnalyzer implementation for StandardAnalyzer.
func (sa StandardAnalyzer) AsBasicAnalyzer() (BasicAnalyzer, bool) {
	return &sa, true
}

// StandardTokenizer breaks text following the Unicode Text Segmentation rules. This tokenizer is
// implemented using Apache Lucene.
type StandardTokenizer struct {
	// MaxTokenLength - The maximum token length. Default is 255. Tokens longer than the maximum length are split.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StandardTokenizer.
func (st StandardTokenizer) MarshalJSON() ([]byte, error) {
	st.OdataType = OdataTypeMicrosoftAzureSearchStandardTokenizer
	objectMap := make(map[string]interface{})
	if st.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = st.MaxTokenLength
	}
	if st.Name != nil {
		objectMap["name"] = st.Name
	}
	if st.OdataType != "" {
		objectMap["@odata.type"] = st.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return &st, true
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for StandardTokenizer.
func (st StandardTokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &st, true
}

// StandardTokenizerV2 breaks text following the Unicode Text Segmentation rules. This tokenizer is
// implemented using Apache Lucene.
type StandardTokenizerV2 struct {
	// MaxTokenLength - The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StandardTokenizerV2.
func (stv StandardTokenizerV2) MarshalJSON() ([]byte, error) {
	stv.OdataType = OdataTypeMicrosoftAzureSearchStandardTokenizerV2
	objectMap := make(map[string]interface{})
	if stv.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = stv.MaxTokenLength
	}
	if stv.Name != nil {
		objectMap["name"] = stv.Name
	}
	if stv.OdataType != "" {
		objectMap["@odata.type"] = stv.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return &stv, true
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for StandardTokenizerV2.
func (stv StandardTokenizerV2) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &stv, true
}

// StemmerOverrideTokenFilter provides the ability to override other stemming filters with custom
// dictionary-based stemming. Any dictionary-stemmed terms will be marked as keywords so that they will not
// be stemmed with stemmers down the chain. Must be placed before any stemming filters. This token filter
// is implemented using Apache Lucene.
type StemmerOverrideTokenFilter struct {
	// Rules - A list of stemming rules in the following format: "word => stem", for example: "ran => run".
	Rules *[]string `json:"rules,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) MarshalJSON() ([]byte, error) {
	sotf.OdataType = OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter
	objectMap := make(map[string]interface{})
	if sotf.Rules != nil {
		objectMap["rules"] = sotf.Rules
	}
	if sotf.Name != nil {
		objectMap["name"] = sotf.Name
	}
	if sotf.OdataType != "" {
		objectMap["@odata.type"] = sotf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return &sotf, true
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for StemmerOverrideTokenFilter.
func (sotf StemmerOverrideTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &sotf, true
}

// StemmerTokenFilter language specific stemming filter. This token filter is implemented using Apache
// Lucene.
type StemmerTokenFilter struct {
	// Language - The language to use. Possible values include: 'StemmerTokenFilterLanguageArabic', 'StemmerTokenFilterLanguageArmenian', 'StemmerTokenFilterLanguageBasque', 'StemmerTokenFilterLanguageBrazilian', 'StemmerTokenFilterLanguageBulgarian', 'StemmerTokenFilterLanguageCatalan', 'StemmerTokenFilterLanguageCzech', 'StemmerTokenFilterLanguageDanish', 'StemmerTokenFilterLanguageDutch', 'StemmerTokenFilterLanguageDutchKp', 'StemmerTokenFilterLanguageEnglish', 'StemmerTokenFilterLanguageLightEnglish', 'StemmerTokenFilterLanguageMinimalEnglish', 'StemmerTokenFilterLanguagePossessiveEnglish', 'StemmerTokenFilterLanguagePorter2', 'StemmerTokenFilterLanguageLovins', 'StemmerTokenFilterLanguageFinnish', 'StemmerTokenFilterLanguageLightFinnish', 'StemmerTokenFilterLanguageFrench', 'StemmerTokenFilterLanguageLightFrench', 'StemmerTokenFilterLanguageMinimalFrench', 'StemmerTokenFilterLanguageGalician', 'StemmerTokenFilterLanguageMinimalGalician', 'StemmerTokenFilterLanguageGerman', 'StemmerTokenFilterLanguageGerman2', 'StemmerTokenFilterLanguageLightGerman', 'StemmerTokenFilterLanguageMinimalGerman', 'StemmerTokenFilterLanguageGreek', 'StemmerTokenFilterLanguageHindi', 'StemmerTokenFilterLanguageHungarian', 'StemmerTokenFilterLanguageLightHungarian', 'StemmerTokenFilterLanguageIndonesian', 'StemmerTokenFilterLanguageIrish', 'StemmerTokenFilterLanguageItalian', 'StemmerTokenFilterLanguageLightItalian', 'StemmerTokenFilterLanguageSorani', 'StemmerTokenFilterLanguageLatvian', 'StemmerTokenFilterLanguageNorwegian', 'StemmerTokenFilterLanguageLightNorwegian', 'StemmerTokenFilterLanguageMinimalNorwegian', 'StemmerTokenFilterLanguageLightNynorsk', 'StemmerTokenFilterLanguageMinimalNynorsk', 'StemmerTokenFilterLanguagePortuguese', 'StemmerTokenFilterLanguageLightPortuguese', 'StemmerTokenFilterLanguageMinimalPortuguese', 'StemmerTokenFilterLanguagePortugueseRslp', 'StemmerTokenFilterLanguageRomanian', 'StemmerTokenFilterLanguageRussian', 'StemmerTokenFilterLanguageLightRussian', 'StemmerTokenFilterLanguageSpanish', 'StemmerTokenFilterLanguageLightSpanish', 'StemmerTokenFilterLanguageSwedish', 'StemmerTokenFilterLanguageLightSwedish', 'StemmerTokenFilterLanguageTurkish'
	Language StemmerTokenFilterLanguage `json:"language,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StemmerTokenFilter.
func (stf StemmerTokenFilter) MarshalJSON() ([]byte, error) {
	stf.OdataType = OdataTypeMicrosoftAzureSearchStemmerTokenFilter
	objectMap := make(map[string]interface{})
	if stf.Language != "" {
		objectMap["language"] = stf.Language
	}
	if stf.Name != nil {
		objectMap["name"] = stf.Name
	}
	if stf.OdataType != "" {
		objectMap["@odata.type"] = stf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return &stf, true
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for StemmerTokenFilter.
func (stf StemmerTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &stf, true
}

// StopAnalyzer divides text at non-letters; Applies the lowercase and stopword token filters. This
// analyzer is implemented using Apache Lucene.
type StopAnalyzer struct {
	// Stopwords - A list of stopwords.
	Stopwords *[]string `json:"stopwords,omitempty"`
	// Name - The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeAnalyzer', 'OdataTypeMicrosoftAzureSearchCustomAnalyzer', 'OdataTypeMicrosoftAzureSearchPatternAnalyzer', 'OdataTypeMicrosoftAzureSearchStandardAnalyzer', 'OdataTypeMicrosoftAzureSearchStopAnalyzer'
	OdataType OdataType `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StopAnalyzer.
func (sa StopAnalyzer) MarshalJSON() ([]byte, error) {
	sa.OdataType = OdataTypeMicrosoftAzureSearchStopAnalyzer
	objectMap := make(map[string]interface{})
	if sa.Stopwords != nil {
		objectMap["stopwords"] = sa.Stopwords
	}
	if sa.Name != nil {
		objectMap["name"] = sa.Name
	}
	if sa.OdataType != "" {
		objectMap["@odata.type"] = sa.OdataType
	}
	return json.Marshal(objectMap)
}

// AsCustomAnalyzer is the BasicAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsCustomAnalyzer() (*CustomAnalyzer, bool) {
	return nil, false
}

// AsPatternAnalyzer is the BasicAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsPatternAnalyzer() (*PatternAnalyzer, bool) {
	return nil, false
}

// AsStandardAnalyzer is the BasicAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsStandardAnalyzer() (*StandardAnalyzer, bool) {
	return nil, false
}

// AsStopAnalyzer is the BasicAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsStopAnalyzer() (*StopAnalyzer, bool) {
	return &sa, true
}

// AsAnalyzer is the BasicAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsAnalyzer() (*Analyzer, bool) {
	return nil, false
}

// AsBasicAnalyzer is the BasicAnalyzer implementation for StopAnalyzer.
func (sa StopAnalyzer) AsBasicAnalyzer() (BasicAnalyzer, bool) {
	return &sa, true
}

// StopwordsTokenFilter removes stop words from a token stream. This token filter is implemented using
// Apache Lucene.
type StopwordsTokenFilter struct {
	// Stopwords - The list of stopwords. This property and the stopwords list property cannot both be set.
	Stopwords *[]string `json:"stopwords,omitempty"`
	// StopwordsList - A predefined list of stopwords to use. This property and the stopwords property cannot both be set. Default is English. Possible values include: 'StopwordsListArabic', 'StopwordsListArmenian', 'StopwordsListBasque', 'StopwordsListBrazilian', 'StopwordsListBulgarian', 'StopwordsListCatalan', 'StopwordsListCzech', 'StopwordsListDanish', 'StopwordsListDutch', 'StopwordsListEnglish', 'StopwordsListFinnish', 'StopwordsListFrench', 'StopwordsListGalician', 'StopwordsListGerman', 'StopwordsListGreek', 'StopwordsListHindi', 'StopwordsListHungarian', 'StopwordsListIndonesian', 'StopwordsListIrish', 'StopwordsListItalian', 'StopwordsListLatvian', 'StopwordsListNorwegian', 'StopwordsListPersian', 'StopwordsListPortuguese', 'StopwordsListRomanian', 'StopwordsListRussian', 'StopwordsListSorani', 'StopwordsListSpanish', 'StopwordsListSwedish', 'StopwordsListThai', 'StopwordsListTurkish'
	StopwordsList StopwordsList `json:"stopwordsList,omitempty"`
	// IgnoreCase - A value indicating whether to ignore case. If true, all words are converted to lower case first. Default is false.
	IgnoreCase *bool `json:"ignoreCase,omitempty"`
	// RemoveTrailingStopWords - A value indicating whether to ignore the last search term if it's a stop word. Default is true.
	RemoveTrailingStopWords *bool `json:"removeTrailing,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) MarshalJSON() ([]byte, error) {
	stf.OdataType = OdataTypeMicrosoftAzureSearchStopwordsTokenFilter
	objectMap := make(map[string]interface{})
	if stf.Stopwords != nil {
		objectMap["stopwords"] = stf.Stopwords
	}
	if stf.StopwordsList != "" {
		objectMap["stopwordsList"] = stf.StopwordsList
	}
	if stf.IgnoreCase != nil {
		objectMap["ignoreCase"] = stf.IgnoreCase
	}
	if stf.RemoveTrailingStopWords != nil {
		objectMap["removeTrailing"] = stf.RemoveTrailingStopWords
	}
	if stf.Name != nil {
		objectMap["name"] = stf.Name
	}
	if stf.OdataType != "" {
		objectMap["@odata.type"] = stf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return &stf, true
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for StopwordsTokenFilter.
func (stf StopwordsTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &stf, true
}

// Suggester defines how the Suggest API should apply to a group of fields in the index.
type Suggester struct {
	// Name - The name of the suggester.
	Name *string `json:"name,omitempty"`
	// SearchMode - A value indicating the capabilities of the suggester.
	SearchMode *string `json:"searchMode,omitempty"`
	// SourceFields - The list of field names to which the suggester applies. Each field must be searchable.
	SourceFields *[]string `json:"sourceFields,omitempty"`
}

// SynonymMap represents a synonym map definition.
type SynonymMap struct {
	autorest.Response `json:"-"`
	// Name - The name of the synonym map.
	Name *string `json:"name,omitempty"`
	// Format - The format of the synonym map. Only the 'solr' format is currently supported.
	Format *string `json:"format,omitempty"`
	// Synonyms - A series of synonym rules in the specified synonym map format. The rules must be separated by newlines.
	Synonyms *string `json:"synonyms,omitempty"`
	// ETag - The ETag of the synonym map.
	ETag *string `json:"@odata.etag,omitempty"`
}

// SynonymTokenFilter matches single or multi-word synonyms in a token stream. This token filter is
// implemented using Apache Lucene.
type SynonymTokenFilter struct {
	// Synonyms - A list of synonyms in following one of two formats: 1. incredible, unbelievable, fabulous => amazing - all terms on the left side of => symbol will be replaced with all terms on its right side; 2. incredible, unbelievable, fabulous, amazing - comma separated list of equivalent words. Set the expand option to change how this list is interpreted.
	Synonyms *[]string `json:"synonyms,omitempty"`
	// IgnoreCase - A value indicating whether to case-fold input for matching. Default is false.
	IgnoreCase *bool `json:"ignoreCase,omitempty"`
	// Expand - A value indicating whether all words in the list of synonyms (if => notation is not used) will map to one another. If true, all words in the list of synonyms (if => notation is not used) will map to one another. The following list: incredible, unbelievable, fabulous, amazing is equivalent to: incredible, unbelievable, fabulous, amazing => incredible, unbelievable, fabulous, amazing. If false, the following list: incredible, unbelievable, fabulous, amazing will be equivalent to: incredible, unbelievable, fabulous, amazing => incredible. Default is true.
	Expand *bool `json:"expand,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for SynonymTokenFilter.
func (stf SynonymTokenFilter) MarshalJSON() ([]byte, error) {
	stf.OdataType = OdataTypeMicrosoftAzureSearchSynonymTokenFilter
	objectMap := make(map[string]interface{})
	if stf.Synonyms != nil {
		objectMap["synonyms"] = stf.Synonyms
	}
	if stf.IgnoreCase != nil {
		objectMap["ignoreCase"] = stf.IgnoreCase
	}
	if stf.Expand != nil {
		objectMap["expand"] = stf.Expand
	}
	if stf.Name != nil {
		objectMap["name"] = stf.Name
	}
	if stf.OdataType != "" {
		objectMap["@odata.type"] = stf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return &stf, true
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for SynonymTokenFilter.
func (stf SynonymTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &stf, true
}

// TagScoringFunction defines a function that boosts scores of documents with string values matching a
// given list of tags.
type TagScoringFunction struct {
	// Parameters - Parameter values for the tag scoring function.
	Parameters *TagScoringParameters `json:"tag,omitempty"`
	// FieldName - The name of the field used as input to the scoring function.
	FieldName *string `json:"fieldName,omitempty"`
	// Boost - A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64 `json:"boost,omitempty"`
	// Interpolation - A value indicating how boosting will be interpolated across document scores; defaults to "Linear". Possible values include: 'Linear', 'Constant', 'Quadratic', 'Logarithmic'
	Interpolation ScoringFunctionInterpolation `json:"interpolation,omitempty"`
	// Type - Possible values include: 'TypeScoringFunction', 'TypeDistance', 'TypeFreshness', 'TypeMagnitude', 'TypeTag'
	Type Type `json:"type,omitempty"`
}

// MarshalJSON is the custom marshaler for TagScoringFunction.
func (tsf TagScoringFunction) MarshalJSON() ([]byte, error) {
	tsf.Type = TypeTag
	objectMap := make(map[string]interface{})
	if tsf.Parameters != nil {
		objectMap["tag"] = tsf.Parameters
	}
	if tsf.FieldName != nil {
		objectMap["fieldName"] = tsf.FieldName
	}
	if tsf.Boost != nil {
		objectMap["boost"] = tsf.Boost
	}
	if tsf.Interpolation != "" {
		objectMap["interpolation"] = tsf.Interpolation
	}
	if tsf.Type != "" {
		objectMap["type"] = tsf.Type
	}
	return json.Marshal(objectMap)
}

// AsDistanceScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsDistanceScoringFunction() (*DistanceScoringFunction, bool) {
	return nil, false
}

// AsFreshnessScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsFreshnessScoringFunction() (*FreshnessScoringFunction, bool) {
	return nil, false
}

// AsMagnitudeScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsMagnitudeScoringFunction() (*MagnitudeScoringFunction, bool) {
	return nil, false
}

// AsTagScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsTagScoringFunction() (*TagScoringFunction, bool) {
	return &tsf, true
}

// AsScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsScoringFunction() (*ScoringFunction, bool) {
	return nil, false
}

// AsBasicScoringFunction is the BasicScoringFunction implementation for TagScoringFunction.
func (tsf TagScoringFunction) AsBasicScoringFunction() (BasicScoringFunction, bool) {
	return &tsf, true
}

// TagScoringParameters provides parameter values to a tag scoring function.
type TagScoringParameters struct {
	// TagsParameter - The name of the parameter passed in search queries to specify the list of tags to compare against the target field.
	TagsParameter *string `json:"tagsParameter,omitempty"`
}

// TextTranslationSkill a skill to translate text from one language to another.
type TextTranslationSkill struct {
	// DefaultToLanguageCode - The language code to translate documents into for documents that don't specify the to language explicitly. Possible values include: 'TextTranslationSkillLanguageAf', 'TextTranslationSkillLanguageAr', 'TextTranslationSkillLanguageBn', 'TextTranslationSkillLanguageBs', 'TextTranslationSkillLanguageBg', 'TextTranslationSkillLanguageYue', 'TextTranslationSkillLanguageCa', 'TextTranslationSkillLanguageZhHans', 'TextTranslationSkillLanguageZhHant', 'TextTranslationSkillLanguageHr', 'TextTranslationSkillLanguageCs', 'TextTranslationSkillLanguageDa', 'TextTranslationSkillLanguageNl', 'TextTranslationSkillLanguageEn', 'TextTranslationSkillLanguageEt', 'TextTranslationSkillLanguageFj', 'TextTranslationSkillLanguageFil', 'TextTranslationSkillLanguageFi', 'TextTranslationSkillLanguageFr', 'TextTranslationSkillLanguageDe', 'TextTranslationSkillLanguageEl', 'TextTranslationSkillLanguageHt', 'TextTranslationSkillLanguageHe', 'TextTranslationSkillLanguageHi', 'TextTranslationSkillLanguageMww', 'TextTranslationSkillLanguageHu', 'TextTranslationSkillLanguageIs', 'TextTranslationSkillLanguageID', 'TextTranslationSkillLanguageIt', 'TextTranslationSkillLanguageJa', 'TextTranslationSkillLanguageSw', 'TextTranslationSkillLanguageTlh', 'TextTranslationSkillLanguageKo', 'TextTranslationSkillLanguageLv', 'TextTranslationSkillLanguageLt', 'TextTranslationSkillLanguageMg', 'TextTranslationSkillLanguageMs', 'TextTranslationSkillLanguageMt', 'TextTranslationSkillLanguageNb', 'TextTranslationSkillLanguageFa', 'TextTranslationSkillLanguagePl', 'TextTranslationSkillLanguagePt', 'TextTranslationSkillLanguageOtq', 'TextTranslationSkillLanguageRo', 'TextTranslationSkillLanguageRu', 'TextTranslationSkillLanguageSm', 'TextTranslationSkillLanguageSrCyrl', 'TextTranslationSkillLanguageSrLatn', 'TextTranslationSkillLanguageSk', 'TextTranslationSkillLanguageSl', 'TextTranslationSkillLanguageEs', 'TextTranslationSkillLanguageSv', 'TextTranslationSkillLanguageTy', 'TextTranslationSkillLanguageTa', 'TextTranslationSkillLanguageTe', 'TextTranslationSkillLanguageTh', 'TextTranslationSkillLanguageTo', 'TextTranslationSkillLanguageTr', 'TextTranslationSkillLanguageUk', 'TextTranslationSkillLanguageUr', 'TextTranslationSkillLanguageVi', 'TextTranslationSkillLanguageCy', 'TextTranslationSkillLanguageYua'
	DefaultToLanguageCode TextTranslationSkillLanguage `json:"defaultToLanguageCode,omitempty"`
	// DefaultFromLanguageCode - The language code to translate documents from for documents that don't specify the from language explicitly. Possible values include: 'TextTranslationSkillLanguageAf', 'TextTranslationSkillLanguageAr', 'TextTranslationSkillLanguageBn', 'TextTranslationSkillLanguageBs', 'TextTranslationSkillLanguageBg', 'TextTranslationSkillLanguageYue', 'TextTranslationSkillLanguageCa', 'TextTranslationSkillLanguageZhHans', 'TextTranslationSkillLanguageZhHant', 'TextTranslationSkillLanguageHr', 'TextTranslationSkillLanguageCs', 'TextTranslationSkillLanguageDa', 'TextTranslationSkillLanguageNl', 'TextTranslationSkillLanguageEn', 'TextTranslationSkillLanguageEt', 'TextTranslationSkillLanguageFj', 'TextTranslationSkillLanguageFil', 'TextTranslationSkillLanguageFi', 'TextTranslationSkillLanguageFr', 'TextTranslationSkillLanguageDe', 'TextTranslationSkillLanguageEl', 'TextTranslationSkillLanguageHt', 'TextTranslationSkillLanguageHe', 'TextTranslationSkillLanguageHi', 'TextTranslationSkillLanguageMww', 'TextTranslationSkillLanguageHu', 'TextTranslationSkillLanguageIs', 'TextTranslationSkillLanguageID', 'TextTranslationSkillLanguageIt', 'TextTranslationSkillLanguageJa', 'TextTranslationSkillLanguageSw', 'TextTranslationSkillLanguageTlh', 'TextTranslationSkillLanguageKo', 'TextTranslationSkillLanguageLv', 'TextTranslationSkillLanguageLt', 'TextTranslationSkillLanguageMg', 'TextTranslationSkillLanguageMs', 'TextTranslationSkillLanguageMt', 'TextTranslationSkillLanguageNb', 'TextTranslationSkillLanguageFa', 'TextTranslationSkillLanguagePl', 'TextTranslationSkillLanguagePt', 'TextTranslationSkillLanguageOtq', 'TextTranslationSkillLanguageRo', 'TextTranslationSkillLanguageRu', 'TextTranslationSkillLanguageSm', 'TextTranslationSkillLanguageSrCyrl', 'TextTranslationSkillLanguageSrLatn', 'TextTranslationSkillLanguageSk', 'TextTranslationSkillLanguageSl', 'TextTranslationSkillLanguageEs', 'TextTranslationSkillLanguageSv', 'TextTranslationSkillLanguageTy', 'TextTranslationSkillLanguageTa', 'TextTranslationSkillLanguageTe', 'TextTranslationSkillLanguageTh', 'TextTranslationSkillLanguageTo', 'TextTranslationSkillLanguageTr', 'TextTranslationSkillLanguageUk', 'TextTranslationSkillLanguageUr', 'TextTranslationSkillLanguageVi', 'TextTranslationSkillLanguageCy', 'TextTranslationSkillLanguageYua'
	DefaultFromLanguageCode TextTranslationSkillLanguage `json:"defaultFromLanguageCode,omitempty"`
	// SuggestedFrom - The language code to translate documents from when neither the fromLanguageCode input nor the defaultFromLanguageCode parameter are provided, and the automatic language detection is unsuccessful. Default is en. Possible values include: 'TextTranslationSkillLanguageAf', 'TextTranslationSkillLanguageAr', 'TextTranslationSkillLanguageBn', 'TextTranslationSkillLanguageBs', 'TextTranslationSkillLanguageBg', 'TextTranslationSkillLanguageYue', 'TextTranslationSkillLanguageCa', 'TextTranslationSkillLanguageZhHans', 'TextTranslationSkillLanguageZhHant', 'TextTranslationSkillLanguageHr', 'TextTranslationSkillLanguageCs', 'TextTranslationSkillLanguageDa', 'TextTranslationSkillLanguageNl', 'TextTranslationSkillLanguageEn', 'TextTranslationSkillLanguageEt', 'TextTranslationSkillLanguageFj', 'TextTranslationSkillLanguageFil', 'TextTranslationSkillLanguageFi', 'TextTranslationSkillLanguageFr', 'TextTranslationSkillLanguageDe', 'TextTranslationSkillLanguageEl', 'TextTranslationSkillLanguageHt', 'TextTranslationSkillLanguageHe', 'TextTranslationSkillLanguageHi', 'TextTranslationSkillLanguageMww', 'TextTranslationSkillLanguageHu', 'TextTranslationSkillLanguageIs', 'TextTranslationSkillLanguageID', 'TextTranslationSkillLanguageIt', 'TextTranslationSkillLanguageJa', 'TextTranslationSkillLanguageSw', 'TextTranslationSkillLanguageTlh', 'TextTranslationSkillLanguageKo', 'TextTranslationSkillLanguageLv', 'TextTranslationSkillLanguageLt', 'TextTranslationSkillLanguageMg', 'TextTranslationSkillLanguageMs', 'TextTranslationSkillLanguageMt', 'TextTranslationSkillLanguageNb', 'TextTranslationSkillLanguageFa', 'TextTranslationSkillLanguagePl', 'TextTranslationSkillLanguagePt', 'TextTranslationSkillLanguageOtq', 'TextTranslationSkillLanguageRo', 'TextTranslationSkillLanguageRu', 'TextTranslationSkillLanguageSm', 'TextTranslationSkillLanguageSrCyrl', 'TextTranslationSkillLanguageSrLatn', 'TextTranslationSkillLanguageSk', 'TextTranslationSkillLanguageSl', 'TextTranslationSkillLanguageEs', 'TextTranslationSkillLanguageSv', 'TextTranslationSkillLanguageTy', 'TextTranslationSkillLanguageTa', 'TextTranslationSkillLanguageTe', 'TextTranslationSkillLanguageTh', 'TextTranslationSkillLanguageTo', 'TextTranslationSkillLanguageTr', 'TextTranslationSkillLanguageUk', 'TextTranslationSkillLanguageUr', 'TextTranslationSkillLanguageVi', 'TextTranslationSkillLanguageCy', 'TextTranslationSkillLanguageYua'
	SuggestedFrom TextTranslationSkillLanguage `json:"suggestedFrom,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for TextTranslationSkill.
func (tts TextTranslationSkill) MarshalJSON() ([]byte, error) {
	tts.OdataType = OdataTypeMicrosoftSkillsTextTranslationSkill
	objectMap := make(map[string]interface{})
	if tts.DefaultToLanguageCode != "" {
		objectMap["defaultToLanguageCode"] = tts.DefaultToLanguageCode
	}
	if tts.DefaultFromLanguageCode != "" {
		objectMap["defaultFromLanguageCode"] = tts.DefaultFromLanguageCode
	}
	if tts.SuggestedFrom != "" {
		objectMap["suggestedFrom"] = tts.SuggestedFrom
	}
	if tts.Name != nil {
		objectMap["name"] = tts.Name
	}
	if tts.Description != nil {
		objectMap["description"] = tts.Description
	}
	if tts.Context != nil {
		objectMap["context"] = tts.Context
	}
	if tts.Inputs != nil {
		objectMap["inputs"] = tts.Inputs
	}
	if tts.Outputs != nil {
		objectMap["outputs"] = tts.Outputs
	}
	if tts.OdataType != "" {
		objectMap["@odata.type"] = tts.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return &tts, true
}

// AsWebAPISkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return nil, false
}

// AsSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for TextTranslationSkill.
func (tts TextTranslationSkill) AsBasicSkill() (BasicSkill, bool) {
	return &tts, true
}

// TextWeights defines weights on index fields for which matches should boost scoring in search queries.
type TextWeights struct {
	// Weights - The dictionary of per-field weights to boost document scoring. The keys are field names and the values are the weights for each field.
	Weights map[string]*float64 `json:"weights"`
}

// MarshalJSON is the custom marshaler for TextWeights.
func (tw TextWeights) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	if tw.Weights != nil {
		objectMap["weights"] = tw.Weights
	}
	return json.Marshal(objectMap)
}

// BasicTokenFilter abstract base class for token filters.
type BasicTokenFilter interface {
	AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool)
	AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool)
	AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool)
	AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool)
	AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool)
	AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool)
	AsElisionTokenFilter() (*ElisionTokenFilter, bool)
	AsKeepTokenFilter() (*KeepTokenFilter, bool)
	AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool)
	AsLengthTokenFilter() (*LengthTokenFilter, bool)
	AsLimitTokenFilter() (*LimitTokenFilter, bool)
	AsNGramTokenFilter() (*NGramTokenFilter, bool)
	AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool)
	AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool)
	AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool)
	AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool)
	AsShingleTokenFilter() (*ShingleTokenFilter, bool)
	AsSnowballTokenFilter() (*SnowballTokenFilter, bool)
	AsStemmerTokenFilter() (*StemmerTokenFilter, bool)
	AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool)
	AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool)
	AsSynonymTokenFilter() (*SynonymTokenFilter, bool)
	AsTruncateTokenFilter() (*TruncateTokenFilter, bool)
	AsUniqueTokenFilter() (*UniqueTokenFilter, bool)
	AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool)
	AsTokenFilter() (*TokenFilter, bool)
}

// TokenFilter abstract base class for token filters.
type TokenFilter struct {
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

func unmarshalBasicTokenFilter(body []byte) (BasicTokenFilter, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter):
		var aftf ASCIIFoldingTokenFilter
		err := json.Unmarshal(body, &aftf)
		return aftf, err
	case string(OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter):
		var cbtf CjkBigramTokenFilter
		err := json.Unmarshal(body, &cbtf)
		return cbtf, err
	case string(OdataTypeMicrosoftAzureSearchCommonGramTokenFilter):
		var cgtf CommonGramTokenFilter
		err := json.Unmarshal(body, &cgtf)
		return cgtf, err
	case string(OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter):
		var ddtf DictionaryDecompounderTokenFilter
		err := json.Unmarshal(body, &ddtf)
		return ddtf, err
	case string(OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter):
		var engtf EdgeNGramTokenFilter
		err := json.Unmarshal(body, &engtf)
		return engtf, err
	case string(OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2):
		var engtfv EdgeNGramTokenFilterV2
		err := json.Unmarshal(body, &engtfv)
		return engtfv, err
	case string(OdataTypeMicrosoftAzureSearchElisionTokenFilter):
		var etf ElisionTokenFilter
		err := json.Unmarshal(body, &etf)
		return etf, err
	case string(OdataTypeMicrosoftAzureSearchKeepTokenFilter):
		var ktf KeepTokenFilter
		err := json.Unmarshal(body, &ktf)
		return ktf, err
	case string(OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter):
		var kmtf KeywordMarkerTokenFilter
		err := json.Unmarshal(body, &kmtf)
		return kmtf, err
	case string(OdataTypeMicrosoftAzureSearchLengthTokenFilter):
		var ltf LengthTokenFilter
		err := json.Unmarshal(body, &ltf)
		return ltf, err
	case string(OdataTypeMicrosoftAzureSearchLimitTokenFilter):
		var ltf LimitTokenFilter
		err := json.Unmarshal(body, &ltf)
		return ltf, err
	case string(OdataTypeMicrosoftAzureSearchNGramTokenFilter):
		var ngtf NGramTokenFilter
		err := json.Unmarshal(body, &ngtf)
		return ngtf, err
	case string(OdataTypeMicrosoftAzureSearchNGramTokenFilterV2):
		var ngtfv NGramTokenFilterV2
		err := json.Unmarshal(body, &ngtfv)
		return ngtfv, err
	case string(OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter):
		var pctf PatternCaptureTokenFilter
		err := json.Unmarshal(body, &pctf)
		return pctf, err
	case string(OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter):
		var prtf PatternReplaceTokenFilter
		err := json.Unmarshal(body, &prtf)
		return prtf, err
	case string(OdataTypeMicrosoftAzureSearchPhoneticTokenFilter):
		var ptf PhoneticTokenFilter
		err := json.Unmarshal(body, &ptf)
		return ptf, err
	case string(OdataTypeMicrosoftAzureSearchShingleTokenFilter):
		var stf ShingleTokenFilter
		err := json.Unmarshal(body, &stf)
		return stf, err
	case string(OdataTypeMicrosoftAzureSearchSnowballTokenFilter):
		var stf SnowballTokenFilter
		err := json.Unmarshal(body, &stf)
		return stf, err
	case string(OdataTypeMicrosoftAzureSearchStemmerTokenFilter):
		var stf StemmerTokenFilter
		err := json.Unmarshal(body, &stf)
		return stf, err
	case string(OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter):
		var sotf StemmerOverrideTokenFilter
		err := json.Unmarshal(body, &sotf)
		return sotf, err
	case string(OdataTypeMicrosoftAzureSearchStopwordsTokenFilter):
		var stf StopwordsTokenFilter
		err := json.Unmarshal(body, &stf)
		return stf, err
	case string(OdataTypeMicrosoftAzureSearchSynonymTokenFilter):
		var stf SynonymTokenFilter
		err := json.Unmarshal(body, &stf)
		return stf, err
	case string(OdataTypeMicrosoftAzureSearchTruncateTokenFilter):
		var ttf TruncateTokenFilter
		err := json.Unmarshal(body, &ttf)
		return ttf, err
	case string(OdataTypeMicrosoftAzureSearchUniqueTokenFilter):
		var utf UniqueTokenFilter
		err := json.Unmarshal(body, &utf)
		return utf, err
	case string(OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter):
		var wdtf WordDelimiterTokenFilter
		err := json.Unmarshal(body, &wdtf)
		return wdtf, err
	default:
		var tf TokenFilter
		err := json.Unmarshal(body, &tf)
		return tf, err
	}
}
func unmarshalBasicTokenFilterArray(body []byte) ([]BasicTokenFilter, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	tfArray := make([]BasicTokenFilter, len(rawMessages))

	for index, rawMessage := range rawMessages {
		tf, err := unmarshalBasicTokenFilter(*rawMessage)
		if err != nil {
			return nil, err
		}
		tfArray[index] = tf
	}
	return tfArray, nil
}

// MarshalJSON is the custom marshaler for TokenFilter.
func (tf TokenFilter) MarshalJSON() ([]byte, error) {
	tf.OdataType = OdataTypeTokenFilter
	objectMap := make(map[string]interface{})
	if tf.Name != nil {
		objectMap["name"] = tf.Name
	}
	if tf.OdataType != "" {
		objectMap["@odata.type"] = tf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return &tf, true
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for TokenFilter.
func (tf TokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &tf, true
}

// TokenInfo information about a token returned by an analyzer.
type TokenInfo struct {
	// Token - READ-ONLY; The token returned by the analyzer.
	Token *string `json:"token,omitempty"`
	// StartOffset - READ-ONLY; The index of the first character of the token in the input text.
	StartOffset *int32 `json:"startOffset,omitempty"`
	// EndOffset - READ-ONLY; The index of the last character of the token in the input text.
	EndOffset *int32 `json:"endOffset,omitempty"`
	// Position - READ-ONLY; The position of the token in the input text relative to other tokens. The first token in the input text has position 0, the next has position 1, and so on. Depending on the analyzer used, some tokens might have the same position, for example if they are synonyms of each other.
	Position *int32 `json:"position,omitempty"`
}

// MarshalJSON is the custom marshaler for TokenInfo.
func (ti TokenInfo) MarshalJSON() ([]byte, error) {
	objectMap := make(map[string]interface{})
	return json.Marshal(objectMap)
}

// BasicTokenizer abstract base class for tokenizers.
type BasicTokenizer interface {
	AsClassicTokenizer() (*ClassicTokenizer, bool)
	AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool)
	AsKeywordTokenizer() (*KeywordTokenizer, bool)
	AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool)
	AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool)
	AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool)
	AsNGramTokenizer() (*NGramTokenizer, bool)
	AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool)
	AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool)
	AsPatternTokenizer() (*PatternTokenizer, bool)
	AsStandardTokenizer() (*StandardTokenizer, bool)
	AsStandardTokenizerV2() (*StandardTokenizerV2, bool)
	AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool)
	AsTokenizer() (*Tokenizer, bool)
}

// Tokenizer abstract base class for tokenizers.
type Tokenizer struct {
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

func unmarshalBasicTokenizer(body []byte) (BasicTokenizer, error) {
	var m map[string]interface{}
	err := json.Unmarshal(body, &m)
	if err != nil {
		return nil, err
	}

	switch m["@odata.type"] {
	case string(OdataTypeMicrosoftAzureSearchClassicTokenizer):
		var ct ClassicTokenizer
		err := json.Unmarshal(body, &ct)
		return ct, err
	case string(OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer):
		var engt EdgeNGramTokenizer
		err := json.Unmarshal(body, &engt)
		return engt, err
	case string(OdataTypeMicrosoftAzureSearchKeywordTokenizer):
		var kt KeywordTokenizer
		err := json.Unmarshal(body, &kt)
		return kt, err
	case string(OdataTypeMicrosoftAzureSearchKeywordTokenizerV2):
		var ktv KeywordTokenizerV2
		err := json.Unmarshal(body, &ktv)
		return ktv, err
	case string(OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer):
		var mlt MicrosoftLanguageTokenizer
		err := json.Unmarshal(body, &mlt)
		return mlt, err
	case string(OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer):
		var mlst MicrosoftLanguageStemmingTokenizer
		err := json.Unmarshal(body, &mlst)
		return mlst, err
	case string(OdataTypeMicrosoftAzureSearchNGramTokenizer):
		var ngt NGramTokenizer
		err := json.Unmarshal(body, &ngt)
		return ngt, err
	case string(OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer):
		var pht PathHierarchyTokenizer
		err := json.Unmarshal(body, &pht)
		return pht, err
	case string(OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2):
		var phtv PathHierarchyTokenizerV2
		err := json.Unmarshal(body, &phtv)
		return phtv, err
	case string(OdataTypeMicrosoftAzureSearchPatternTokenizer):
		var pt PatternTokenizer
		err := json.Unmarshal(body, &pt)
		return pt, err
	case string(OdataTypeMicrosoftAzureSearchStandardTokenizer):
		var st StandardTokenizer
		err := json.Unmarshal(body, &st)
		return st, err
	case string(OdataTypeMicrosoftAzureSearchStandardTokenizerV2):
		var stv StandardTokenizerV2
		err := json.Unmarshal(body, &stv)
		return stv, err
	case string(OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer):
		var uuet UaxURLEmailTokenizer
		err := json.Unmarshal(body, &uuet)
		return uuet, err
	default:
		var t Tokenizer
		err := json.Unmarshal(body, &t)
		return t, err
	}
}
func unmarshalBasicTokenizerArray(body []byte) ([]BasicTokenizer, error) {
	var rawMessages []*json.RawMessage
	err := json.Unmarshal(body, &rawMessages)
	if err != nil {
		return nil, err
	}

	tArray := make([]BasicTokenizer, len(rawMessages))

	for index, rawMessage := range rawMessages {
		t, err := unmarshalBasicTokenizer(*rawMessage)
		if err != nil {
			return nil, err
		}
		tArray[index] = t
	}
	return tArray, nil
}

// MarshalJSON is the custom marshaler for Tokenizer.
func (t Tokenizer) MarshalJSON() ([]byte, error) {
	t.OdataType = OdataTypeTokenizer
	objectMap := make(map[string]interface{})
	if t.Name != nil {
		objectMap["name"] = t.Name
	}
	if t.OdataType != "" {
		objectMap["@odata.type"] = t.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return nil, false
}

// AsTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsTokenizer() (*Tokenizer, bool) {
	return &t, true
}

// AsBasicTokenizer is the BasicTokenizer implementation for Tokenizer.
func (t Tokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &t, true
}

// TruncateTokenFilter truncates the terms to a specific length. This token filter is implemented using
// Apache Lucene.
type TruncateTokenFilter struct {
	// Length - The length at which terms will be truncated. Default and maximum is 300.
	Length *int32 `json:"length,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for TruncateTokenFilter.
func (ttf TruncateTokenFilter) MarshalJSON() ([]byte, error) {
	ttf.OdataType = OdataTypeMicrosoftAzureSearchTruncateTokenFilter
	objectMap := make(map[string]interface{})
	if ttf.Length != nil {
		objectMap["length"] = ttf.Length
	}
	if ttf.Name != nil {
		objectMap["name"] = ttf.Name
	}
	if ttf.OdataType != "" {
		objectMap["@odata.type"] = ttf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return &ttf, true
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for TruncateTokenFilter.
func (ttf TruncateTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &ttf, true
}

// UaxURLEmailTokenizer tokenizes urls and emails as one token. This tokenizer is implemented using Apache
// Lucene.
type UaxURLEmailTokenizer struct {
	// MaxTokenLength - The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that can be used is 300 characters.
	MaxTokenLength *int32 `json:"maxTokenLength,omitempty"`
	// Name - The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenizer', 'OdataTypeMicrosoftAzureSearchClassicTokenizer', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizer', 'OdataTypeMicrosoftAzureSearchKeywordTokenizerV2', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageTokenizer', 'OdataTypeMicrosoftAzureSearchMicrosoftLanguageStemmingTokenizer', 'OdataTypeMicrosoftAzureSearchNGramTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizer', 'OdataTypeMicrosoftAzureSearchPathHierarchyTokenizerV2', 'OdataTypeMicrosoftAzureSearchPatternTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizer', 'OdataTypeMicrosoftAzureSearchStandardTokenizerV2', 'OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer'
	OdataType OdataTypeBasicTokenizer `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) MarshalJSON() ([]byte, error) {
	uuet.OdataType = OdataTypeMicrosoftAzureSearchUaxURLEmailTokenizer
	objectMap := make(map[string]interface{})
	if uuet.MaxTokenLength != nil {
		objectMap["maxTokenLength"] = uuet.MaxTokenLength
	}
	if uuet.Name != nil {
		objectMap["name"] = uuet.Name
	}
	if uuet.OdataType != "" {
		objectMap["@odata.type"] = uuet.OdataType
	}
	return json.Marshal(objectMap)
}

// AsClassicTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsClassicTokenizer() (*ClassicTokenizer, bool) {
	return nil, false
}

// AsEdgeNGramTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsEdgeNGramTokenizer() (*EdgeNGramTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsKeywordTokenizer() (*KeywordTokenizer, bool) {
	return nil, false
}

// AsKeywordTokenizerV2 is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsKeywordTokenizerV2() (*KeywordTokenizerV2, bool) {
	return nil, false
}

// AsMicrosoftLanguageTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsMicrosoftLanguageTokenizer() (*MicrosoftLanguageTokenizer, bool) {
	return nil, false
}

// AsMicrosoftLanguageStemmingTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsMicrosoftLanguageStemmingTokenizer() (*MicrosoftLanguageStemmingTokenizer, bool) {
	return nil, false
}

// AsNGramTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsNGramTokenizer() (*NGramTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsPathHierarchyTokenizer() (*PathHierarchyTokenizer, bool) {
	return nil, false
}

// AsPathHierarchyTokenizerV2 is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsPathHierarchyTokenizerV2() (*PathHierarchyTokenizerV2, bool) {
	return nil, false
}

// AsPatternTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsPatternTokenizer() (*PatternTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsStandardTokenizer() (*StandardTokenizer, bool) {
	return nil, false
}

// AsStandardTokenizerV2 is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsStandardTokenizerV2() (*StandardTokenizerV2, bool) {
	return nil, false
}

// AsUaxURLEmailTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsUaxURLEmailTokenizer() (*UaxURLEmailTokenizer, bool) {
	return &uuet, true
}

// AsTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsTokenizer() (*Tokenizer, bool) {
	return nil, false
}

// AsBasicTokenizer is the BasicTokenizer implementation for UaxURLEmailTokenizer.
func (uuet UaxURLEmailTokenizer) AsBasicTokenizer() (BasicTokenizer, bool) {
	return &uuet, true
}

// UniqueTokenFilter filters out tokens with same text as the previous token. This token filter is
// implemented using Apache Lucene.
type UniqueTokenFilter struct {
	// OnlyOnSamePosition - A value indicating whether to remove duplicates only at the same position. Default is false.
	OnlyOnSamePosition *bool `json:"onlyOnSamePosition,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for UniqueTokenFilter.
func (utf UniqueTokenFilter) MarshalJSON() ([]byte, error) {
	utf.OdataType = OdataTypeMicrosoftAzureSearchUniqueTokenFilter
	objectMap := make(map[string]interface{})
	if utf.OnlyOnSamePosition != nil {
		objectMap["onlyOnSamePosition"] = utf.OnlyOnSamePosition
	}
	if utf.Name != nil {
		objectMap["name"] = utf.Name
	}
	if utf.OdataType != "" {
		objectMap["@odata.type"] = utf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return &utf, true
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return nil, false
}

// AsTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for UniqueTokenFilter.
func (utf UniqueTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &utf, true
}

// WebAPISkill a skill that can call a Web API endpoint, allowing you to extend a skillset by having it
// call your custom code.
type WebAPISkill struct {
	// URI - The url for the Web API.
	URI *string `json:"uri,omitempty"`
	// HTTPHeaders - The headers required to make the http request.
	HTTPHeaders map[string]*string `json:"httpHeaders"`
	// HTTPMethod - The method for the http request.
	HTTPMethod *string `json:"httpMethod,omitempty"`
	// Timeout - The desired timeout for the request. Default is 30 seconds.
	Timeout *string `json:"timeout,omitempty"`
	// BatchSize - The desired batch size which indicates number of documents.
	BatchSize *int32 `json:"batchSize,omitempty"`
	// DegreeOfParallelism - If set, the number of parallel calls that can be made to the Web API.
	DegreeOfParallelism *int32 `json:"degreeOfParallelism,omitempty"`
	// Name - The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default name of its 1-based index in the skills array, prefixed with the character '#'.
	Name *string `json:"name,omitempty"`
	// Description - The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string `json:"description,omitempty"`
	// Context - Represents the level at which operations take place, such as the document root or document content (for example, /document or /document/content). The default is /document.
	Context *string `json:"context,omitempty"`
	// Inputs - Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs *[]InputFieldMappingEntry `json:"inputs,omitempty"`
	// Outputs - The output of a skill is either a field in a search index, or a value that can be consumed as an input by another skill.
	Outputs *[]OutputFieldMappingEntry `json:"outputs,omitempty"`
	// OdataType - Possible values include: 'OdataTypeSkill', 'OdataTypeMicrosoftSkillsUtilConditionalSkill', 'OdataTypeMicrosoftSkillsTextKeyPhraseExtractionSkill', 'OdataTypeMicrosoftSkillsVisionOcrSkill', 'OdataTypeMicrosoftSkillsVisionImageAnalysisSkill', 'OdataTypeMicrosoftSkillsTextLanguageDetectionSkill', 'OdataTypeMicrosoftSkillsUtilShaperSkill', 'OdataTypeMicrosoftSkillsTextMergeSkill', 'OdataTypeMicrosoftSkillsTextEntityRecognitionSkill', 'OdataTypeMicrosoftSkillsTextSentimentSkill', 'OdataTypeMicrosoftSkillsTextSplitSkill', 'OdataTypeMicrosoftSkillsTextTranslationSkill', 'OdataTypeMicrosoftSkillsCustomWebAPISkill'
	OdataType OdataTypeBasicSkill `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for WebAPISkill.
func (was WebAPISkill) MarshalJSON() ([]byte, error) {
	was.OdataType = OdataTypeMicrosoftSkillsCustomWebAPISkill
	objectMap := make(map[string]interface{})
	if was.URI != nil {
		objectMap["uri"] = was.URI
	}
	if was.HTTPHeaders != nil {
		objectMap["httpHeaders"] = was.HTTPHeaders
	}
	if was.HTTPMethod != nil {
		objectMap["httpMethod"] = was.HTTPMethod
	}
	if was.Timeout != nil {
		objectMap["timeout"] = was.Timeout
	}
	if was.BatchSize != nil {
		objectMap["batchSize"] = was.BatchSize
	}
	if was.DegreeOfParallelism != nil {
		objectMap["degreeOfParallelism"] = was.DegreeOfParallelism
	}
	if was.Name != nil {
		objectMap["name"] = was.Name
	}
	if was.Description != nil {
		objectMap["description"] = was.Description
	}
	if was.Context != nil {
		objectMap["context"] = was.Context
	}
	if was.Inputs != nil {
		objectMap["inputs"] = was.Inputs
	}
	if was.Outputs != nil {
		objectMap["outputs"] = was.Outputs
	}
	if was.OdataType != "" {
		objectMap["@odata.type"] = was.OdataType
	}
	return json.Marshal(objectMap)
}

// AsConditionalSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsConditionalSkill() (*ConditionalSkill, bool) {
	return nil, false
}

// AsKeyPhraseExtractionSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsKeyPhraseExtractionSkill() (*KeyPhraseExtractionSkill, bool) {
	return nil, false
}

// AsOcrSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsOcrSkill() (*OcrSkill, bool) {
	return nil, false
}

// AsImageAnalysisSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsImageAnalysisSkill() (*ImageAnalysisSkill, bool) {
	return nil, false
}

// AsLanguageDetectionSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsLanguageDetectionSkill() (*LanguageDetectionSkill, bool) {
	return nil, false
}

// AsShaperSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsShaperSkill() (*ShaperSkill, bool) {
	return nil, false
}

// AsMergeSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsMergeSkill() (*MergeSkill, bool) {
	return nil, false
}

// AsEntityRecognitionSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsEntityRecognitionSkill() (*EntityRecognitionSkill, bool) {
	return nil, false
}

// AsSentimentSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsSentimentSkill() (*SentimentSkill, bool) {
	return nil, false
}

// AsSplitSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsSplitSkill() (*SplitSkill, bool) {
	return nil, false
}

// AsTextTranslationSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsTextTranslationSkill() (*TextTranslationSkill, bool) {
	return nil, false
}

// AsWebAPISkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsWebAPISkill() (*WebAPISkill, bool) {
	return &was, true
}

// AsSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsSkill() (*Skill, bool) {
	return nil, false
}

// AsBasicSkill is the BasicSkill implementation for WebAPISkill.
func (was WebAPISkill) AsBasicSkill() (BasicSkill, bool) {
	return &was, true
}

// WordDelimiterTokenFilter splits words into subwords and performs optional transformations on subword
// groups. This token filter is implemented using Apache Lucene.
type WordDelimiterTokenFilter struct {
	// GenerateWordParts - A value indicating whether to generate part words. If set, causes parts of words to be generated; for example "AzureSearch" becomes "Azure" "Search". Default is true.
	GenerateWordParts *bool `json:"generateWordParts,omitempty"`
	// GenerateNumberParts - A value indicating whether to generate number subwords. Default is true.
	GenerateNumberParts *bool `json:"generateNumberParts,omitempty"`
	// CatenateWords - A value indicating whether maximum runs of word parts will be catenated. For example, if this is set to true, "Azure-Search" becomes "AzureSearch". Default is false.
	CatenateWords *bool `json:"catenateWords,omitempty"`
	// CatenateNumbers - A value indicating whether maximum runs of number parts will be catenated. For example, if this is set to true, "1-2" becomes "12". Default is false.
	CatenateNumbers *bool `json:"catenateNumbers,omitempty"`
	// CatenateAll - A value indicating whether all subword parts will be catenated. For example, if this is set to true, "Azure-Search-1" becomes "AzureSearch1". Default is false.
	CatenateAll *bool `json:"catenateAll,omitempty"`
	// SplitOnCaseChange - A value indicating whether to split words on caseChange. For example, if this is set to true, "AzureSearch" becomes "Azure" "Search". Default is true.
	SplitOnCaseChange *bool `json:"splitOnCaseChange,omitempty"`
	// PreserveOriginal - A value indicating whether original words will be preserved and added to the subword list. Default is false.
	PreserveOriginal *bool `json:"preserveOriginal,omitempty"`
	// SplitOnNumerics - A value indicating whether to split on numbers. For example, if this is set to true, "Azure1Search" becomes "Azure" "1" "Search". Default is true.
	SplitOnNumerics *bool `json:"splitOnNumerics,omitempty"`
	// StemEnglishPossessive - A value indicating whether to remove trailing "'s" for each subword. Default is true.
	StemEnglishPossessive *bool `json:"stemEnglishPossessive,omitempty"`
	// ProtectedWords - A list of tokens to protect from being delimited.
	ProtectedWords *[]string `json:"protectedWords,omitempty"`
	// Name - The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start and end with alphanumeric characters, and is limited to 128 characters.
	Name *string `json:"name,omitempty"`
	// OdataType - Possible values include: 'OdataTypeTokenFilter', 'OdataTypeMicrosoftAzureSearchASCIIFoldingTokenFilter', 'OdataTypeMicrosoftAzureSearchCjkBigramTokenFilter', 'OdataTypeMicrosoftAzureSearchCommonGramTokenFilter', 'OdataTypeMicrosoftAzureSearchDictionaryDecompounderTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchEdgeNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchElisionTokenFilter', 'OdataTypeMicrosoftAzureSearchKeepTokenFilter', 'OdataTypeMicrosoftAzureSearchKeywordMarkerTokenFilter', 'OdataTypeMicrosoftAzureSearchLengthTokenFilter', 'OdataTypeMicrosoftAzureSearchLimitTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilter', 'OdataTypeMicrosoftAzureSearchNGramTokenFilterV2', 'OdataTypeMicrosoftAzureSearchPatternCaptureTokenFilter', 'OdataTypeMicrosoftAzureSearchPatternReplaceTokenFilter', 'OdataTypeMicrosoftAzureSearchPhoneticTokenFilter', 'OdataTypeMicrosoftAzureSearchShingleTokenFilter', 'OdataTypeMicrosoftAzureSearchSnowballTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerTokenFilter', 'OdataTypeMicrosoftAzureSearchStemmerOverrideTokenFilter', 'OdataTypeMicrosoftAzureSearchStopwordsTokenFilter', 'OdataTypeMicrosoftAzureSearchSynonymTokenFilter', 'OdataTypeMicrosoftAzureSearchTruncateTokenFilter', 'OdataTypeMicrosoftAzureSearchUniqueTokenFilter', 'OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter'
	OdataType OdataTypeBasicTokenFilter `json:"@odata.type,omitempty"`
}

// MarshalJSON is the custom marshaler for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) MarshalJSON() ([]byte, error) {
	wdtf.OdataType = OdataTypeMicrosoftAzureSearchWordDelimiterTokenFilter
	objectMap := make(map[string]interface{})
	if wdtf.GenerateWordParts != nil {
		objectMap["generateWordParts"] = wdtf.GenerateWordParts
	}
	if wdtf.GenerateNumberParts != nil {
		objectMap["generateNumberParts"] = wdtf.GenerateNumberParts
	}
	if wdtf.CatenateWords != nil {
		objectMap["catenateWords"] = wdtf.CatenateWords
	}
	if wdtf.CatenateNumbers != nil {
		objectMap["catenateNumbers"] = wdtf.CatenateNumbers
	}
	if wdtf.CatenateAll != nil {
		objectMap["catenateAll"] = wdtf.CatenateAll
	}
	if wdtf.SplitOnCaseChange != nil {
		objectMap["splitOnCaseChange"] = wdtf.SplitOnCaseChange
	}
	if wdtf.PreserveOriginal != nil {
		objectMap["preserveOriginal"] = wdtf.PreserveOriginal
	}
	if wdtf.SplitOnNumerics != nil {
		objectMap["splitOnNumerics"] = wdtf.SplitOnNumerics
	}
	if wdtf.StemEnglishPossessive != nil {
		objectMap["stemEnglishPossessive"] = wdtf.StemEnglishPossessive
	}
	if wdtf.ProtectedWords != nil {
		objectMap["protectedWords"] = wdtf.ProtectedWords
	}
	if wdtf.Name != nil {
		objectMap["name"] = wdtf.Name
	}
	if wdtf.OdataType != "" {
		objectMap["@odata.type"] = wdtf.OdataType
	}
	return json.Marshal(objectMap)
}

// AsASCIIFoldingTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsASCIIFoldingTokenFilter() (*ASCIIFoldingTokenFilter, bool) {
	return nil, false
}

// AsCjkBigramTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsCjkBigramTokenFilter() (*CjkBigramTokenFilter, bool) {
	return nil, false
}

// AsCommonGramTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsCommonGramTokenFilter() (*CommonGramTokenFilter, bool) {
	return nil, false
}

// AsDictionaryDecompounderTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsDictionaryDecompounderTokenFilter() (*DictionaryDecompounderTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsEdgeNGramTokenFilter() (*EdgeNGramTokenFilter, bool) {
	return nil, false
}

// AsEdgeNGramTokenFilterV2 is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsEdgeNGramTokenFilterV2() (*EdgeNGramTokenFilterV2, bool) {
	return nil, false
}

// AsElisionTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsElisionTokenFilter() (*ElisionTokenFilter, bool) {
	return nil, false
}

// AsKeepTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsKeepTokenFilter() (*KeepTokenFilter, bool) {
	return nil, false
}

// AsKeywordMarkerTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsKeywordMarkerTokenFilter() (*KeywordMarkerTokenFilter, bool) {
	return nil, false
}

// AsLengthTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsLengthTokenFilter() (*LengthTokenFilter, bool) {
	return nil, false
}

// AsLimitTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsLimitTokenFilter() (*LimitTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsNGramTokenFilter() (*NGramTokenFilter, bool) {
	return nil, false
}

// AsNGramTokenFilterV2 is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsNGramTokenFilterV2() (*NGramTokenFilterV2, bool) {
	return nil, false
}

// AsPatternCaptureTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsPatternCaptureTokenFilter() (*PatternCaptureTokenFilter, bool) {
	return nil, false
}

// AsPatternReplaceTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsPatternReplaceTokenFilter() (*PatternReplaceTokenFilter, bool) {
	return nil, false
}

// AsPhoneticTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsPhoneticTokenFilter() (*PhoneticTokenFilter, bool) {
	return nil, false
}

// AsShingleTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsShingleTokenFilter() (*ShingleTokenFilter, bool) {
	return nil, false
}

// AsSnowballTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsSnowballTokenFilter() (*SnowballTokenFilter, bool) {
	return nil, false
}

// AsStemmerTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsStemmerTokenFilter() (*StemmerTokenFilter, bool) {
	return nil, false
}

// AsStemmerOverrideTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsStemmerOverrideTokenFilter() (*StemmerOverrideTokenFilter, bool) {
	return nil, false
}

// AsStopwordsTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsStopwordsTokenFilter() (*StopwordsTokenFilter, bool) {
	return nil, false
}

// AsSynonymTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsSynonymTokenFilter() (*SynonymTokenFilter, bool) {
	return nil, false
}

// AsTruncateTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsTruncateTokenFilter() (*TruncateTokenFilter, bool) {
	return nil, false
}

// AsUniqueTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsUniqueTokenFilter() (*UniqueTokenFilter, bool) {
	return nil, false
}

// AsWordDelimiterTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsWordDelimiterTokenFilter() (*WordDelimiterTokenFilter, bool) {
	return &wdtf, true
}

// AsTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsTokenFilter() (*TokenFilter, bool) {
	return nil, false
}

// AsBasicTokenFilter is the BasicTokenFilter implementation for WordDelimiterTokenFilter.
func (wdtf WordDelimiterTokenFilter) AsBasicTokenFilter() (BasicTokenFilter, bool) {
	return &wdtf, true
}
